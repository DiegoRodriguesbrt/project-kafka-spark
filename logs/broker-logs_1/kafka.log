[2025-05-20 22:15:26,093] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 22:15:26,422] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 22:15:26,469] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 22:15:26,477] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:31,705] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 22:15:31,796] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 22:15:31,803] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:31,976] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:31,996] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:32,034] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 22:15:32,037] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-20 22:15:32,043] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2025-05-20 22:15:32,048] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:32,157] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:32,159] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:32,160] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:32,191] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-20 22:15:32,204] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:15:32,217] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:15:32,227] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:15:32,319] INFO [RaftManager id=4] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1194) from null (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:15:32,335] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:15:32,337] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:15:32,431] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:32,448] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2025-05-20 22:15:32,451] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:32,480] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:15:32,485] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:15:32,488] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:15:32,506] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:15:32,551] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:32,588] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 22:15:32,589] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 22:15:32,616] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:32,622] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@198480695 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:15:32,640] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,645] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,645] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:15:32,654] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:32,763] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:32,790] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,800] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,816] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,817] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,821] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,822] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,868] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:32,914] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,921] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,923] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,924] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,935] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,941] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,969] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,071] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,123] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,125] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,145] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,154] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,159] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,165] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,172] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,174] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 22:15:33,228] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-20 22:15:33,231] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 22:15:33,243] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-20 22:15:33,265] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,275] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,303] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,324] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,403] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,412] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,464] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,466] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,466] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,491] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,504] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,506] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,528] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-05-20 22:15:33,549] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,578] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,552] INFO [BrokerLifecycleManager id=4] Incarnation IYofL_UyQ2GDre7Jka-bIA of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:15:33,609] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,674] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,675] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 22:15:33,676] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 22:15:33,687] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 22:15:33,746] INFO [RaftManager id=4] Completed transition to Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1194) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:15:33,775] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,822] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:15:33,876] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,886] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,912] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,915] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,919] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,977] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,994] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:34,002] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:34,045] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:34,079] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,109] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 1533 (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:15:34,181] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=1531, metadata=Optional.empty)] for the first time for epoch 8 (org.apache.kafka.raft.FollowerState)
[2025-05-20 22:15:34,184] INFO [MetadataLoader id=4] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1531 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,248] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 1531 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,482] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 1530, but the high water mark is 1536 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,514] INFO [MetadataLoader id=4] initializeNewPublishers: The loader is still catching up because we have loaded up to offset 1530, but the high water mark is 1536 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,518] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 1536 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,522] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 1535 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,523] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 1535 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,524] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 1535 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,526] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=1535, epoch=8) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-20 22:15:34,536] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:15:34,541] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-05-20 22:15:34,558] INFO Loaded 0 logs in 21ms (kafka.log.LogManager)
[2025-05-20 22:15:34,560] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-20 22:15:34,565] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-20 22:15:34,585] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-20 22:15:34,593] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:15:34,611] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 22:15:34,620] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 22:15:34,627] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:15:34,806] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:15:34,814] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:15:34,816] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:15:34,816] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:34,824] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:34,825] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:15:34,828] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:15:34,828] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:15:34,842] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 1535 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,842] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 22:15:34,845] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 22:15:34,852] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:34,874] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 22:15:34,929] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:15:34,930] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 22:15:34,937] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 22:15:34,937] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 22:15:34,939] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2025-05-20 22:15:34,943] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 22:15:34,948] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 22:15:34,957] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 22:15:34,958] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 22:15:34,958] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 22:15:34,959] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 22:15:34,960] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-20 22:15:34,961] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:15:34,962] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:15:34,964] INFO Kafka startTimeMs: 1747779334961 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:15:34,969] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-20 22:15:40,560] INFO [Broker id=4] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:15:40,566] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:15:40,570] INFO [Broker id=4] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-20 22:15:40,604] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:40,617] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-05-20 22:15:40,624] INFO [Partition _schemas-0 broker=4] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-05-20 22:15:40,633] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:40,636] INFO [Broker id=4] Leader _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:40,658] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 22:15:40,985] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-05-20 22:15:41,088] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-05-20 22:15:41,192] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-05-20 22:15:41,295] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-05-20 22:15:41,396] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-05-20 22:15:41,500] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-05-20 22:15:41,558] INFO [Broker id=4] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:15:41,559] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-16, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-29, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-39, __consumer_offsets-5, __consumer_offsets-37, __consumer_offsets-2, __consumer_offsets-34) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:15:41,560] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,570] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,573] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,576] INFO [Partition __consumer_offsets-16 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-05-20 22:15:41,577] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,577] INFO [Broker id=4] Leader __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,590] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,598] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,602] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,602] INFO [Partition __consumer_offsets-13 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-05-20 22:15:41,603] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,603] INFO [Broker id=4] Leader __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,623] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,632] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,635] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,637] INFO [Partition __consumer_offsets-46 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-05-20 22:15:41,637] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,638] INFO [Broker id=4] Leader __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,644] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,651] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,654] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,656] INFO [Partition __consumer_offsets-11 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-05-20 22:15:41,657] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,657] INFO [Broker id=4] Leader __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,663] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,669] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,670] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,671] INFO [Partition __consumer_offsets-44 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-05-20 22:15:41,671] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,672] INFO [Broker id=4] Leader __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,676] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,680] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,684] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,684] INFO [Partition __consumer_offsets-23 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-05-20 22:15:41,685] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,685] INFO [Broker id=4] Leader __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,690] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,694] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,696] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,696] INFO [Partition __consumer_offsets-49 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-05-20 22:15:41,697] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,697] INFO [Broker id=4] Leader __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,702] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,707] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,709] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,709] INFO [Partition __consumer_offsets-18 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-05-20 22:15:41,709] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,710] INFO [Broker id=4] Leader __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,715] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,721] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,723] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,723] INFO [Partition __consumer_offsets-29 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-05-20 22:15:41,724] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,725] INFO [Broker id=4] Leader __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,729] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,735] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,736] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,737] INFO [Partition __consumer_offsets-30 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-05-20 22:15:41,737] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,738] INFO [Broker id=4] Leader __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,743] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,749] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,751] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,751] INFO [Partition __consumer_offsets-26 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-05-20 22:15:41,751] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,752] INFO [Broker id=4] Leader __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,757] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,763] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,765] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,766] INFO [Partition __consumer_offsets-7 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-05-20 22:15:41,767] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,768] INFO [Broker id=4] Leader __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,774] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,779] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,780] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,781] INFO [Partition __consumer_offsets-39 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-05-20 22:15:41,782] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,782] INFO [Broker id=4] Leader __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,787] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,792] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,793] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,795] INFO [Partition __consumer_offsets-5 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-05-20 22:15:41,796] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,796] INFO [Broker id=4] Leader __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,802] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,809] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,810] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,811] INFO [Partition __consumer_offsets-37 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-05-20 22:15:41,811] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,812] INFO [Broker id=4] Leader __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,818] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,823] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,825] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,826] INFO [Partition __consumer_offsets-2 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-05-20 22:15:41,826] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,827] INFO [Broker id=4] Leader __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,833] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,838] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,839] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,840] INFO [Partition __consumer_offsets-34 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-05-20 22:15:41,840] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,841] INFO [Broker id=4] Leader __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,848] INFO [Broker id=4] Transitioning 33 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:15:41,849] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,856] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,857] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,858] INFO [Partition __consumer_offsets-15 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-05-20 22:15:41,859] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,860] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,860] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,865] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,866] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,867] INFO [Partition __consumer_offsets-48 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-05-20 22:15:41,868] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,869] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,869] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,880] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,882] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,883] INFO [Partition __consumer_offsets-9 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-05-20 22:15:41,883] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,883] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,885] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,890] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,892] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,892] INFO [Partition __consumer_offsets-42 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-05-20 22:15:41,893] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,893] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,894] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,898] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,900] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,901] INFO [Partition __consumer_offsets-21 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-05-20 22:15:41,901] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,902] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,902] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,907] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,909] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,910] INFO [Partition __consumer_offsets-19 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-05-20 22:15:41,910] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,910] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,911] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,919] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,922] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,923] INFO [Partition __consumer_offsets-17 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-05-20 22:15:41,923] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,924] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,924] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,931] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,932] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,932] INFO [Partition __consumer_offsets-32 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-05-20 22:15:41,933] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,933] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,934] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,939] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,940] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,940] INFO [Partition __consumer_offsets-28 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-05-20 22:15:41,941] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,941] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,942] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,949] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,951] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,951] INFO [Partition __consumer_offsets-40 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-05-20 22:15:41,952] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,952] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,953] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,959] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,961] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,961] INFO [Partition __consumer_offsets-38 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-05-20 22:15:41,962] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,962] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,963] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,969] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,971] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,971] INFO [Partition __consumer_offsets-3 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-05-20 22:15:41,971] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,971] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,972] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,978] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,980] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,980] INFO [Partition __consumer_offsets-36 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-05-20 22:15:41,980] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,981] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,981] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,987] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,991] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,992] INFO [Partition __consumer_offsets-1 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-05-20 22:15:41,993] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,993] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,994] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,001] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,003] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,003] INFO [Partition __consumer_offsets-47 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-05-20 22:15:42,004] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,005] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,005] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,011] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,013] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,014] INFO [Partition __consumer_offsets-45 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-05-20 22:15:42,014] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,015] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,015] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,020] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,023] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,023] INFO [Partition __consumer_offsets-14 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-05-20 22:15:42,023] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,024] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,024] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,031] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,032] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,033] INFO [Partition __consumer_offsets-43 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-05-20 22:15:42,033] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,034] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,034] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,039] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,040] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,041] INFO [Partition __consumer_offsets-12 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-05-20 22:15:42,041] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,042] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,042] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,047] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,049] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,049] INFO [Partition __consumer_offsets-41 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-05-20 22:15:42,050] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,051] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,051] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,057] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,058] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,060] INFO [Partition __consumer_offsets-10 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-05-20 22:15:42,060] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,060] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,061] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,066] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,067] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,067] INFO [Partition __consumer_offsets-24 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-05-20 22:15:42,068] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,068] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,069] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,075] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,076] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,076] INFO [Partition __consumer_offsets-22 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-05-20 22:15:42,077] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,077] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,077] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,082] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,083] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,084] INFO [Partition __consumer_offsets-20 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-05-20 22:15:42,084] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,084] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,085] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,089] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,090] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,091] INFO [Partition __consumer_offsets-31 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-05-20 22:15:42,091] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,091] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,092] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,096] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,097] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,098] INFO [Partition __consumer_offsets-0 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,098] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,099] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,099] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,103] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,104] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,106] INFO [Partition __consumer_offsets-27 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-05-20 22:15:42,107] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,107] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,107] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,112] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,113] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,115] INFO [Partition __consumer_offsets-25 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-05-20 22:15:42,116] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,116] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,117] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,122] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,124] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,125] INFO [Partition __consumer_offsets-8 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-05-20 22:15:42,126] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,126] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,127] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,134] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,136] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,136] INFO [Partition __consumer_offsets-6 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-05-20 22:15:42,137] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,137] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,137] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,143] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,144] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,145] INFO [Partition __consumer_offsets-35 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-05-20 22:15:42,146] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,146] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,147] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,153] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,154] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,155] INFO [Partition __consumer_offsets-4 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-05-20 22:15:42,155] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,155] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,157] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,162] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,163] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,164] INFO [Partition __consumer_offsets-33 broker=4] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-05-20 22:15:42,164] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,165] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,165] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-40, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-1, __consumer_offsets-47, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-31, __consumer_offsets-0, __consumer_offsets-27, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-33) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:15:42,166] INFO [Broker id=4] Stopped fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-05-20 22:15:42,193] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,199] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-47 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-48 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-14 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-28 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-25 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-8 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-35 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-4 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:15:42,202] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,204] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,204] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-45 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-21 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-17 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-32 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-6 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:15:42,206] INFO [Broker id=4] Started fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-05-20 22:15:42,204] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,207] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,210] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,207] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,210] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,211] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,213] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,214] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,214] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,215] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,216] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,217] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,217] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,217] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,215] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,218] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,218] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,218] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,219] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,219] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,219] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,220] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,222] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,222] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,224] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,224] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,224] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,225] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,226] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,227] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,227] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,227] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,226] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,228] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,228] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,229] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,229] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,229] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,230] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,230] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,231] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,231] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,231] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,232] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,232] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,233] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,233] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,233] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,234] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,234] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,235] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,235] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,235] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,236] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,236] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,236] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,237] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,237] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,238] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,237] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,238] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,238] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,239] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,239] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,242] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,267] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,268] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,269] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,269] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,270] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,270] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,270] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,271] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,271] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,272] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,272] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,272] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,272] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,273] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,273] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,273] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,274] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,274] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,274] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,275] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,275] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,275] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,276] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,276] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,276] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-16 in 7 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,276] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,278] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,277] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-13 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,278] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,279] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-46 in 9 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,279] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,280] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,280] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-11 in 9 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,280] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,280] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-44 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,280] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,281] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,281] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-23 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,281] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,282] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-49 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,282] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,283] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-18 in 9 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,284] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-29 in 10 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,284] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-30 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,284] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,284] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-26 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,285] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-7 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,285] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,285] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-39 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,286] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,287] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,287] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-5 in 8 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,287] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,288] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,288] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-37 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,288] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,289] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-2 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,289] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,290] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-34 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,290] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,290] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,290] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,291] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,291] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,291] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,291] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,291] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,292] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,292] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,292] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,292] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,293] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,293] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,294] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,294] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,294] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,295] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,295] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,295] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,295] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,295] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,296] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,296] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,296] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,296] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,297] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,297] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,297] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,297] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,297] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,298] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,298] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,298] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,298] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,299] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,298] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,299] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,299] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,299] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,300] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,300] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,300] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,300] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,301] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,301] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,301] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,301] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,302] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,302] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,302] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,303] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,302] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,303] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,303] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,303] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,303] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,304] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,304] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,304] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,304] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,305] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,305] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,305] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,305] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,305] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,306] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,306] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,306] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,307] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,307] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,307] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,307] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,307] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,307] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,308] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,308] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,308] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,309] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,309] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,309] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,310] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,310] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,310] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,310] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,310] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,310] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,311] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,311] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,311] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,312] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,312] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 22:15:42,325] INFO [GroupCoordinator 4]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-9e39be7b-a95e-4522-9a0c-51d8825d1b3a and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,332] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-9e39be7b-a95e-4522-9a0c-51d8825d1b3a with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,337] INFO [GroupCoordinator 4]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,354] INFO [GroupCoordinator 4]: Assignment received from leader sr-1-9e39be7b-a95e-4522-9a0c-51d8825d1b3a for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:57,382] INFO Error processing describe configs request for resource DescribeConfigsResource(resourceType=4, resourceName='5', configurationKeys=null) (kafka.server.ConfigHelper)
org.apache.kafka.common.errors.InvalidRequestException: Unexpected broker id, expected 4 or empty string, but received 5
[2025-05-20 22:15:57,386] INFO Error processing describe configs request for resource DescribeConfigsResource(resourceType=4, resourceName='6', configurationKeys=null) (kafka.server.ConfigHelper)
org.apache.kafka.common.errors.InvalidRequestException: Unexpected broker id, expected 4 or empty string, but received 6
[2025-05-20 22:16:10,331] INFO Error processing describe configs request for resource DescribeConfigsResource(resourceType=4, resourceName='5', configurationKeys=null) (kafka.server.ConfigHelper)
org.apache.kafka.common.errors.InvalidRequestException: Unexpected broker id, expected 4 or empty string, but received 5
[2025-05-20 22:16:10,331] INFO Error processing describe configs request for resource DescribeConfigsResource(resourceType=4, resourceName='6', configurationKeys=null) (kafka.server.ConfigHelper)
org.apache.kafka.common.errors.InvalidRequestException: Unexpected broker id, expected 4 or empty string, but received 6
[2025-05-20 22:16:19,882] INFO [Broker id=4] Transitioning 2 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:16:19,885] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:19,886] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:19,887] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:19,888] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:19,890] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:19,890] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:19,892] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:19,894] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,068] INFO [Broker id=4] Transitioning 31 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:16:20,069] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,071] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,072] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,072] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,073] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,074] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,075] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,075] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,076] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,077] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,078] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,078] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,079] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,080] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,081] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,081] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,081] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,082] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,082] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,082] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,083] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,083] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,084] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,084] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,084] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,085] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,085] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,085] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,086] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,086] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,086] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,087] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,088] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,088] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,088] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,089] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,090] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,090] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,091] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,091] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,091] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,091] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,092] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,092] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,092] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,093] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,093] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,093] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,093] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,094] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,094] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,095] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,095] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,095] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,096] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,097] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,096] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,097] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,097] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,098] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,099] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,099] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,099] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,099] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,100] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,099] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,100] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,101] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,100] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,101] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,102] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,101] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,102] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,103] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,102] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,103] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,104] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,104] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,105] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,105] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,106] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,106] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,106] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,106] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,107] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,107] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,108] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,108] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,108] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,108] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,109] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,109] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,109] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,110] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,110] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,110] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,111] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,111] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,111] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,112] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,112] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,112] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,112] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,113] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,113] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,113] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,114] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,114] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,114] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,115] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,114] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,115] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,115] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,115] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,116] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,116] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,116] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,117] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,117] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,117] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,117] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,117] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,117] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,118] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:21,181] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 1 (__consumer_offsets-29) (reason: Removing member sr-1-9e39be7b-a95e-4522-9a0c-51d8825d1b3a on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:21,183] INFO [GroupCoordinator 4]: Group schema-registry with generation 2 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:21,187] INFO [GroupCoordinator 4]: Member MemberMetadata(memberId=sr-1-9e39be7b-a95e-4522-9a0c-51d8825d1b3a, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,372] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 22:16:22,376] INFO [BrokerServer id=4] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
[2025-05-20 22:16:22,377] INFO [BrokerServer id=4] shutting down (kafka.server.BrokerServer)
[2025-05-20 22:16:22,381] INFO [BrokerLifecycleManager id=4] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:16:22,428] INFO [BrokerLifecycleManager id=4] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:16:22,514] INFO [Broker id=4] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:16:22,515] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,515] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,516] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,516] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,517] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,517] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,517] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,518] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,518] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,520] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,522] INFO [BrokerLifecycleManager id=4] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:16:22,522] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,522] INFO [BrokerLifecycleManager id=4] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,523] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,523] INFO [BrokerLifecycleManager id=4] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:16:22,524] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,524] INFO [broker-4-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,525] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 2 from offset 2 with partition epoch 3 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,526] INFO [SocketServer listenerType=BROKER, nodeId=4] Stopping socket server request processors (kafka.network.SocketServer)
[2025-05-20 22:16:22,525] INFO [broker-4-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,525] INFO [broker-4-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,527] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,540] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:16:22,540] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,541] INFO [SocketServer listenerType=BROKER, nodeId=4] Stopped socket server request processors (kafka.network.SocketServer)
[2025-05-20 22:16:22,541] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,541] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,542] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,543] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,543] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,544] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 2 from offset 2 with partition epoch 3 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,545] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,546] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,547] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,548] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,548] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,551] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,551] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,552] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,553] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,553] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,553] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,553] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,554] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,554] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,554] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,555] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,555] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,556] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,556] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,557] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,557] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,559] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,559] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,560] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,560] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,561] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,561] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,561] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,562] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,577] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:16:22,578] INFO [ReplicaAlterLogDirsManager on broker 4] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:16:22,588] INFO [Broker id=4] Stopped fetchers as part of controlled shutdown for 51 partitions (state.change.logger)
[2025-05-20 22:16:22,588] INFO [ReplicaFetcherThread-0-6]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:16:22,590] INFO [ReplicaFetcherThread-0-6]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:16:22,590] INFO [ReplicaFetcherThread-0-6]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:16:22,593] INFO [ReplicaFetcherThread-0-5]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:16:22,593] INFO [ReplicaFetcherThread-0-5]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:16:22,593] INFO [ReplicaFetcherThread-0-5]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:16:22,596] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,596] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,597] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,597] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,597] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,597] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,598] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,598] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,599] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,599] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,599] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,599] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,600] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,601] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,602] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,602] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,603] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,604] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,604] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,604] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,604] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,604] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,605] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,605] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,605] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,605] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,606] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,606] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,606] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,607] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,607] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,607] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,608] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,609] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,608] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,609] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,609] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,609] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,610] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,610] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,611] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,611] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,611] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,611] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,612] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,612] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,612] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,613] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,613] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,613] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,614] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,614] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,615] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,615] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,616] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,616] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,615] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,616] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,617] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,617] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,618] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,618] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,618] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,619] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,619] INFO [GroupCoordinator 4]: Unloading group metadata for schema-registry with generation 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,620] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,620] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,620] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,621] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,621] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,621] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,622] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,621] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,622] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,622] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,622] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,623] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,623] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,623] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,624] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,624] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,624] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,624] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,625] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,625] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,625] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,626] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,626] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,626] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,627] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,627] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,627] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,628] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,628] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,628] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,628] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,629] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,629] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,629] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,630] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,631] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,630] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,631] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,631] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,632] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,632] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,632] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,632] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,633] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,633] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,633] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,634] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,634] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,634] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,635] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,635] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,636] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,635] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,636] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,636] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,636] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,637] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,637] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,638] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,638] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,638] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,638] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,639] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,639] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,640] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,640] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,640] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,640] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,641] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,641] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,641] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,642] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,642] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,642] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,643] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,643] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,643] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,644] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,643] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,644] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,644] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,645] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,645] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,645] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,645] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,646] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,647] INFO [data-plane Kafka Request Handler on Broker 4], shutting down (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 22:16:22,650] INFO [data-plane Kafka Request Handler on Broker 4], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 22:16:22,651] INFO [ExpirationReaper-4-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,652] INFO [ExpirationReaper-4-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,652] INFO [ExpirationReaper-4-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,653] INFO [KafkaApi-4] Shutdown complete. (kafka.server.KafkaApis)
[2025-05-20 22:16:22,658] INFO [TransactionCoordinator id=4] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:16:22,659] INFO [Transaction State Manager 4]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2025-05-20 22:16:22,659] INFO [TxnMarkerSenderThread-4]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:16:22,660] INFO [TxnMarkerSenderThread-4]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:16:22,660] INFO [TxnMarkerSenderThread-4]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:16:22,663] INFO [TransactionCoordinator id=4] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:16:22,664] INFO [GroupCoordinator 4]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,665] INFO [ExpirationReaper-4-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,666] INFO [ExpirationReaper-4-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,666] INFO [ExpirationReaper-4-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,666] INFO [ExpirationReaper-4-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,667] INFO [ExpirationReaper-4-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,667] INFO [ExpirationReaper-4-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,668] INFO [GroupCoordinator 4]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,669] INFO [AssignmentsManager id=4]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,670] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,671] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,671] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,671] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:16:22,672] INFO [AssignmentsManager id=4]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,673] INFO [ReplicaManager broker=4] Shutting down (kafka.server.ReplicaManager)
[2025-05-20 22:16:22,674] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:16:22,675] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:16:22,675] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:16:22,676] INFO [ReplicaFetcherManager on broker 4] shutting down (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:16:22,678] INFO [ReplicaFetcherManager on broker 4] shutdown completed (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:16:22,679] INFO [ReplicaAlterLogDirsManager on broker 4] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:16:22,679] INFO [ReplicaAlterLogDirsManager on broker 4] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:16:22,679] INFO [ExpirationReaper-4-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,680] INFO [ExpirationReaper-4-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,680] INFO [ExpirationReaper-4-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,682] INFO [ExpirationReaper-4-RemoteFetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,682] INFO [ExpirationReaper-4-RemoteFetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,682] INFO [ExpirationReaper-4-RemoteFetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,683] INFO [ExpirationReaper-4-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,684] INFO [ExpirationReaper-4-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,684] INFO [ExpirationReaper-4-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,685] INFO [ExpirationReaper-4-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,686] INFO [ExpirationReaper-4-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,686] INFO [ExpirationReaper-4-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,687] INFO [ExpirationReaper-4-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,688] INFO [ExpirationReaper-4-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,689] INFO [ExpirationReaper-4-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,698] INFO [AddPartitionsToTxnSenderThread-4]: Shutting down (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:16:22,698] INFO [AddPartitionsToTxnSenderThread-4]: Stopped (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:16:22,698] INFO [AddPartitionsToTxnSenderThread-4]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:16:22,700] INFO [ReplicaManager broker=4] Shut down completely (kafka.server.ReplicaManager)
[2025-05-20 22:16:22,701] INFO [broker-4-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,701] INFO [broker-4-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,701] INFO [broker-4-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,702] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:16:22,703] INFO [broker-4-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,703] INFO [broker-4-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,703] INFO [broker-4-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,704] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:16:22,705] INFO Shutting down. (kafka.log.LogManager)
[2025-05-20 22:16:22,707] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
[2025-05-20 22:16:22,708] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:16:22,708] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:16:22,708] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:16:22,721] INFO [ProducerStateManager partition=__consumer_offsets-29] Wrote producer snapshot at offset 2 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:16:22,829] INFO [ProducerStateManager partition=_schemas-0] Wrote producer snapshot at offset 2 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:16:22,922] INFO Shutdown complete. (kafka.log.LogManager)
[2025-05-20 22:16:22,924] INFO [broker-4-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,927] INFO [broker-4-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,927] INFO [broker-4-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,928] INFO [broker-4-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,930] INFO [broker-4-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,930] INFO [broker-4-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,931] INFO [broker-4-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,932] INFO [broker-4-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,932] INFO [broker-4-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,933] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,933] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,934] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,939] INFO [SocketServer listenerType=BROKER, nodeId=4] Shutting down socket server (kafka.network.SocketServer)
[2025-05-20 22:16:22,957] INFO [SocketServer listenerType=BROKER, nodeId=4] Shutdown completed (kafka.network.SocketServer)
[2025-05-20 22:16:22,959] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
[2025-05-20 22:16:22,962] INFO [BrokerLifecycleManager id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,965] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:16:22,969] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:16:22,969] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:16:22,973] INFO [SharedServer id=4] Stopping SharedServer (kafka.server.SharedServer)
[2025-05-20 22:16:22,974] INFO [MetadataLoader id=4] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,975] INFO [SnapshotGenerator id=4] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,976] INFO [SnapshotGenerator id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,976] INFO [MetadataLoader id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,978] INFO [SnapshotGenerator id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,979] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:16:23,112] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:16:23,112] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:16:23,114] INFO [kafka-4-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:16:23,114] INFO [RaftManager id=4] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:16:23,115] INFO [RaftManager id=4] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:16:23,115] INFO [RaftManager id=4] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:16:23,116] INFO [kafka-4-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:16:23,116] INFO [kafka-4-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:16:23,121] INFO [kafka-4-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:16:23,121] INFO [kafka-4-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:16:23,121] INFO [kafka-4-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:16:23,126] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 1821 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:16:23,130] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:16:23,130] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:16:23,131] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:16:23,131] INFO App info kafka.server for 4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:16:23,132] INFO [BrokerServer id=4] shut down completed (kafka.server.BrokerServer)
[2025-05-20 22:16:23,132] INFO [BrokerServer id=4] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
[2025-05-20 22:16:23,133] INFO App info kafka.server for 4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:21:35,539] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 22:21:35,835] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 22:21:35,858] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 22:21:35,862] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:41,431] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 22:21:41,548] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 22:21:41,553] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:41,747] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:41,761] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:41,803] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 22:21:41,814] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-20 22:21:41,816] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2025-05-20 22:21:41,824] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:41,897] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
[2025-05-20 22:21:41,911] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:41,917] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:41,918] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000001821.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-20 22:21:41,919] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:42,044] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 1821 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:21:42,058] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 1821 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:42,059] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 1821 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:42,059] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=1821, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000001821.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:21:42,064] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 5ms for snapshot load and 0ms for segment recovery from offset 1821 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:42,082] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-20 22:21:42,102] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:21:42,107] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:21:42,160] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:21:42,227] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from null (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:42,229] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:21:42,249] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:21:42,411] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,431] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2025-05-20 22:21:42,434] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:42,469] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:21:42,475] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:21:42,471] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:21:42,483] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:21:42,521] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,537] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 22:21:42,537] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 22:21:42,550] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1622577965 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:21:42,552] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:42,553] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:42,577] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:21:42,624] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,734] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,768] INFO [RaftManager id=4] Completed transition to Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:42,849] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,954] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,986] INFO [RaftManager id=4] Completed transition to Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:43,055] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,072] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 22:21:43,098] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-20 22:21:43,100] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 22:21:43,116] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-20 22:21:43,119] INFO [RaftManager id=4] Completed transition to Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:43,170] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:43,170] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,184] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:43,193] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,198] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,205] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,207] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,224] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,225] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,225] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,280] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,326] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:43,327] INFO [BrokerLifecycleManager id=4] Incarnation 42wbKEKPQGi8TOnlU7_9jA of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:43,341] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,370] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,371] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 22:21:43,372] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 22:21:43,372] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 22:21:43,471] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,572] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,673] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,775] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,838] INFO [RaftManager id=4] Completed transition to Unattached(epoch=12, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:43,876] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,976] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,077] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,120] INFO [RaftManager id=4] Completed transition to Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=12, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:44,178] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,182] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=13, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:44,189] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,230] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,278] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,280] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,381] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,382] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:21:44,384] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,434] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,482] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,501] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=1825, metadata=Optional.empty)] for the first time for epoch 13 (org.apache.kafka.raft.FollowerState)
[2025-05-20 22:21:44,505] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 1826 (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:44,510] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 1825 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,701] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 1824, but the high water mark is 1829 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,726] INFO [MetadataLoader id=4] initializeNewPublishers: The loader is still catching up because we have loaded up to offset 1824, but the high water mark is 1829 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,747] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 1828, but the high water mark is 1830 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,774] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 1830 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,782] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 1829 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,788] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:44,791] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 22:21:44,793] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 22:21:44,791] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 1829 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,794] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 1829 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,795] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=1829, epoch=13) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-20 22:21:44,799] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:44,809] INFO Skipping recovery of 51 logs from /tmp/kafka-logs since clean shutdown file was found (kafka.log.LogManager)
[2025-05-20 22:21:44,811] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:44,863] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:44,864] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:44,865] INFO [ProducerStateManager partition=__consumer_offsets-29] Loading producer state from snapshot file 'SnapshotFile(offset=2, file=/tmp/kafka-logs/__consumer_offsets-29/00000000000000000002.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:21:44,865] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:44,872] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-29, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=29, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=2) with 1 segments, local-log-start-offset 0 and log-end-offset 2 in 34ms (1/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:44,878] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:44,887] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-38, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=38, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (2/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:44,906] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:44,911] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-27, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=27, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (3/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:44,921] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:44,931] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-39, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=39, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (4/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:44,944] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:44,948] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-42, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=42, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (5/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:44,958] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:44,966] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-9, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (6/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:44,972] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:44,976] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-22, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=22, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (7/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:44,987] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:44,989] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-26, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=26, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (8/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,000] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,006] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-40, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=40, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 17ms (9/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,016] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,031] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-47, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=47, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 19ms (10/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,037] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,044] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-15, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (11/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,050] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,058] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-10, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (12/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,075] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,082] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-2, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (13/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,100] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,103] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-18, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (14/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,112] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,118] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-46, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=46, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (15/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,121] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,124] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-12, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (16/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,148] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,153] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-23, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=23, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 28ms (17/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,160] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,165] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-14, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (18/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,170] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,172] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-5, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (19/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,175] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,178] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-41, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=41, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (20/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,183] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,186] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-1, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (21/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,190] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,193] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-19, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (22/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,200] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,205] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-3, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (23/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,230] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,235] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-16, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 25ms (24/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,242] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,244] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-25, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=25, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (25/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,258] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,260] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,263] INFO [ProducerStateManager partition=_schemas-0] Loading producer state from snapshot file 'SnapshotFile(offset=2, file=/tmp/kafka-logs/_schemas-0/00000000000000000002.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:21:45,263] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,281] INFO Completed load of Log(dir=/tmp/kafka-logs/_schemas-0, topicId=RrE8eovWRKu4kLR3MRJ0fA, topic=_schemas, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=2) with 1 segments, local-log-start-offset 0 and log-end-offset 2 in 31ms (26/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,286] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,291] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-28, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=28, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (27/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,304] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,318] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-13, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 24ms (28/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,321] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,326] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-24, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=24, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (29/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,340] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,350] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-32, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=32, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (30/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,371] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,373] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-31, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=31, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 20ms (31/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,377] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,379] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-21, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=21, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (32/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,384] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,389] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-6, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (33/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,393] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,396] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-11, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (34/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,403] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,407] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-30, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=30, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (35/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,411] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,413] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-43, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=43, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (36/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,431] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,432] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-7, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (37/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,438] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,440] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-33, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=33, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (38/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,454] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,458] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-36, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=36, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 16ms (39/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,462] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,474] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-45, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=45, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (40/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,478] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,480] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-0, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (41/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,488] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,492] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-20, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=20, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (42/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,496] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,497] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-4, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (43/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,504] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,509] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-8, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (44/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,521] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,526] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-49, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=49, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (45/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,542] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,545] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-34, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=34, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (46/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,562] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,573] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-48, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=48, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 24ms (47/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,592] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,598] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-44, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=44, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (48/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,618] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,621] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-37, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=37, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 20ms (49/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,642] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,643] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-35, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=35, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 21ms (50/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,655] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,662] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-17, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (51/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:45,672] INFO Loaded 51 logs in 872ms (kafka.log.LogManager)
[2025-05-20 22:21:45,674] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-20 22:21:45,680] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-20 22:21:45,687] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-20 22:21:45,909] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:21:45,919] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:21:45,925] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:45,922] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:21:45,941] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:45,944] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:21:45,968] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:21:45,998] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:21:46,024] INFO [Broker id=4] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:21:46,037] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,080] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,085] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,089] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,094] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,096] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,101] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,103] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,104] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,104] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,105] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,106] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,115] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,117] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,120] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,127] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,134] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,139] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,139] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,141] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,142] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,143] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,144] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,146] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,147] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,149] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,154] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,154] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,156] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,159] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,160] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,161] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,162] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,162] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,163] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,164] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,165] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,167] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,173] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,179] INFO [Broker id=4] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-20 22:21:46,181] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 2 (kafka.cluster.Partition)
[2025-05-20 22:21:46,181] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 2 from offset 2 with partition epoch 3 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,182] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,183] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,184] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,200] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,202] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,203] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,203] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,205] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,205] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,207] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,208] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,210] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,210] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,212] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,214] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,215] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,217] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,217] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,218] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,219] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,219] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,219] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,220] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 2 (kafka.cluster.Partition)
[2025-05-20 22:21:46,222] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 2 from offset 2 with partition epoch 3 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,223] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,227] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,231] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,231] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,233] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,233] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,233] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,236] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,250] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,252] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,260] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,262] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,263] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,265] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,274] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,276] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,277] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,277] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,278] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,279] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,284] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,288] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,293] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,295] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,295] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,296] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,297] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,298] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,300] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,300] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,300] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,302] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,302] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,303] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,306] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,306] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,307] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,310] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,312] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,313] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,315] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,316] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,317] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,319] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,320] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,325] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,342] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,342] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,344] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,346] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,346] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,348] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,349] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,349] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,350] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,351] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,354] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,357] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,363] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,365] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,366] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,367] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,368] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,368] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,369] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,369] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,370] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,371] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,371] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,371] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,373] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,374] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,375] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,376] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,376] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,377] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,379] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,379] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,379] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,381] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,382] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,382] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,383] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,390] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,390] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,391] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,392] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,397] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:21:46,399] INFO [Broker id=4] Stopped fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 22:21:46,405] INFO [Broker id=4] Started fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 22:21:46,424] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,425] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,426] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,426] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,427] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,429] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,430] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,430] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,431] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,431] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,432] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,433] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,433] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,434] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,434] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,435] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,437] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,438] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,438] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,438] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,439] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,439] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,440] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,442] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,445] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,446] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,429] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,449] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,452] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,453] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,453] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,454] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,454] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,455] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,456] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,457] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,458] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,459] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,460] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,450] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,461] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,463] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,464] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,465] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,466] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,466] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,467] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,468] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,469] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,470] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,470] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,471] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,471] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,472] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,461] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,473] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,473] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,474] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,473] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,480] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,479] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,482] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,484] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,487] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,488] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,489] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,487] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,494] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,495] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,496] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,497] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,498] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,496] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,501] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,502] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,502] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,502] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,507] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,504] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,508] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,508] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,509] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,512] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,515] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,522] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,523] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,524] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,526] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,529] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,530] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,530] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,531] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,537] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,538] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,539] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,539] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,539] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,539] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,509] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,540] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,542] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,542] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,543] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,544] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,540] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,544] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,545] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,546] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,544] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,549] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,549] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,550] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,550] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,550] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,549] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,556] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,563] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,564] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,564] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,564] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,564] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,566] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,566] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,570] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,570] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,570] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,571] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,571] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,575] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,577] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,577] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,579] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,581] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,582] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,584] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,584] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,585] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,574] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,585] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,586] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,586] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,587] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,587] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,587] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,585] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,588] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,589] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,589] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,589] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,590] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,599] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 22:21:46,605] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 22:21:46,620] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 1829 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:46,624] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 22:21:46,631] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 22:21:46,660] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:46,681] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 22:21:46,755] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:46,756] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 22:21:46,759] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 22:21:46,759] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 22:21:46,761] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2025-05-20 22:21:46,764] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 22:21:46,768] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 22:21:46,772] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 22:21:46,773] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 22:21:46,774] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 22:21:46,774] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 22:21:46,775] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-20 22:21:46,775] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:21:46,776] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:21:46,780] INFO Kafka startTimeMs: 1747779706775 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:21:46,797] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-20 22:21:47,475] INFO [Broker id=4] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:21:47,477] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,479] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,482] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,483] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,483] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,484] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,484] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,485] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,485] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,489] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,494] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,503] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,504] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,505] INFO [Broker id=4] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,548] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,549] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,550] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,551] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,551] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,552] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,552] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,552] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,553] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,553] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,554] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,555] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,556] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,557] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,558] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,558] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,559] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,568] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,575] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,583] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,586] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,586] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,587] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,587] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,588] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,588] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,590] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,591] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,592] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,594] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,597] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,600] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,601] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,603] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,606] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,606] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,620] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,636] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,638] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,639] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,639] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,642] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,642] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,643] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,643] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,645] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,645] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,647] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,649] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,649] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,649] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,655] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,655] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,656] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,659] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,660] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,660] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,661] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,665] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,665] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,666] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,667] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,659] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,669] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,670] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,673] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,675] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,668] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,685] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,689] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,690] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,699] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,730] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,736] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,736] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,784] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,761] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,803] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,809] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,818] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,852] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,855] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,856] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,857] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,858] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,858] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,858] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,868] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,874] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,874] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,878] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,888] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,892] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,908] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,913] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,921] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,921] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,921] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,922] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,922] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,925] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,926] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,937] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,940] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,943] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,944] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,948] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,950] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,809] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,959] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,977] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,977] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,978] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,978] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,979] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,979] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,979] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,979] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,980] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,980] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,991] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,993] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,006] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,969] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,033] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,034] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,035] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,035] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,035] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,035] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,042] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,054] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,054] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,055] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,055] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,055] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,055] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,057] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,058] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,058] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,058] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,058] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,059] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,059] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,059] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,059] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,059] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,060] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,060] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,060] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,060] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,061] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,064] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,012] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,076] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,077] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,077] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,083] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,084] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,086] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,087] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,087] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,087] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,088] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,104] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,107] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,108] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,109] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,109] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,074] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,123] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,125] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,126] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,128] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,128] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,128] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,129] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,129] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,129] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,130] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:48,148] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,126] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,165] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,165] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,167] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,176] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:48,179] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:24:30,894] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 22:24:30,896] INFO [BrokerServer id=4] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
[2025-05-20 22:24:30,897] INFO [BrokerServer id=4] shutting down (kafka.server.BrokerServer)
[2025-05-20 22:24:30,898] INFO [BrokerLifecycleManager id=4] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:30,936] INFO [BrokerLifecycleManager id=4] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:30,936] INFO [BrokerLifecycleManager id=4] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:24:30,937] INFO [BrokerLifecycleManager id=4] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:30,938] INFO [broker-4-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:30,938] INFO [broker-4-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:30,938] INFO [broker-4-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:30,939] INFO [SocketServer listenerType=BROKER, nodeId=4] Stopping socket server request processors (kafka.network.SocketServer)
[2025-05-20 22:24:30,944] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:24:30,952] INFO [SocketServer listenerType=BROKER, nodeId=4] Stopped socket server request processors (kafka.network.SocketServer)
[2025-05-20 22:24:30,954] INFO [data-plane Kafka Request Handler on Broker 4], shutting down (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 22:24:30,956] INFO [data-plane Kafka Request Handler on Broker 4], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 22:24:30,956] INFO [ExpirationReaper-4-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:30,958] INFO [ExpirationReaper-4-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:30,958] INFO [ExpirationReaper-4-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:30,960] INFO [KafkaApi-4] Shutdown complete. (kafka.server.KafkaApis)
[2025-05-20 22:24:30,963] INFO [TransactionCoordinator id=4] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:24:30,964] INFO [Transaction State Manager 4]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2025-05-20 22:24:30,965] INFO [TxnMarkerSenderThread-4]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:24:30,965] INFO [TxnMarkerSenderThread-4]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:24:30,965] INFO [TxnMarkerSenderThread-4]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:24:30,968] INFO [TransactionCoordinator id=4] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:24:30,969] INFO [GroupCoordinator 4]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:24:30,970] INFO [ExpirationReaper-4-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:30,971] INFO [ExpirationReaper-4-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:30,971] INFO [ExpirationReaper-4-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:30,972] INFO [ExpirationReaper-4-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:30,974] INFO [ExpirationReaper-4-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:30,974] INFO [ExpirationReaper-4-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:30,978] INFO [GroupCoordinator 4]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:24:30,979] INFO [AssignmentsManager id=4]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:24:30,987] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:30,993] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:30,993] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:30,994] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:24:30,995] INFO [AssignmentsManager id=4]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:24:30,996] INFO [ReplicaManager broker=4] Shutting down (kafka.server.ReplicaManager)
[2025-05-20 22:24:30,997] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:24:30,997] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:24:30,997] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:24:30,999] INFO [ReplicaFetcherManager on broker 4] shutting down (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:24:31,001] INFO [ReplicaFetcherManager on broker 4] shutdown completed (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:24:31,002] INFO [ReplicaAlterLogDirsManager on broker 4] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:24:31,004] INFO [ReplicaAlterLogDirsManager on broker 4] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:24:31,006] INFO [ExpirationReaper-4-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,007] INFO [ExpirationReaper-4-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,007] INFO [ExpirationReaper-4-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,010] INFO [ExpirationReaper-4-RemoteFetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,015] INFO [ExpirationReaper-4-RemoteFetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,015] INFO [ExpirationReaper-4-RemoteFetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,017] INFO [ExpirationReaper-4-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,018] INFO [ExpirationReaper-4-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,018] INFO [ExpirationReaper-4-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,019] INFO [ExpirationReaper-4-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,020] INFO [ExpirationReaper-4-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,020] INFO [ExpirationReaper-4-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,022] INFO [ExpirationReaper-4-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,023] INFO [ExpirationReaper-4-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,023] INFO [ExpirationReaper-4-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,031] INFO [AddPartitionsToTxnSenderThread-4]: Shutting down (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:24:31,032] INFO [AddPartitionsToTxnSenderThread-4]: Stopped (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:24:31,032] INFO [AddPartitionsToTxnSenderThread-4]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:24:31,033] INFO [ReplicaManager broker=4] Shut down completely (kafka.server.ReplicaManager)
[2025-05-20 22:24:31,034] INFO [broker-4-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:31,035] INFO [broker-4-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:31,035] INFO [broker-4-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:31,036] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:24:31,036] INFO [broker-4-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:31,037] INFO [broker-4-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:31,037] INFO [broker-4-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:31,038] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:24:31,039] INFO Shutting down. (kafka.log.LogManager)
[2025-05-20 22:24:31,041] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
[2025-05-20 22:24:31,042] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:24:31,042] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:24:31,042] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:24:31,206] INFO Shutdown complete. (kafka.log.LogManager)
[2025-05-20 22:24:31,208] INFO [broker-4-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,209] INFO [broker-4-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,209] INFO [broker-4-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,209] INFO [broker-4-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,210] INFO [broker-4-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,210] INFO [broker-4-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,210] INFO [broker-4-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,211] INFO [broker-4-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,211] INFO [broker-4-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,212] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,212] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,212] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,216] INFO [SocketServer listenerType=BROKER, nodeId=4] Shutting down socket server (kafka.network.SocketServer)
[2025-05-20 22:24:31,230] INFO [SocketServer listenerType=BROKER, nodeId=4] Shutdown completed (kafka.network.SocketServer)
[2025-05-20 22:24:31,232] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
[2025-05-20 22:24:31,232] INFO [BrokerLifecycleManager id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:24:31,233] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:24:31,234] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:24:31,234] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:24:31,236] INFO [SharedServer id=4] Stopping SharedServer (kafka.server.SharedServer)
[2025-05-20 22:24:31,236] INFO [MetadataLoader id=4] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:24:31,237] INFO [SnapshotGenerator id=4] close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:24:31,237] INFO [SnapshotGenerator id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:24:31,241] INFO [MetadataLoader id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:24:31,243] INFO [SnapshotGenerator id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:24:31,243] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:24:31,267] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:24:31,267] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:24:31,269] INFO [kafka-4-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:24:31,269] INFO [RaftManager id=4] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:24:31,270] INFO [RaftManager id=4] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:24:31,270] INFO [RaftManager id=4] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:24:31,270] INFO [kafka-4-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:24:31,270] INFO [kafka-4-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:24:31,275] INFO [kafka-4-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:24:31,275] INFO [kafka-4-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:24:31,275] INFO [kafka-4-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:24:31,279] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2219 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:24:31,282] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:24:31,283] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:24:31,283] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:24:31,284] INFO App info kafka.server for 4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:24:31,284] INFO [BrokerServer id=4] shut down completed (kafka.server.BrokerServer)
[2025-05-20 22:24:31,285] INFO [BrokerServer id=4] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
[2025-05-20 22:24:31,285] INFO App info kafka.server for 4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:24:50,661] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 22:24:51,070] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 22:24:51,103] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 22:24:51,116] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:24:56,649] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 22:24:56,760] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 22:24:56,771] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:24:56,967] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:24:56,984] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:24:57,040] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 22:24:57,051] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-20 22:24:57,054] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2025-05-20 22:24:57,058] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:24:57,149] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
[2025-05-20 22:24:57,153] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:57,155] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:57,156] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000001821.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-20 22:24:57,157] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000002219.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-20 22:24:57,158] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:57,311] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2219 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:24:57,320] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 2219 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:57,321] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2219 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:57,321] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=2219, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000002219.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:24:57,323] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 2219 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:57,336] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-20 22:24:57,355] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:24:57,361] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:24:57,478] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:24:57,606] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=13, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from null (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:24:57,632] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:24:57,665] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:24:57,694] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:57,695] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2025-05-20 22:24:57,715] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:24:57,739] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:57,745] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:57,761] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:57,776] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:57,824] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:57,862] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 22:24:57,872] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 22:24:57,893] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1616152967 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:24:57,935] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:57,935] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:57,943] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:57,951] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:24:58,039] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,133] INFO [RaftManager id=4] Completed transition to Unattached(epoch=15, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=13, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:24:58,155] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:24:58,156] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,159] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:24:58,216] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:24:58,224] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:24:58,260] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,362] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,470] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,571] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,572] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=15, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=15, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:24:58,673] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,710] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 22:24:58,775] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,780] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-20 22:24:58,783] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 22:24:58,836] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-20 22:24:58,856] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:58,857] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:58,885] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,916] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:58,918] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:58,941] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:58,956] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:58,981] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:58,995] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:58,989] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,985] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:59,047] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:59,059] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:59,099] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,106] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=2224, metadata=Optional.empty)] for the first time for epoch 15 (org.apache.kafka.raft.FollowerState)
[2025-05-20 22:24:59,140] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 2224 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,240] INFO [BrokerLifecycleManager id=4] Incarnation z_x4JRYRS5q8vgQ65Q3lfw of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:59,258] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:59,245] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:59,261] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:59,350] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 2227 (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:59,416] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 22:24:59,454] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 2223, but the high water mark is 2227 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,457] INFO [MetadataLoader id=4] initializeNewPublishers: The loader is still catching up because we have loaded up to offset 2223, but the high water mark is 2227 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,459] INFO [MetadataLoader id=4] initializeNewPublishers: The loader is still catching up because we have loaded up to offset 2223, but the high water mark is 2227 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,459] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 22:24:59,492] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 22:24:59,502] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 2226, but the high water mark is 2228 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,504] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 2228 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,513] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 2227 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,515] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 2227 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,516] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 2227 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,518] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=2227, epoch=15) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-20 22:24:59,522] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:24:59,538] INFO Skipping recovery of 51 logs from /tmp/kafka-logs since clean shutdown file was found (kafka.log.LogManager)
[2025-05-20 22:24:59,551] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:59,568] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 22:24:59,569] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 22:24:59,577] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,585] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,603] INFO [ProducerStateManager partition=__consumer_offsets-29] Loading producer state from snapshot file 'SnapshotFile(offset=2, file=/tmp/kafka-logs/__consumer_offsets-29/00000000000000000002.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:24:59,606] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 0ms for segment recovery from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,586] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:59,701] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-29, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=29, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=2) with 1 segments, local-log-start-offset 0 and log-end-offset 2 in 130ms (1/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:24:59,725] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,744] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-38, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=38, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 40ms (2/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:24:59,781] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,790] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-27, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=27, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (3/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:24:59,794] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,800] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-39, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=39, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (4/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:24:59,809] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,837] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-42, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=42, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 31ms (5/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:24:59,885] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,920] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-9, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 54ms (6/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:24:59,936] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,966] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-22, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=22, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 41ms (7/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,004] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,014] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-26, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=26, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 39ms (8/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,039] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,063] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-40, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=40, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 39ms (9/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,074] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,082] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-47, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=47, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 17ms (10/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,087] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,115] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-15, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 32ms (11/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,121] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,125] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-10, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (12/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,170] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,173] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-2, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 42ms (13/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,196] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,206] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-18, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 27ms (14/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,261] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,265] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-46, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=46, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 55ms (15/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,278] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,301] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-12, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 26ms (16/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,335] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,343] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-23, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=23, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 30ms (17/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,355] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,372] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-14, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 20ms (18/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,391] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,400] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-5, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (19/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,432] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,439] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-41, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=41, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 32ms (20/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,457] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,458] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-1, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 19ms (21/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,466] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,473] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-19, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (22/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,490] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,497] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-3, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (23/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,516] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,526] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-16, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (24/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,546] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,547] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-25, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=25, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 21ms (25/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,560] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,567] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,568] INFO [ProducerStateManager partition=_schemas-0] Loading producer state from snapshot file 'SnapshotFile(offset=2, file=/tmp/kafka-logs/_schemas-0/00000000000000000002.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:25:00,569] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,573] INFO Completed load of Log(dir=/tmp/kafka-logs/_schemas-0, topicId=RrE8eovWRKu4kLR3MRJ0fA, topic=_schemas, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=2) with 1 segments, local-log-start-offset 0 and log-end-offset 2 in 24ms (26/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,579] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,581] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-28, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=28, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (27/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,584] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,587] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-13, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (28/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,597] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,600] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-24, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=24, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (29/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,610] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,612] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-32, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=32, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (30/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,616] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,626] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-31, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=31, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (31/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,637] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,640] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-21, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=21, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (32/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,645] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,650] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-6, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (33/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,660] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,663] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-11, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (34/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,667] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,668] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-30, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=30, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (35/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,682] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,685] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-43, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=43, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (36/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,701] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,703] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-7, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (37/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,706] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,710] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-33, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=33, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (38/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,720] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,723] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-36, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=36, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (39/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,727] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,729] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-45, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=45, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (40/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,732] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,736] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-0, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (41/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,741] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,743] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-20, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=20, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (42/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,746] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,755] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-4, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (43/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,761] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,762] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-8, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (44/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,765] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,783] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-49, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=49, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 20ms (45/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,791] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,792] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-34, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=34, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (46/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,796] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,798] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-48, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=48, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (47/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,802] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,808] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-44, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=44, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (48/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,815] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,817] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-37, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=37, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (49/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,825] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,830] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-35, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=35, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (50/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,835] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,848] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-17, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 16ms (51/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,858] INFO Loaded 51 logs in 1330ms (kafka.log.LogManager)
[2025-05-20 22:25:00,871] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-20 22:25:00,873] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-20 22:25:00,880] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-20 22:25:01,080] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:25:01,084] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:25:01,087] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:25:01,092] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,098] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,099] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:25:01,105] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:25:01,106] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:25:01,111] INFO [Broker id=4] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:01,113] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,126] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,128] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,129] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,130] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,130] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,131] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,132] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,134] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,135] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,138] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,138] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,140] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,144] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,145] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,146] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,148] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,148] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,149] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,150] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,150] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,151] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,152] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,154] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,154] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,160] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,161] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,162] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,166] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,167] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,167] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,169] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,170] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,171] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,174] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,176] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,183] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,188] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,189] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,189] INFO [Broker id=4] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-20 22:25:01,191] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 2 (kafka.cluster.Partition)
[2025-05-20 22:25:01,192] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 2 from offset 2 with partition epoch 4 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,193] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,198] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,199] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,200] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,203] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,205] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,205] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,207] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,208] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,210] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,214] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,215] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,216] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,218] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,221] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,224] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,232] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,233] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,234] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,235] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,236] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,236] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,237] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 2 (kafka.cluster.Partition)
[2025-05-20 22:25:01,238] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 2 from offset 2 with partition epoch 4 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,238] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,240] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,242] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,242] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,245] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,248] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,249] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,251] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,253] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,255] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,259] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,260] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,261] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,263] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,263] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,266] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,268] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,269] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,269] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,270] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,272] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,272] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,274] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,275] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,277] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,280] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,281] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,282] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,283] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,284] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,284] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,285] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,285] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,286] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,288] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,288] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,289] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,290] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,291] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,291] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,293] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,293] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,294] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,295] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,295] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,296] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,297] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,297] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,298] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,300] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,300] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,300] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,303] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,304] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,304] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,305] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,306] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,306] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,307] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,308] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,308] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,313] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,313] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,313] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,315] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,316] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,317] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,318] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,318] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,319] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,323] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,324] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,324] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,326] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,327] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,327] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,328] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,329] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,330] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,333] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,334] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,334] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,335] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,336] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,338] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,339] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,343] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,345] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:01,346] INFO [Broker id=4] Stopped fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 22:25:01,363] INFO [Broker id=4] Started fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 22:25:01,380] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,381] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,382] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,382] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,382] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,383] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,383] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,383] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,384] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,384] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,385] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,386] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,387] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,387] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,389] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,385] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,391] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,390] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,392] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,392] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,393] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,393] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,393] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,394] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,394] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,395] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,395] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,395] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,396] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,396] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,396] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,397] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,397] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,398] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,397] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,398] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,399] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,398] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,399] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,399] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,400] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,401] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,402] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,401] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,402] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,402] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,403] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,404] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,404] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,404] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,405] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,405] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,405] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,406] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,406] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,406] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,408] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,408] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,408] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,408] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,408] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,409] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,409] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,409] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,410] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,410] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,410] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,411] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,411] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,411] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,412] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,412] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,413] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,412] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,413] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,414] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,414] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,415] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,415] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,415] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,416] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,416] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,416] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,416] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,416] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,417] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,417] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,417] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,418] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,418] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,418] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,420] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,423] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,423] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,423] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,425] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,425] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,425] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,425] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,426] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,426] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,428] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,428] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,428] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,429] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,430] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,430] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,431] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,430] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,431] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,432] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,432] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,432] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,433] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,433] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,433] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,435] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,436] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,435] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,437] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,437] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,438] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,439] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,439] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,439] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,440] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,440] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,440] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,441] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,441] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,441] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,441] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,442] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,442] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,442] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,442] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,442] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,443] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,443] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,443] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,444] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,444] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,444] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,444] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,445] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,445] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,446] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,445] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,447] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,448] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,453] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 22:25:01,460] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 22:25:01,468] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 22:25:01,468] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 2227 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:25:01,470] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 22:25:01,472] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:25:01,477] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 22:25:01,554] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:25:01,555] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 22:25:01,558] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 22:25:01,559] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 22:25:01,583] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2025-05-20 22:25:01,586] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 22:25:01,591] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 22:25:01,607] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 22:25:01,607] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 22:25:01,608] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 22:25:01,608] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 22:25:01,609] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-20 22:25:01,610] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:25:01,611] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:25:01,611] INFO Kafka startTimeMs: 1747779901610 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:25:01,619] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-20 22:25:02,470] INFO [Broker id=4] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:02,474] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,475] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,477] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,478] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,479] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,480] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,480] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,481] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,482] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,483] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,485] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,486] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,488] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,489] INFO [Broker id=4] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,490] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,491] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,493] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,495] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,496] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,497] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,498] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,499] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,500] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,500] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,501] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,501] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,502] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,503] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,504] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,506] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,507] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,508] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,508] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,508] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,509] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,511] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,512] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,513] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,514] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,514] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,515] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,515] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,516] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,516] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,516] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,517] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,517] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,518] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,518] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,519] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,519] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,521] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,521] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,521] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,522] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,522] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,524] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,524] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,524] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,526] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,526] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,526] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,526] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,527] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,528] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,528] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,528] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,529] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,529] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,529] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,529] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,530] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,530] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,531] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,532] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,532] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,533] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,533] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,534] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,534] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,534] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,534] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,535] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,535] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,535] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,536] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,536] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,537] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,537] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,538] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,538] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,538] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,538] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,539] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,539] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,540] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,541] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,541] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,541] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,541] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,542] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,542] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,542] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,542] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,542] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,543] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,543] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,543] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,544] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,544] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,544] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,545] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,545] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,545] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,545] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,546] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,546] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,547] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,547] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,548] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,549] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,550] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,550] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,552] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,552] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,551] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,553] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,552] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,555] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,555] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,555] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,556] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,556] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,557] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,557] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,557] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,557] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,558] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,559] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,559] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,559] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,559] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,560] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,559] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,560] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,560] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,561] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,562] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,562] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,562] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,563] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,563] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,564] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,564] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,565] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,565] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,565] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,567] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,573] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,573] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,577] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,587] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,587] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,587] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,589] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,588] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,592] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,593] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,594] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,597] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,598] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,598] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,599] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,598] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,600] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,600] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,602] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,602] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,603] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,608] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,609] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,609] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,612] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,612] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,614] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,614] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,614] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,615] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,616] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,616] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,617] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,618] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,619] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,618] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,620] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,621] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,620] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,622] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,621] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,624] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,625] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,629] INFO [Broker id=4] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:02,631] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,633] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,634] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,635] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,640] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,643] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,644] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,645] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,647] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,652] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,654] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,654] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,655] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,656] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 3 from offset 2 with partition epoch 6 and high watermark 2. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,657] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,659] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,660] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,661] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,661] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,662] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,663] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,663] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 3 from offset 2 with partition epoch 6 and high watermark 2. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,664] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,666] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,667] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,669] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,671] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,672] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,672] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,673] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,674] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,674] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,675] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,676] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,676] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,678] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,679] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,680] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,681] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,681] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,682] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,682] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,683] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,684] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,684] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,684] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,686] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,687] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,688] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,689] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,690] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,693] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, _schemas-0, __consumer_offsets-14, __consumer_offsets-41, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:02,694] INFO [Broker id=4] Stopped fetchers as part of become-follower for 35 partitions (state.change.logger)
[2025-05-20 22:25:02,718] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,723] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-48 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-9 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-42 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-28 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-38 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-1 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-47 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), _schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,2), __consumer_offsets-14 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-41 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-22 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-20 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-31 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,2), __consumer_offsets-25 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-8 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-35 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-4 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:02,724] INFO [Broker id=4] Started fetchers as part of become-follower for 35 partitions (state.change.logger)
[2025-05-20 22:25:02,726] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,728] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,730] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,731] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,731] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,732] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,733] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,733] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,733] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,734] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,734] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,735] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,735] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,736] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,737] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,738] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,738] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,739] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,740] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,740] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,741] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,741] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,742] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,742] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,743] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,743] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,744] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,744] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,744] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,744] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,745] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,745] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,746] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,746] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,746] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,746] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,747] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,747] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,747] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,748] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,748] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,748] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,749] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,749] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,749] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,750] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,750] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,750] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,750] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,751] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,751] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,751] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,751] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,752] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,752] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,752] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,753] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,753] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,754] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,754] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,754] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,755] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,755] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,755] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,756] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,756] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,764] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,765] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,765] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,765] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,765] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,766] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,767] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,766] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,767] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,768] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,768] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,769] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,769] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,769] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,770] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,770] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,770] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,771] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,771] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,772] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,773] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,773] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,773] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,774] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,774] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,774] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,774] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,775] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,775] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,776] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,776] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,777] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,778] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,778] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,778] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,779] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,779] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,779] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,780] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,779] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,780] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,781] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,781] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,781] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,782] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,781] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,783] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,783] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,784] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,785] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,785] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,785] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,786] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,786] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,786] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,787] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,787] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,788] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,788] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,788] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,789] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,788] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,789] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,789] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,790] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,790] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,790] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,791] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,792] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,792] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,792] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,793] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,793] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,793] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,794] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,794] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,794] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,795] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,794] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,795] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,796] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,796] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,796] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,797] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,797] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,797] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,798] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,799] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,799] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,799] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,800] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,799] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,800] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,801] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,800] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,801] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,802] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,802] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,802] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,801] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,803] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,804] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,803] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,805] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,805] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,805] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,806] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,806] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,807] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,807] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,806] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,808] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,808] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,809] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,809] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,809] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,810] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,810] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,810] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,808] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,811] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,812] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,813] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,812] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,813] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,814] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,813] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,815] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,814] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,815] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,815] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,816] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,816] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,817] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,817] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,818] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,818] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,818] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,819] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,819] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,819] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,820] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,820] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,821] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,820] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,821] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,823] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,823] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,823] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,824] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,885] INFO [Broker id=4] Transitioning 2 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:02,886] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,887] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,888] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,889] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,889] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,889] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,889] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,892] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,993] INFO [Broker id=4] Transitioning 16 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:02,994] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,995] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:02,996] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:02,997] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:02,997] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:02,998] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,999] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,000] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,001] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,001] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,003] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,003] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,004] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,004] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,005] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,005] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,006] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,006] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,007] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,007] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,007] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,008] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,008] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,008] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,009] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,009] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,009] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,010] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,010] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,010] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,010] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,010] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,011] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,011] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,011] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,012] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,012] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,013] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,012] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,013] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,013] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,013] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,014] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,014] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,014] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,015] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,015] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,015] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,016] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,016] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,016] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,016] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,017] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,016] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,017] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,017] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,017] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,018] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,018] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,018] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,018] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,019] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,019] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,019] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,388] INFO [Broker id=4] Transitioning 16 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:03,389] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,390] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,391] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,392] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,392] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,393] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,393] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,394] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,395] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,395] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,396] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,397] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,398] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,399] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,400] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,401] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,403] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-45, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-0, __consumer_offsets-27, __consumer_offsets-40, __consumer_offsets-6, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-33) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:03,403] INFO [Broker id=4] Stopped fetchers as part of become-follower for 16 partitions (state.change.logger)
[2025-05-20 22:25:03,407] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,408] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-45 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-43 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-12 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-10 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-24 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-21 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-19 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-17 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-32 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-0 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-27 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-40 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-6 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-3 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-36 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-33 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:03,408] INFO [Broker id=4] Started fetchers as part of become-follower for 16 partitions (state.change.logger)
[2025-05-20 22:25:03,408] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,410] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,410] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,410] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,417] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,418] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,418] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,419] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,419] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,420] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,421] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,421] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,421] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,422] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,422] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,423] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,423] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,423] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,424] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,424] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,424] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,424] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,425] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,425] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,425] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,426] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,426] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,426] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,427] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,427] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,427] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,428] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,429] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,429] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,430] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,430] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,430] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,430] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,431] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,431] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,431] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,431] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,430] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,432] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,432] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,432] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,433] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,433] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,434] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,434] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,434] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,435] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,435] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,435] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,435] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,436] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,436] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,436] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,437] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,437] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,437] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,437] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,438] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,438] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,439] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,439] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,439] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,440] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,439] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,440] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,440] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,441] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,441] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,442] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,441] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,443] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,443] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,445] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,446] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,447] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,448] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:03,453] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,453] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,455] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,456] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,496] INFO [Broker id=4] Transitioning 17 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:03,497] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,498] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,498] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,499] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,499] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,499] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,499] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,500] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,500] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,502] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,502] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,503] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,503] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,504] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,504] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,505] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,505] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,506] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,506] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,507] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,507] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,515] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,526] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,526] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,526] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,527] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,527] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,527] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,527] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,528] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,528] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,528] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,528] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,529] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,529] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,529] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,530] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,530] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,531] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,531] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,531] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,532] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,532] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,533] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,532] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,534] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,534] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,534] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,535] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,536] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,536] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,536] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,536] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,537] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,537] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,538] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,538] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,538] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,538] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,539] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,539] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,539] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,540] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,540] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,540] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,541] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,540] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,541] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,556] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:03,557] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,558] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,558] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,559] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,875] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=0, leaderEpoch=0, endOffset=0) (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,876] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to offset 0 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,879] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:03,879] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:03,880] INFO Deleted producer state snapshot /tmp/kafka-logs/_schemas-0/00000000000000000002.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-20 22:25:03,880] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:03,881] WARN [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Non-monotonic update of high watermark from (offset=2, segment=[0:192]) to (offset=0, segment=[0:0]) (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,888] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=29, leaderEpoch=0, endOffset=0) (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,888] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to offset 0 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,889] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:03,889] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:03,890] INFO Deleted producer state snapshot /tmp/kafka-logs/__consumer_offsets-29/00000000000000000002.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-20 22:25:03,890] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:03,890] WARN [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Non-monotonic update of high watermark from (offset=2, segment=[0:633]) to (offset=0, segment=[0:0]) (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,895] INFO [Broker id=4] Transitioning 10 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:03,898] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,899] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,900] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,900] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,901] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,902] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,903] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,904] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,904] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,906] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,907] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,907] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,907] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,908] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,908] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,909] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,909] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,910] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,912] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,908] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,912] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,913] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,913] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,913] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,914] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,915] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,915] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,914] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,915] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,916] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,916] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,916] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,917] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,917] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,917] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,917] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,918] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,918] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,918] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,919] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,943] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:03,943] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,944] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,944] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,945] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,995] INFO [Broker id=4] Transitioning 16 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:03,996] INFO [Broker id=4] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,997] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,998] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,998] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,999] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,999] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,000] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,001] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,002] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,003] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,003] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,004] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,005] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,006] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,007] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,008] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,009] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,009] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,009] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,009] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,013] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,014] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,015] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,014] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,015] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,016] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,017] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,017] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,018] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,018] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,018] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,017] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,019] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,020] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,027] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,020] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,045] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,046] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,045] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,047] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,048] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,048] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,051] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,051] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,051] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,052] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,052] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,052] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,054] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,054] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,054] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,055] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,055] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,055] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,055] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,056] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,056] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,057] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,057] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,057] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,058] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,128] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:04,129] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,130] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,131] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,131] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,420] INFO [Broker id=4] Transitioning 6 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:04,421] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,421] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,422] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,422] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,422] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,423] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,423] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,423] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,426] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,431] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,426] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,435] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,432] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,437] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,437] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,438] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,439] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,439] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,440] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,441] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,437] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,443] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,444] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,444] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,488] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:04,491] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,492] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,493] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,493] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,583] INFO [Broker id=4] Transitioning 16 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:04,588] INFO [Broker id=4] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,595] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,596] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,597] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,598] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,600] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,601] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,602] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,603] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,603] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,604] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,604] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,604] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,607] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,608] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,608] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,609] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,610] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,611] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,611] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,611] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,612] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,612] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,612] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,613] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,619] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,619] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,619] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,620] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,621] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,621] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,621] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,621] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,620] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,625] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,622] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,626] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,626] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,625] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,627] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,627] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,627] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,627] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,628] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,629] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,629] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,629] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,629] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,630] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,628] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,630] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,631] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,631] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,631] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,631] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,632] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,633] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,633] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,634] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,634] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,635] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,638] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:04,642] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,644] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,644] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,646] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,994] INFO [Broker id=4] Transitioning 8 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:04,997] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,997] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,998] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,000] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,001] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,002] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,008] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,009] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,009] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,010] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,010] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,010] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,011] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,011] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,012] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,012] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,013] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,013] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,013] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,014] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,014] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,015] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,016] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,016] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,016] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,017] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,018] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,018] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,018] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,020] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,019] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,020] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,137] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:05,138] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,139] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,141] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,143] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,499] INFO [Broker id=4] Transitioning 4 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:05,501] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,501] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,502] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,503] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,503] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,503] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,504] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,504] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,504] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,505] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,505] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,506] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,506] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,506] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,507] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,508] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,392] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 22:26:02,399] INFO [BrokerServer id=4] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
[2025-05-20 22:26:02,400] INFO [BrokerServer id=4] shutting down (kafka.server.BrokerServer)
[2025-05-20 22:26:02,401] INFO [BrokerLifecycleManager id=4] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:26:02,440] INFO [BrokerLifecycleManager id=4] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:26:02,466] INFO [Broker id=4] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:26:02,467] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,468] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,471] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 3 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,472] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 3 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,473] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,473] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,474] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,474] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,474] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,475] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 3 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,475] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 4 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,476] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,476] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,477] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 5 from offset 2 with partition epoch 10 and high watermark 2. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,478] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,480] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,480] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 4 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,481] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,481] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 4 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,483] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,487] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,488] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 5 from offset 2 with partition epoch 10 and high watermark 2. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,489] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 4 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,489] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 3 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,490] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,490] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 4 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,491] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,496] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 4 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,497] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 4 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,498] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,498] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,502] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,503] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,503] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,504] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 4 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,504] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,504] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,505] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,505] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,506] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 3 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,506] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 3 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,507] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,507] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,507] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 4 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,508] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,508] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 3 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,508] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,508] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,509] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,509] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 3 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,509] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,529] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:26:02,530] INFO [ReplicaAlterLogDirsManager on broker 4] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:26:02,533] INFO [Broker id=4] Stopped fetchers as part of controlled shutdown for 51 partitions (state.change.logger)
[2025-05-20 22:26:02,534] INFO [ReplicaFetcherThread-0-6]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:26:02,536] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Client requested connection close from node 6 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:26:02,537] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Cancelled in-flight FETCH request with correlation id 122 due to node 6 being disconnected (elapsed time since creation: 251ms, elapsed time since send: 251ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:26:02,539] INFO [BrokerLifecycleManager id=4] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:26:02,539] INFO [BrokerLifecycleManager id=4] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:02,539] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Error sending fetch request (sessionId=303903651, epoch=122) to node 6: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Client was shutdown before response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:109)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 22:26:02,540] INFO [BrokerLifecycleManager id=4] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:26:02,549] INFO [broker-4-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,548] INFO [ReplicaFetcherThread-0-6]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:26:02,548] INFO [ReplicaFetcherThread-0-6]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:26:02,550] INFO [broker-4-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,551] INFO [SocketServer listenerType=BROKER, nodeId=4] Stopping socket server request processors (kafka.network.SocketServer)
[2025-05-20 22:26:02,550] INFO [broker-4-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,554] INFO [ReplicaFetcherThread-0-5]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:26:02,555] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:26:02,562] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Cancelled in-flight FETCH request with correlation id 118 due to node 5 being disconnected (elapsed time since creation: 165ms, elapsed time since send: 165ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:26:02,563] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=47637111, epoch=118) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Client was shutdown before response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:109)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 22:26:02,564] INFO [ReplicaFetcherThread-0-5]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:26:02,566] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:26:02,564] INFO [ReplicaFetcherThread-0-5]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:26:02,571] INFO [SocketServer listenerType=BROKER, nodeId=4] Stopped socket server request processors (kafka.network.SocketServer)
[2025-05-20 22:26:02,572] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,572] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,573] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,573] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,573] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,573] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,574] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,574] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,575] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,576] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,577] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,577] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,577] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,577] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,578] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,578] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,578] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,579] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,579] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,579] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,580] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,580] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,580] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,580] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,581] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,581] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,582] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,582] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,582] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,583] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,583] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,583] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,584] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,584] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,584] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,585] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,585] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,585] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,586] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,586] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,586] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,587] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,587] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,587] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,588] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,588] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,588] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,588] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,589] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,589] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,589] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,590] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,590] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,590] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,591] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,591] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,591] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,591] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,590] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,592] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,592] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,593] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,592] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,601] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,601] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,603] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,603] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,603] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,603] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,604] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,604] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,604] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,605] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,604] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,605] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,606] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,606] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,607] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,607] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,608] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,609] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,610] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,609] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,610] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,610] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,611] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,612] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,612] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,612] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,613] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,613] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,613] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,614] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,614] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,614] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,615] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,616] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,615] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,616] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,617] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,616] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,617] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,618] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,617] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,618] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,618] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,619] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,619] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,619] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,620] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,620] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,621] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,620] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,621] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,622] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,622] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,621] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,623] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,625] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,625] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,626] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,626] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,627] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,627] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,627] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,627] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,628] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,628] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,628] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,629] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,629] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,630] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,630] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,631] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,632] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,633] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,632] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,633] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,634] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,634] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,634] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,635] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,634] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,635] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,636] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,635] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,636] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,637] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,637] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,638] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,639] INFO [Broker id=4] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:26:02,640] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 22:26:02,641] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 22:26:02,642] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,643] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,644] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,645] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,645] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 22:26:02,647] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 22:26:02,647] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 22:26:02,648] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,649] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,650] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 22:26:02,650] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 22:26:02,651] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 6 from offset 2 with partition epoch 11 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 22:26:02,652] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,652] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,653] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,653] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,653] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,654] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 22:26:02,654] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,654] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 6 from offset 2 with partition epoch 11 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 22:26:02,655] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,655] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,656] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 22:26:02,656] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,658] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,659] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,660] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,661] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 22:26:02,662] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 22:26:02,664] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 22:26:02,667] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,668] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,668] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,668] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 22:26:02,669] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,669] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,670] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,670] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,671] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,672] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,673] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,673] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,674] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 22:26:02,674] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,674] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,674] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 22:26:02,675] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,675] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,675] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 22:26:02,676] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:26:02,676] INFO [ReplicaAlterLogDirsManager on broker 4] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:26:02,677] INFO [Broker id=4] Stopped fetchers as part of controlled shutdown for 51 partitions (state.change.logger)
[2025-05-20 22:26:02,678] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,678] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,679] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,679] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,679] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,680] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,680] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,681] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,681] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,682] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,681] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,682] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,683] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,683] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,683] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,684] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,684] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,685] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,685] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,685] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,687] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,687] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,688] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,688] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,688] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,689] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,690] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,690] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,690] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,691] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,692] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,693] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,691] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,693] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,694] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,695] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,695] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,696] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,696] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,697] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,696] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,697] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,698] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,698] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,701] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,701] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,702] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,703] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,703] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,703] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,704] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,704] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,705] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,705] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,705] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,706] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,706] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,707] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,706] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,707] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,708] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,708] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,708] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,709] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,708] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,709] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,709] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,709] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,710] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,710] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,710] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,710] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,711] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,711] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,713] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,714] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,714] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,715] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,715] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,715] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,716] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,716] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,717] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,717] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,718] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,717] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,718] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,718] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,718] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,719] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,719] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,719] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,720] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,720] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,720] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,720] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,721] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,721] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,721] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,722] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,722] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,722] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,723] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,723] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,723] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,723] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,723] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,724] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,724] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,724] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,724] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,725] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,725] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,726] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,726] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,726] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,726] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,727] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,727] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,727] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,728] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,728] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,728] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,729] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,729] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,729] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,730] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,729] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,730] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,730] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,730] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,731] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,731] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,731] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,731] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,732] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,731] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,732] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,733] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,732] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,733] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,734] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,734] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,734] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,735] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,735] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,735] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,734] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,736] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,736] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,736] INFO [data-plane Kafka Request Handler on Broker 4], shutting down (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 22:26:02,738] INFO [data-plane Kafka Request Handler on Broker 4], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 22:26:02,739] INFO [ExpirationReaper-4-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,740] INFO [ExpirationReaper-4-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,740] INFO [ExpirationReaper-4-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,742] INFO [KafkaApi-4] Shutdown complete. (kafka.server.KafkaApis)
[2025-05-20 22:26:02,746] INFO [TransactionCoordinator id=4] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:26:02,747] INFO [Transaction State Manager 4]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2025-05-20 22:26:02,748] INFO [TxnMarkerSenderThread-4]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:26:02,749] INFO [TxnMarkerSenderThread-4]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:26:02,749] INFO [TxnMarkerSenderThread-4]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:26:02,752] INFO [TransactionCoordinator id=4] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:26:02,752] INFO [GroupCoordinator 4]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,753] INFO [ExpirationReaper-4-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,754] INFO [ExpirationReaper-4-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,754] INFO [ExpirationReaper-4-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,755] INFO [ExpirationReaper-4-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,756] INFO [ExpirationReaper-4-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,756] INFO [ExpirationReaper-4-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,758] INFO [GroupCoordinator 4]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,758] INFO [AssignmentsManager id=4]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:02,759] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,759] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,759] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,760] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:26:02,760] INFO [AssignmentsManager id=4]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:02,761] INFO [ReplicaManager broker=4] Shutting down (kafka.server.ReplicaManager)
[2025-05-20 22:26:02,762] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:26:02,762] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:26:02,762] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:26:02,763] INFO [ReplicaFetcherManager on broker 4] shutting down (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:26:02,766] INFO [ReplicaFetcherManager on broker 4] shutdown completed (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:26:02,767] INFO [ReplicaAlterLogDirsManager on broker 4] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:26:02,767] INFO [ReplicaAlterLogDirsManager on broker 4] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:26:02,768] INFO [ExpirationReaper-4-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,769] INFO [ExpirationReaper-4-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,769] INFO [ExpirationReaper-4-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,769] INFO [ExpirationReaper-4-RemoteFetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,770] INFO [ExpirationReaper-4-RemoteFetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,770] INFO [ExpirationReaper-4-RemoteFetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,771] INFO [ExpirationReaper-4-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,772] INFO [ExpirationReaper-4-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,772] INFO [ExpirationReaper-4-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,772] INFO [ExpirationReaper-4-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,773] INFO [ExpirationReaper-4-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,773] INFO [ExpirationReaper-4-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,774] INFO [ExpirationReaper-4-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,774] INFO [ExpirationReaper-4-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,774] INFO [ExpirationReaper-4-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,782] INFO [AddPartitionsToTxnSenderThread-4]: Shutting down (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:26:02,783] INFO [AddPartitionsToTxnSenderThread-4]: Stopped (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:26:02,783] INFO [AddPartitionsToTxnSenderThread-4]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:26:02,784] INFO [ReplicaManager broker=4] Shut down completely (kafka.server.ReplicaManager)
[2025-05-20 22:26:02,785] INFO [broker-4-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,785] INFO [broker-4-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,785] INFO [broker-4-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,786] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:26:02,787] INFO [broker-4-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,787] INFO [broker-4-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,787] INFO [broker-4-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,788] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:26:02,789] INFO Shutting down. (kafka.log.LogManager)
[2025-05-20 22:26:02,790] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
[2025-05-20 22:26:02,791] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:26:02,791] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:26:02,791] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:26:02,805] INFO [ProducerStateManager partition=__consumer_offsets-29] Wrote producer snapshot at offset 2 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:26:02,907] INFO [ProducerStateManager partition=_schemas-0] Wrote producer snapshot at offset 2 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:26:02,982] INFO Shutdown complete. (kafka.log.LogManager)
[2025-05-20 22:26:02,984] INFO [broker-4-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,985] INFO [broker-4-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,985] INFO [broker-4-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,986] INFO [broker-4-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,986] INFO [broker-4-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,986] INFO [broker-4-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,987] INFO [broker-4-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,987] INFO [broker-4-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,987] INFO [broker-4-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,988] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,988] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,988] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,989] INFO [SocketServer listenerType=BROKER, nodeId=4] Shutting down socket server (kafka.network.SocketServer)
[2025-05-20 22:26:03,003] INFO [SocketServer listenerType=BROKER, nodeId=4] Shutdown completed (kafka.network.SocketServer)
[2025-05-20 22:26:03,004] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
[2025-05-20 22:26:03,005] INFO [BrokerLifecycleManager id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:03,006] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:26:03,007] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:26:03,007] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:26:03,008] INFO [SharedServer id=4] Stopping SharedServer (kafka.server.SharedServer)
[2025-05-20 22:26:03,009] INFO [MetadataLoader id=4] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:03,009] INFO [SnapshotGenerator id=4] close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:03,009] INFO [SnapshotGenerator id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:03,010] INFO [MetadataLoader id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:03,011] INFO [SnapshotGenerator id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:03,012] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:26:03,128] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:26:03,128] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:26:03,130] INFO [kafka-4-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:26:03,130] INFO [RaftManager id=4] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:26:03,130] INFO [RaftManager id=4] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:26:03,131] INFO [RaftManager id=4] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:26:03,131] INFO [kafka-4-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:26:03,131] INFO [kafka-4-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:26:03,136] INFO [kafka-4-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:26:03,136] INFO [kafka-4-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:26:03,136] INFO [kafka-4-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:26:03,140] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2740 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:26:03,142] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:26:03,143] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:26:03,143] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:26:03,144] INFO App info kafka.server for 4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:26:03,144] INFO [BrokerServer id=4] shut down completed (kafka.server.BrokerServer)
[2025-05-20 22:26:03,144] INFO [BrokerServer id=4] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
[2025-05-20 22:26:03,145] INFO App info kafka.server for 4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 23:12:46,827] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 23:12:47,212] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 23:12:47,235] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 23:12:47,241] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:53,055] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 23:12:53,306] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 23:12:53,324] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:53,578] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:53,610] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:53,650] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 23:12:53,674] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-20 23:12:53,677] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2025-05-20 23:12:53,691] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:53,864] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
[2025-05-20 23:12:53,882] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:53,888] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:53,889] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000002219.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-20 23:12:53,895] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000002740.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-20 23:12:53,895] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 7ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:54,081] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2740 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:12:54,091] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 2740 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:54,097] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2740 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:54,099] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=2740, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000002740.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:12:54,102] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 0ms for segment recovery from offset 2740 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:54,143] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-20 23:12:54,195] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 23:12:54,202] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 23:12:54,386] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 23:12:54,599] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=15, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from null (org.apache.kafka.raft.QuorumState)
[2025-05-20 23:12:54,625] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 23:12:54,631] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 23:12:54,666] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:54,670] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2025-05-20 23:12:54,678] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:54,772] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:12:54,775] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:54,788] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:12:54,793] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:12:54,807] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@643709918 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 23:12:54,830] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:12:54,833] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,839] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,874] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 23:12:54,875] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:54,877] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 23:12:54,918] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,920] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,917] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:54,927] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,928] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:54,928] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,937] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 23:12:54,955] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,956] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,980] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,060] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,102] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,104] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,105] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,095] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,119] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,123] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,217] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,317] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,336] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,338] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,342] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,352] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,358] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,361] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,422] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,522] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,577] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,592] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,610] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,612] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,624] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,678] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 23:12:55,759] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,770] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-20 23:12:55,909] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,912] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 23:12:55,921] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-20 23:12:55,941] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:55,945] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:55,960] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:55,961] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,012] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,016] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:56,023] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:56,026] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:56,053] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:56,054] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:56,102] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:56,110] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:56,118] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,237] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,313] INFO [BrokerLifecycleManager id=4] Incarnation ITjAcu-tSLWZ_BVXULkQMA of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:12:56,316] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,318] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,352] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,361] INFO [RaftManager id=4] Completed transition to Unattached(epoch=16, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=15, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-20 23:12:56,366] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:56,415] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:56,432] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,436] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 23:12:56,439] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 23:12:56,440] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 23:12:56,445] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=16, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=16, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 23:12:56,520] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,538] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,641] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,749] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,856] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,966] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,043] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=2744, metadata=Optional.empty)] for the first time for epoch 16 (org.apache.kafka.raft.FollowerState)
[2025-05-20 23:12:57,054] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 2744 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,078] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:57,080] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:57,144] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:57,226] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 2746 (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:12:57,299] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 2744 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,334] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 2743 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,339] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 2743 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,340] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 2743 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,383] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=2743, epoch=16) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-20 23:12:57,424] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,492] INFO Skipping recovery of 51 logs from /tmp/kafka-logs since clean shutdown file was found (kafka.log.LogManager)
[2025-05-20 23:12:57,612] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,618] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,627] INFO [ProducerStateManager partition=__consumer_offsets-29] Loading producer state from snapshot file 'SnapshotFile(offset=2, file=/tmp/kafka-logs/__consumer_offsets-29/00000000000000000002.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:12:57,641] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Producer state recovery took 14ms for snapshot load and 0ms for segment recovery from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,694] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-29, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=29, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=2) with 1 segments, local-log-start-offset 0 and log-end-offset 2 in 153ms (1/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,701] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,715] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-38, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=38, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 19ms (2/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,738] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,756] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-27, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=27, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 27ms (3/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,760] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,763] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-39, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=39, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (4/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,767] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,770] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-42, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=42, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (5/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,774] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,786] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-9, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (6/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,808] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,842] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-22, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=22, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 48ms (7/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,874] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,917] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-26, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=26, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 62ms (8/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,934] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,006] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-40, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=40, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 80ms (9/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,035] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,038] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-47, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=47, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 28ms (10/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,047] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,061] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-15, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 22ms (11/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,082] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,088] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-10, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (12/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,097] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,100] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-2, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (13/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,113] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,116] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-18, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (14/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,130] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,134] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-46, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=46, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (15/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,145] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,150] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-12, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (16/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,167] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,180] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-23, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=23, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 30ms (17/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,188] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,192] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-14, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (18/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,213] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,215] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-5, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 22ms (19/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,225] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,230] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-41, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=41, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (20/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,253] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,268] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-1, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 36ms (21/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,273] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,281] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-19, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (22/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,288] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,293] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-3, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (23/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,298] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,302] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-16, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (24/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,310] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,317] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-25, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=25, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (25/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,325] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,326] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,326] INFO [ProducerStateManager partition=_schemas-0] Loading producer state from snapshot file 'SnapshotFile(offset=2, file=/tmp/kafka-logs/_schemas-0/00000000000000000002.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:12:58,329] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 0ms for segment recovery from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,332] INFO Completed load of Log(dir=/tmp/kafka-logs/_schemas-0, topicId=RrE8eovWRKu4kLR3MRJ0fA, topic=_schemas, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=2) with 1 segments, local-log-start-offset 0 and log-end-offset 2 in 15ms (26/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,341] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,343] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-28, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=28, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (27/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,347] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,353] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-13, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (28/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,358] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,361] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-24, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=24, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (29/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,365] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,369] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-32, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=32, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (30/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,397] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,402] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-31, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=31, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 33ms (31/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,407] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,412] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-21, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=21, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (32/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,417] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,420] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-6, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (33/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,425] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,427] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-11, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (34/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,432] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,436] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-30, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=30, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (35/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,441] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,451] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-43, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=43, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (36/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,467] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,472] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-7, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 17ms (37/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,476] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,480] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-33, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=33, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (38/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,486] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,489] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-36, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=36, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (39/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,493] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,495] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-45, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=45, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (40/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,499] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,501] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-0, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (41/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,510] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,512] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-20, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=20, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (42/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,518] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,521] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-4, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (43/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,527] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,531] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-8, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (44/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,542] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,544] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-49, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=49, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (45/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,548] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,552] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-34, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=34, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (46/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,566] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,570] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-48, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=48, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 16ms (47/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,592] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,596] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-44, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=44, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 23ms (48/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,615] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,635] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-37, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=37, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 36ms (49/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,668] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,741] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-35, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=35, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 104ms (50/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,746] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,748] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-17, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (51/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,752] INFO Loaded 51 logs in 1306ms (kafka.log.LogManager)
[2025-05-20 23:12:58,753] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-20 23:12:58,755] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-20 23:12:58,768] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-20 23:12:59,121] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 23:12:59,162] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 23:12:59,179] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 23:12:59,180] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,198] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,200] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 23:12:59,213] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 23:12:59,213] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 23:12:59,221] INFO [Broker id=4] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:12:59,230] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,273] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,277] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,284] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,291] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,293] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,295] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,296] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,307] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,309] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,326] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,326] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,330] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,333] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,334] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,336] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,342] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,344] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,344] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,354] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,354] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,355] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,356] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,358] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,359] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,360] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,362] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,363] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,367] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,367] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,369] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,372] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,374] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,374] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,376] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,377] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,378] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,387] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,388] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,389] INFO [Broker id=4] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-20 23:12:59,393] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 2 (kafka.cluster.Partition)
[2025-05-20 23:12:59,395] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 6 from offset 2 with partition epoch 11 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,397] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,401] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,402] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,404] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,412] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,415] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,419] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,420] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,420] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,421] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,422] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,423] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,425] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,428] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,433] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,433] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,441] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,441] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,444] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,452] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,464] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,469] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,486] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 2 (kafka.cluster.Partition)
[2025-05-20 23:12:59,497] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 6 from offset 2 with partition epoch 11 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,505] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,516] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,519] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,519] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,541] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,555] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,561] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,566] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,575] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,577] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,578] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,578] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,579] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,583] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,585] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,588] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,589] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,591] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,592] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,593] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,593] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,594] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,595] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,596] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,596] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,601] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,604] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,606] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,610] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,610] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,611] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,613] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,614] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,616] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,617] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,621] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,623] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,624] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,625] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,629] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,630] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,631] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,632] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,638] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,639] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,642] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,643] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,643] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,643] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,644] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,645] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,647] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,652] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,654] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,654] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,656] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,658] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,659] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,664] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,666] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,667] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,668] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,674] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,677] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,679] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,679] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,680] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,681] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,682] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,683] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,684] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,685] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,689] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,692] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,695] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,696] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,697] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,697] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,697] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,699] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,700] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,700] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,702] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,703] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,704] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,707] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,711] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,713] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:12:59,714] INFO [Broker id=4] Stopped fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 23:12:59,732] INFO [Broker id=4] Started fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 23:12:59,819] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,821] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,830] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,834] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,837] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,838] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,838] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,840] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,842] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,842] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,843] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,843] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,844] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,844] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,844] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,845] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,849] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,849] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,850] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,851] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,852] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,852] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,853] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,853] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,853] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,853] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,854] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,854] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,854] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,855] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,855] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,856] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,856] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,856] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,857] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,857] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,857] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,859] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,859] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,859] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,861] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,861] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,861] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,863] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,864] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,864] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,865] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,865] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,865] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,866] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,866] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,866] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,866] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,867] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,867] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,868] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,868] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,868] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,869] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,870] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,870] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,870] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,871] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,871] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,871] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,871] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,872] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,872] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,872] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,872] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,873] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,879] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,879] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,880] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,881] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,882] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,882] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,882] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,883] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,883] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,883] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,883] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,884] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,884] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,884] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,884] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,885] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,885] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,885] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,886] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,886] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,886] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,887] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,887] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,887] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,887] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,887] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,888] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,888] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,888] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,888] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,889] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,889] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,889] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,889] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,889] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,890] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,890] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,890] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,890] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,879] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,891] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,894] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,900] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,900] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,897] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,901] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,901] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,902] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,902] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,902] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,902] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,903] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,903] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,903] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,903] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,903] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,901] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,905] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,905] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,906] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,906] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,906] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,907] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,907] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,910] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,910] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,911] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,911] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 23:12:59,911] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,922] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,923] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,923] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,924] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,924] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,926] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,927] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,927] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,927] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,928] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,929] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,928] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 23:12:59,941] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 2743 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:13:00,036] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:13:00,037] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 23:13:00,037] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 23:13:00,038] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 23:13:00,040] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 23:13:00,055] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:13:00,073] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 23:13:00,074] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:13:00,120] INFO [Broker id=4] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:13:00,132] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,133] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,138] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,142] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,143] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,144] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,144] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,145] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,145] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,146] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,146] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,146] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,147] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,147] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 7 from offset 2 with partition epoch 12 and high watermark 2. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,147] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,146] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:13:00,156] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,157] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,162] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,162] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,164] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,164] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,165] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 7 from offset 2 with partition epoch 12 and high watermark 2. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,165] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,166] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,166] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,167] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,167] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,168] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,168] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,169] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,169] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,176] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,176] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,177] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,185] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,190] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,191] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,192] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,165] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 23:13:00,193] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,197] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,199] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,201] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,205] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,206] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,206] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,208] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,208] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 23:13:00,209] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 23:13:00,209] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,211] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,210] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2025-05-20 23:13:00,216] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 23:13:00,211] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,220] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,222] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,226] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:13:00,226] INFO [Broker id=4] Stopped fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 23:13:00,235] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 23:13:00,282] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 23:13:00,286] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 23:13:00,287] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 23:13:00,288] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 23:13:00,288] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,289] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-20 23:13:00,290] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 23:13:00,291] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 23:13:00,291] INFO Kafka startTimeMs: 1747782780290 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 23:13:00,292] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-20 23:13:00,296] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-9 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-42 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-21 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-17 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-38 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-1 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,0), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), _schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,2), __consumer_offsets-45 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-12 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-41 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,0), __consumer_offsets-24 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-20 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,0), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-0 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,2), __consumer_offsets-25 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,0), __consumer_offsets-8 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-4 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,0), __consumer_offsets-33 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-15 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,0), __consumer_offsets-48 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,0), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-19 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-32 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-28 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-40 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-3 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-36 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-47 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-14 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-43 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-10 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-22 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-31 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-27 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-6 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-35 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:13:00,297] INFO [Broker id=4] Started fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 23:13:00,302] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,305] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,308] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,309] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,309] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,309] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,310] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,310] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,310] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,311] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,311] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,311] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,311] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,312] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,312] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,313] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,313] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,313] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,314] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,314] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,314] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,315] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,316] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,317] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,318] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,318] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,318] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,319] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,320] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,322] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,323] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,324] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,325] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,326] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,326] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,327] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,327] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,329] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,329] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,330] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,330] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,330] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,330] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,331] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,331] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,332] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,333] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,334] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,334] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,335] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,337] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,338] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,338] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,339] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,340] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,344] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,346] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,346] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,347] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,347] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,348] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,348] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,348] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,349] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,349] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,350] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,352] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,353] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,354] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,354] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,355] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,356] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,357] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,358] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,359] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,360] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,361] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,366] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,369] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,369] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,370] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,370] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,371] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,375] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,383] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,384] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,385] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,388] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,389] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,389] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,390] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,394] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,402] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,404] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,404] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,405] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,405] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,406] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,412] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,413] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,413] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,414] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,419] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,420] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,421] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,421] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,422] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,422] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,422] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,423] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,423] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,421] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,438] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,439] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,439] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,439] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,438] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,440] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,441] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,441] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,442] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,442] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,443] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,444] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,444] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,441] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,445] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,445] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,446] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,447] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,448] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,448] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,448] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,449] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,449] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,449] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,450] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,450] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,450] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,451] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,451] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,451] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,452] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,452] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,452] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,452] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,453] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,452] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,453] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,454] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,453] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,454] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,454] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,454] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,455] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,455] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,455] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,456] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,456] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,456] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,456] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,457] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,457] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,458] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,458] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,458] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,458] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,459] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,459] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,459] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,459] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,460] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,460] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,460] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,457] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,461] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,462] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,463] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,465] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,466] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,466] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,468] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,468] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,468] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,469] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,465] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,469] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,470] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,470] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,471] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,471] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,472] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,472] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,473] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,473] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,474] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,475] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,476] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,477] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,470] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,481] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,483] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,483] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,484] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,477] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,485] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,486] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,486] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,487] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,487] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,485] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,488] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,489] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,489] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,489] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,488] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,490] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,490] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,491] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,491] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,492] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,492] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,493] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,493] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,493] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,494] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,494] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,494] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,494] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,495] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,490] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,495] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,496] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,495] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,499] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,500] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,500] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,499] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,504] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,501] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,505] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,505] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,505] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,506] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,507] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,507] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,507] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,508] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,660] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:13:00,666] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:00,668] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,669] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,673] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,153] INFO [Broker id=4] Transitioning 31 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:13:01,154] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,155] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,158] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,158] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,159] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,159] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:01,160] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:01,160] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,161] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,161] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,161] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,162] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,162] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,163] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:01,163] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,164] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,165] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,166] INFO [Broker id=4] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,168] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,169] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,169] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,171] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,172] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,173] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,173] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,175] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,176] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,177] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:01,178] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,178] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,179] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,180] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,180] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,181] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,181] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,183] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,185] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,185] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,186] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,187] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,187] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,188] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,189] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,189] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,190] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,190] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,190] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,191] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,191] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,191] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,192] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,192] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,192] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,193] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,193] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,194] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,194] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,195] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,195] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,197] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,198] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,198] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,198] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,199] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,199] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,200] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,202] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,202] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,202] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,202] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,202] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,203] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,203] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,203] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,204] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,204] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,204] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,204] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,206] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,206] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,203] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,207] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,207] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,209] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,209] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,209] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,210] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,210] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,210] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,208] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,212] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,212] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,212] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,213] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,213] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,214] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,214] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,212] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,216] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,216] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,217] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,217] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,217] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,216] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,218] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,218] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,219] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,219] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,219] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,220] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,220] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,220] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,221] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,221] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,222] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,218] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,223] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,223] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,224] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,224] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,225] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,227] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:13:01,229] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,233] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,234] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,235] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,658] INFO [Broker id=4] Transitioning 25 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:13:01,658] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,659] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:01,659] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:01,660] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,660] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,666] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,674] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,675] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,680] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:01,687] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,688] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,689] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,690] INFO [Broker id=4] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,691] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,694] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,704] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,709] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,709] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,718] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,719] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,720] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,721] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,724] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:01,725] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,727] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,729] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,730] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,732] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,732] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,736] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,738] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,738] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,744] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,746] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,746] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,748] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,750] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,751] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,751] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,751] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,751] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,752] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,754] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,752] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,758] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,759] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,760] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,761] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,763] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,759] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,768] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,769] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,770] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,770] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,771] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,771] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,772] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,773] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,773] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,774] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,775] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,775] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,776] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,778] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,778] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,779] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,780] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,780] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,781] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,781] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,781] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,782] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,783] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,783] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,784] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,789] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,789] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,791] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,795] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,797] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,799] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,800] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,801] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,802] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,803] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,803] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,804] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,805] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,805] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,806] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,806] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,806] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,807] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,808] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,808] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,808] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,811] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,156] INFO [Broker id=4] Transitioning 25 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:13:02,158] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:02,159] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:02,163] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,165] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,165] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,175] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,176] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,176] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:02,177] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,177] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,177] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,178] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,181] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:02,181] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:02,182] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,182] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,183] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:02,183] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,184] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:02,186] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,186] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,186] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,187] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:02,187] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,188] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,189] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,189] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,189] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,189] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,190] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,190] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,190] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,191] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,192] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,193] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,194] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,194] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,194] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,194] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,195] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,196] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,196] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,196] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,196] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,197] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,197] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,198] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,198] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,197] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,198] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,199] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,199] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,200] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,200] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,200] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,201] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,201] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,201] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,202] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,203] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,202] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,203] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,204] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,203] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,204] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,205] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,204] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,205] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,206] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,205] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,206] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,206] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,207] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,208] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,208] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,209] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,208] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,209] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,209] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,210] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,210] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,210] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,211] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,211] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,212] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,212] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,212] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,213] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,213] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,213] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,214] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,214] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,214] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,215] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,215] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,216] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,216] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,216] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,216] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,217] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,351] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:13:02,351] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:02,352] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,353] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,355] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,654] INFO [Broker id=4] Transitioning 18 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:13:02,655] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:02,656] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,656] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,657] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4, 6], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:02,657] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,658] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4, 6], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,660] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:02,675] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,676] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,677] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4, 6], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:02,678] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4, 6], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,679] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4, 6], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,680] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,682] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,682] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,682] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:02,683] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,684] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4, 6], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,685] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,685] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,686] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,686] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,687] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,688] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,688] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,690] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,691] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,691] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,691] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,692] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,693] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,693] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,693] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,694] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,694] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,691] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,695] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,696] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,697] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,697] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,696] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,698] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,699] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,700] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,698] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,702] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,707] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,707] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,708] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,708] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,709] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,709] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,701] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,725] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,725] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,725] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,726] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,726] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,727] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,726] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,727] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,728] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,728] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,728] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,728] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,729] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,729] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,729] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,730] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,730] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,730] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,732] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,239] INFO [Broker id=4] Transitioning 18 partition(s) to local leaders. (state.change.logger)
[2025-05-20 23:17:57,242] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-16, _schemas-0, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-29, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-39, __consumer_offsets-5, __consumer_offsets-37, __consumer_offsets-2, __consumer_offsets-34) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:17:57,247] INFO [Broker id=4] Leader __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 15, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,254] INFO [Broker id=4] Leader _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) starts at leader epoch 8 from offset 4 with partition epoch 15, high watermark 4, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,258] INFO [Broker id=4] Leader __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 15, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,263] INFO [Broker id=4] Leader __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 15, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,268] INFO [Broker id=4] Leader __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 15, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,273] INFO [Broker id=4] Leader __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 15, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,275] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-9 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,276] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-9 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,276] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-42 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,277] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-42 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,277] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-38 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,278] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-38 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,278] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-1 has an older epoch (6) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,279] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-1 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,279] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-41 has an older epoch (6) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,280] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-41 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,281] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-20 has an older epoch (6) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,281] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-20 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,282] INFO [Broker id=4] Leader __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 15, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,283] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-25 has an older epoch (6) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,284] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-25 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,284] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-8 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,285] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-8 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,286] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-4 has an older epoch (6) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,286] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-4 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,286] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-15 has an older epoch (6) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,287] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-15 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,288] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-48 has an older epoch (6) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,288] INFO [Broker id=4] Leader __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 15, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,289] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-48 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,289] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-28 has an older epoch (6) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,290] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-28 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,290] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-47 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,291] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-47 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,291] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-14 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,292] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-14 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,292] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-22 has an older epoch (6) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,293] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-22 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,293] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-31 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,294] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-31 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,294] INFO [Broker id=4] Leader __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 15, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,294] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-35 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,296] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-35 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,301] INFO [Broker id=4] Leader __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 3 with partition epoch 15, high watermark 3, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,307] INFO [Broker id=4] Leader __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 15, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,314] INFO [Broker id=4] Leader __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 15, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,322] INFO [Broker id=4] Leader __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 15, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,327] INFO [Broker id=4] Leader __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 15, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,331] INFO [Broker id=4] Leader __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 15, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,336] INFO [Broker id=4] Leader __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 15, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,340] INFO [Broker id=4] Leader __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 15, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,345] INFO [Broker id=4] Leader __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 15, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,349] INFO [Broker id=4] Transitioning 17 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:17:57,349] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,349] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,350] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,350] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,350] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,350] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,351] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,351] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,351] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,352] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,352] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,352] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,352] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,353] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,353] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,353] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,353] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,355] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-47, __consumer_offsets-48, __consumer_offsets-14, __consumer_offsets-41, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-31, __consumer_offsets-28, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-38, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-1) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:17:57,355] INFO [Broker id=4] Stopped fetchers as part of become-follower for 17 partitions (state.change.logger)
[2025-05-20 23:17:57,358] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,358] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-47 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-48 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-14 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-41 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-9 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-42 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-22 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-20 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-31 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-28 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-25 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-8 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-38 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-35 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-4 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-1 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:17:57,358] INFO [Broker id=4] Started fetchers as part of become-follower for 17 partitions (state.change.logger)
[2025-05-20 23:17:57,358] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,359] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,359] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,360] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,360] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,360] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,361] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,361] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,361] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,362] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,362] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,362] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,363] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,363] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,363] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,363] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,364] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,364] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,364] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,364] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,365] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,365] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,365] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,365] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,366] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,366] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,366] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,366] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,367] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,367] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,367] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,367] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,368] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,368] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,369] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 16 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,371] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,371] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 13 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,372] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,372] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 46 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,372] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,373] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 11 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,373] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,373] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 44 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,374] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,374] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 23 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,375] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,375] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 49 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,375] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,375] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 18 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,376] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,376] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-16 in 4 milliseconds for epoch 8, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,376] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 29 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,376] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,376] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-13 in 4 milliseconds for epoch 8, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,377] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 30 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,377] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-46 in 4 milliseconds for epoch 8, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,377] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,378] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-11 in 5 milliseconds for epoch 8, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,378] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 26 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,378] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-44 in 4 milliseconds for epoch 8, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,378] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,379] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-23 in 4 milliseconds for epoch 8, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,379] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 7 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,379] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-49 in 4 milliseconds for epoch 8, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,380] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,380] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-18 in 4 milliseconds for epoch 8, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,380] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 39 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,381] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,381] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 5 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,381] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,381] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 37 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,382] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,382] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 2 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,382] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,382] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 34 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,382] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,383] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,383] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,383] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,384] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,384] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,384] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,384] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,385] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,385] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,385] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,385] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,386] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,386] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,386] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,386] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,387] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,387] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,387] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,387] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,388] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,388] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,388] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,388] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,388] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,389] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,389] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,389] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,389] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,390] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,390] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,390] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,390] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,390] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,391] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,394] INFO Loaded member MemberMetadata(memberId=sr-1-99122467-ec1a-4fc4-88f0-59fda8f60096, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 1. (kafka.coordinator.group.GroupMetadata$)
[2025-05-20 23:17:57,394] INFO Loaded member MemberMetadata(memberId=sr-1-3027da35-fa63-4fb3-9cba-2acfe293f9a1, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 3. (kafka.coordinator.group.GroupMetadata$)
[2025-05-20 23:17:57,396] INFO [GroupCoordinator 4]: Loading group metadata for schema-registry with generation 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,397] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-29 in 20 milliseconds for epoch 8, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,397] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-30 in 19 milliseconds for epoch 8, of which 19 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,398] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-26 in 19 milliseconds for epoch 8, of which 19 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,398] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-7 in 18 milliseconds for epoch 8, of which 18 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,398] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-39 in 17 milliseconds for epoch 8, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,399] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-5 in 18 milliseconds for epoch 8, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,399] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-37 in 17 milliseconds for epoch 8, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,399] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-2 in 17 milliseconds for epoch 8, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,399] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-34 in 16 milliseconds for epoch 8, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,400] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,400] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,400] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,400] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,401] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,401] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,401] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,401] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,401] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,402] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,402] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,402] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,402] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,402] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,403] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,403] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,403] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:22:56,321] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:22:56,607] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:31:58,482] INFO [Broker id=4] Transitioning 6 partition(s) to local leaders. (state.change.logger)
[2025-05-20 23:31:58,483] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(financial_transactions-13, financial_transactions-5, financial_transactions-16, financial_transactions-8, financial_transactions-1, financial_transactions-9) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:31:58,485] INFO [Broker id=4] Creating new partition financial_transactions-13 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,516] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,524] INFO Created log for partition financial_transactions-13 in /tmp/kafka-logs/financial_transactions-13 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,525] INFO [Partition financial_transactions-13 broker=4] No checkpointed highwatermark is found for partition financial_transactions-13 (kafka.cluster.Partition)
[2025-05-20 23:31:58,526] INFO [Partition financial_transactions-13 broker=4] Log loaded for partition financial_transactions-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,526] INFO [Broker id=4] Leader financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,532] INFO [Broker id=4] Creating new partition financial_transactions-5 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,539] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,541] INFO Created log for partition financial_transactions-5 in /tmp/kafka-logs/financial_transactions-5 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,541] INFO [Partition financial_transactions-5 broker=4] No checkpointed highwatermark is found for partition financial_transactions-5 (kafka.cluster.Partition)
[2025-05-20 23:31:58,542] INFO [Partition financial_transactions-5 broker=4] Log loaded for partition financial_transactions-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,542] INFO [Broker id=4] Leader financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,549] INFO [Broker id=4] Creating new partition financial_transactions-16 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,554] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,555] INFO Created log for partition financial_transactions-16 in /tmp/kafka-logs/financial_transactions-16 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,555] INFO [Partition financial_transactions-16 broker=4] No checkpointed highwatermark is found for partition financial_transactions-16 (kafka.cluster.Partition)
[2025-05-20 23:31:58,555] INFO [Partition financial_transactions-16 broker=4] Log loaded for partition financial_transactions-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,556] INFO [Broker id=4] Leader financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,563] INFO [Broker id=4] Creating new partition financial_transactions-8 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,568] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,569] INFO Created log for partition financial_transactions-8 in /tmp/kafka-logs/financial_transactions-8 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,569] INFO [Partition financial_transactions-8 broker=4] No checkpointed highwatermark is found for partition financial_transactions-8 (kafka.cluster.Partition)
[2025-05-20 23:31:58,569] INFO [Partition financial_transactions-8 broker=4] Log loaded for partition financial_transactions-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,570] INFO [Broker id=4] Leader financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,580] INFO [Broker id=4] Creating new partition financial_transactions-1 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,586] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,587] INFO Created log for partition financial_transactions-1 in /tmp/kafka-logs/financial_transactions-1 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,588] INFO [Partition financial_transactions-1 broker=4] No checkpointed highwatermark is found for partition financial_transactions-1 (kafka.cluster.Partition)
[2025-05-20 23:31:58,588] INFO [Partition financial_transactions-1 broker=4] Log loaded for partition financial_transactions-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,589] INFO [Broker id=4] Leader financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,5,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,596] INFO [Broker id=4] Creating new partition financial_transactions-9 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,600] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,602] INFO Created log for partition financial_transactions-9 in /tmp/kafka-logs/financial_transactions-9 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,603] INFO [Partition financial_transactions-9 broker=4] No checkpointed highwatermark is found for partition financial_transactions-9 (kafka.cluster.Partition)
[2025-05-20 23:31:58,604] INFO [Partition financial_transactions-9 broker=4] Log loaded for partition financial_transactions-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,604] INFO [Broker id=4] Leader financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [4,6,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,615] INFO [Broker id=4] Transitioning 14 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:31:58,617] INFO [Broker id=4] Creating new partition financial_transactions-14 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,624] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,627] INFO Created log for partition financial_transactions-14 in /tmp/kafka-logs/financial_transactions-14 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,628] INFO [Partition financial_transactions-14 broker=4] No checkpointed highwatermark is found for partition financial_transactions-14 (kafka.cluster.Partition)
[2025-05-20 23:31:58,628] INFO [Partition financial_transactions-14 broker=4] Log loaded for partition financial_transactions-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,629] INFO [Broker id=4] Follower financial_transactions-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,629] INFO [Broker id=4] Creating new partition financial_transactions-15 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,637] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,642] INFO Created log for partition financial_transactions-15 in /tmp/kafka-logs/financial_transactions-15 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,643] INFO [Partition financial_transactions-15 broker=4] No checkpointed highwatermark is found for partition financial_transactions-15 (kafka.cluster.Partition)
[2025-05-20 23:31:58,644] INFO [Partition financial_transactions-15 broker=4] Log loaded for partition financial_transactions-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,646] INFO [Broker id=4] Follower financial_transactions-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,647] INFO [Broker id=4] Creating new partition financial_transactions-17 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,651] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,652] INFO Created log for partition financial_transactions-17 in /tmp/kafka-logs/financial_transactions-17 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,653] INFO [Partition financial_transactions-17 broker=4] No checkpointed highwatermark is found for partition financial_transactions-17 (kafka.cluster.Partition)
[2025-05-20 23:31:58,653] INFO [Partition financial_transactions-17 broker=4] Log loaded for partition financial_transactions-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,653] INFO [Broker id=4] Follower financial_transactions-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,654] INFO [Broker id=4] Creating new partition financial_transactions-18 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,660] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,661] INFO Created log for partition financial_transactions-18 in /tmp/kafka-logs/financial_transactions-18 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,662] INFO [Partition financial_transactions-18 broker=4] No checkpointed highwatermark is found for partition financial_transactions-18 (kafka.cluster.Partition)
[2025-05-20 23:31:58,662] INFO [Partition financial_transactions-18 broker=4] Log loaded for partition financial_transactions-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,663] INFO [Broker id=4] Follower financial_transactions-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,663] INFO [Broker id=4] Creating new partition financial_transactions-19 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,669] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,670] INFO Created log for partition financial_transactions-19 in /tmp/kafka-logs/financial_transactions-19 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,671] INFO [Partition financial_transactions-19 broker=4] No checkpointed highwatermark is found for partition financial_transactions-19 (kafka.cluster.Partition)
[2025-05-20 23:31:58,672] INFO [Partition financial_transactions-19 broker=4] Log loaded for partition financial_transactions-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,673] INFO [Broker id=4] Follower financial_transactions-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,674] INFO [Broker id=4] Creating new partition financial_transactions-0 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,679] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,681] INFO Created log for partition financial_transactions-0 in /tmp/kafka-logs/financial_transactions-0 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,682] INFO [Partition financial_transactions-0 broker=4] No checkpointed highwatermark is found for partition financial_transactions-0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,682] INFO [Partition financial_transactions-0 broker=4] Log loaded for partition financial_transactions-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,682] INFO [Broker id=4] Follower financial_transactions-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,683] INFO [Broker id=4] Creating new partition financial_transactions-2 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,687] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,688] INFO Created log for partition financial_transactions-2 in /tmp/kafka-logs/financial_transactions-2 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,689] INFO [Partition financial_transactions-2 broker=4] No checkpointed highwatermark is found for partition financial_transactions-2 (kafka.cluster.Partition)
[2025-05-20 23:31:58,689] INFO [Partition financial_transactions-2 broker=4] Log loaded for partition financial_transactions-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,690] INFO [Broker id=4] Follower financial_transactions-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,690] INFO [Broker id=4] Creating new partition financial_transactions-3 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,694] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,695] INFO Created log for partition financial_transactions-3 in /tmp/kafka-logs/financial_transactions-3 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,695] INFO [Partition financial_transactions-3 broker=4] No checkpointed highwatermark is found for partition financial_transactions-3 (kafka.cluster.Partition)
[2025-05-20 23:31:58,696] INFO [Partition financial_transactions-3 broker=4] Log loaded for partition financial_transactions-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,696] INFO [Broker id=4] Follower financial_transactions-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,696] INFO [Broker id=4] Creating new partition financial_transactions-4 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,708] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,710] INFO Created log for partition financial_transactions-4 in /tmp/kafka-logs/financial_transactions-4 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,711] INFO [Partition financial_transactions-4 broker=4] No checkpointed highwatermark is found for partition financial_transactions-4 (kafka.cluster.Partition)
[2025-05-20 23:31:58,711] INFO [Partition financial_transactions-4 broker=4] Log loaded for partition financial_transactions-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,712] INFO [Broker id=4] Follower financial_transactions-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,712] INFO [Broker id=4] Creating new partition financial_transactions-6 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,717] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,718] INFO Created log for partition financial_transactions-6 in /tmp/kafka-logs/financial_transactions-6 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,718] INFO [Partition financial_transactions-6 broker=4] No checkpointed highwatermark is found for partition financial_transactions-6 (kafka.cluster.Partition)
[2025-05-20 23:31:58,719] INFO [Partition financial_transactions-6 broker=4] Log loaded for partition financial_transactions-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,719] INFO [Broker id=4] Follower financial_transactions-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,720] INFO [Broker id=4] Creating new partition financial_transactions-7 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,726] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,727] INFO Created log for partition financial_transactions-7 in /tmp/kafka-logs/financial_transactions-7 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,728] INFO [Partition financial_transactions-7 broker=4] No checkpointed highwatermark is found for partition financial_transactions-7 (kafka.cluster.Partition)
[2025-05-20 23:31:58,728] INFO [Partition financial_transactions-7 broker=4] Log loaded for partition financial_transactions-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,730] INFO [Broker id=4] Follower financial_transactions-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,731] INFO [Broker id=4] Creating new partition financial_transactions-10 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,737] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,739] INFO Created log for partition financial_transactions-10 in /tmp/kafka-logs/financial_transactions-10 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,741] INFO [Partition financial_transactions-10 broker=4] No checkpointed highwatermark is found for partition financial_transactions-10 (kafka.cluster.Partition)
[2025-05-20 23:31:58,743] INFO [Partition financial_transactions-10 broker=4] Log loaded for partition financial_transactions-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,744] INFO [Broker id=4] Follower financial_transactions-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,744] INFO [Broker id=4] Creating new partition financial_transactions-11 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,750] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,752] INFO Created log for partition financial_transactions-11 in /tmp/kafka-logs/financial_transactions-11 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,752] INFO [Partition financial_transactions-11 broker=4] No checkpointed highwatermark is found for partition financial_transactions-11 (kafka.cluster.Partition)
[2025-05-20 23:31:58,752] INFO [Partition financial_transactions-11 broker=4] Log loaded for partition financial_transactions-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,753] INFO [Broker id=4] Follower financial_transactions-11 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,754] INFO [Broker id=4] Creating new partition financial_transactions-12 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,758] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,759] INFO Created log for partition financial_transactions-12 in /tmp/kafka-logs/financial_transactions-12 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,760] INFO [Partition financial_transactions-12 broker=4] No checkpointed highwatermark is found for partition financial_transactions-12 (kafka.cluster.Partition)
[2025-05-20 23:31:58,760] INFO [Partition financial_transactions-12 broker=4] Log loaded for partition financial_transactions-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,761] INFO [Broker id=4] Follower financial_transactions-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,762] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(financial_transactions-14, financial_transactions-15, financial_transactions-17, financial_transactions-18, financial_transactions-19, financial_transactions-0, financial_transactions-2, financial_transactions-3, financial_transactions-4, financial_transactions-6, financial_transactions-7, financial_transactions-10, financial_transactions-11, financial_transactions-12) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:31:58,762] INFO [Broker id=4] Stopped fetchers as part of become-follower for 14 partitions (state.change.logger)
[2025-05-20 23:31:58,763] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(financial_transactions-0 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), financial_transactions-17 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), financial_transactions-18 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), financial_transactions-3 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), financial_transactions-7 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), financial_transactions-10 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), financial_transactions-12 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:31:58,764] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(financial_transactions-14 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), financial_transactions-15 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), financial_transactions-2 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), financial_transactions-19 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), financial_transactions-4 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), financial_transactions-6 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), financial_transactions-11 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:31:58,764] INFO [Broker id=4] Started fetchers as part of become-follower for 14 partitions (state.change.logger)
[2025-05-20 23:31:59,082] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition financial_transactions-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,083] INFO [UnifiedLog partition=financial_transactions-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,084] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition financial_transactions-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,084] INFO [UnifiedLog partition=financial_transactions-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,084] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition financial_transactions-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,085] INFO [UnifiedLog partition=financial_transactions-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,085] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition financial_transactions-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,085] INFO [UnifiedLog partition=financial_transactions-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,086] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition financial_transactions-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,086] INFO [UnifiedLog partition=financial_transactions-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,086] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition financial_transactions-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,087] INFO [UnifiedLog partition=financial_transactions-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,087] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition financial_transactions-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,087] INFO [UnifiedLog partition=financial_transactions-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,220] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition financial_transactions-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,221] INFO [UnifiedLog partition=financial_transactions-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,222] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition financial_transactions-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,222] INFO [UnifiedLog partition=financial_transactions-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,222] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition financial_transactions-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,223] INFO [UnifiedLog partition=financial_transactions-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,223] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition financial_transactions-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,224] INFO [UnifiedLog partition=financial_transactions-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,224] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition financial_transactions-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,224] INFO [UnifiedLog partition=financial_transactions-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,225] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition financial_transactions-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,225] INFO [UnifiedLog partition=financial_transactions-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,225] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition financial_transactions-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,226] INFO [UnifiedLog partition=financial_transactions-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:37:27,762] INFO Sent auto-creation request for Set(aggregated_transactions) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-05-20 23:37:27,775] INFO [NodeToControllerChannelManager id=4 name=forwarding] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:37:27,776] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:37:27,978] INFO Sent auto-creation request for Set(aggregated_transactions) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-05-20 23:47:28,205] INFO [NodeToControllerChannelManager id=4 name=forwarding] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:43,989] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 3 (__consumer_offsets-29) (reason: Removing member sr-1-3027da35-fa63-4fb3-9cba-2acfe293f9a1 on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:43,992] INFO [GroupCoordinator 4]: Group schema-registry with generation 4 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:44,001] INFO [GroupCoordinator 4]: Member MemberMetadata(memberId=sr-1-3027da35-fa63-4fb3-9cba-2acfe293f9a1, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,094] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 23:53:45,099] INFO [BrokerServer id=4] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
[2025-05-20 23:53:45,100] INFO [BrokerServer id=4] shutting down (kafka.server.BrokerServer)
[2025-05-20 23:53:45,102] INFO [BrokerLifecycleManager id=4] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:53:45,152] INFO [BrokerLifecycleManager id=4] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:53:45,234] INFO [Broker id=4] Transitioning 71 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:53:45,234] INFO [Broker id=4] Follower financial_transactions-13 starts at leader epoch 3 from offset 53728 with partition epoch 3 and high watermark 53728. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:53:45,235] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,235] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,235] INFO [Broker id=4] Follower financial_transactions-17 starts at leader epoch 1 from offset 53580 with partition epoch 3 and high watermark 53580. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,236] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,236] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,236] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,236] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,237] INFO [Broker id=4] Follower financial_transactions-0 starts at leader epoch 1 from offset 53808 with partition epoch 3 and high watermark 53808. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,237] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,237] INFO [Broker id=4] Follower financial_transactions-4 starts at leader epoch 2 from offset 53258 with partition epoch 3 and high watermark 53258. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,237] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,238] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,238] INFO [Broker id=4] Follower financial_transactions-8 starts at leader epoch 3 from offset 53130 with partition epoch 3 and high watermark 53130. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:53:45,239] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,239] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,239] INFO [Broker id=4] Follower financial_transactions-12 starts at leader epoch 1 from offset 53441 with partition epoch 3 and high watermark 53441. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,240] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,240] INFO [Broker id=4] Follower financial_transactions-14 starts at leader epoch 2 from offset 53468 with partition epoch 3 and high watermark 53468. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,241] INFO [BrokerLifecycleManager id=4] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:53:45,241] INFO [BrokerLifecycleManager id=4] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:45,242] INFO [BrokerLifecycleManager id=4] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:53:45,243] INFO [broker-4-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,241] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,244] INFO [broker-4-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,244] INFO [broker-4-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,244] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 11 from offset 4 with partition epoch 18 and high watermark 4. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,248] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,249] INFO [Broker id=4] Follower financial_transactions-18 starts at leader epoch 1 from offset 53605 with partition epoch 3 and high watermark 53605. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,249] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,249] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,250] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,250] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,249] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 23:53:45,252] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,253] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,254] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 11 from offset 4 with partition epoch 18 and high watermark 4. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,254] INFO [Broker id=4] Follower financial_transactions-1 starts at leader epoch 3 from offset 53591 with partition epoch 3 and high watermark 53591. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:53:45,254] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,255] INFO [Broker id=4] Follower financial_transactions-5 starts at leader epoch 2 from offset 53395 with partition epoch 3 and high watermark 53395. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,255] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,256] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,256] INFO [Broker id=4] Follower financial_transactions-9 starts at leader epoch 2 from offset 53169 with partition epoch 3 and high watermark 53169. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,256] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,257] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,257] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Node 5 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,257] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,257] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Cancelled in-flight FETCH request with correlation id 28264 due to node 5 being disconnected (elapsed time since creation: 213ms, elapsed time since send: 213ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,257] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,258] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,258] INFO [Broker id=4] Follower financial_transactions-15 starts at leader epoch 2 from offset 53502 with partition epoch 3 and high watermark 53502. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,258] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,259] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,260] INFO [SocketServer listenerType=BROKER, nodeId=4] Stopping socket server request processors (kafka.network.SocketServer)
[2025-05-20 23:53:45,260] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=231292958, epoch=28264) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 23:53:45,303] INFO [SocketServer listenerType=BROKER, nodeId=4] Stopped socket server request processors (kafka.network.SocketServer)
[2025-05-20 23:53:45,288] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Node 6 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,270] INFO [Broker id=4] Follower financial_transactions-19 starts at leader epoch 2 from offset 53309 with partition epoch 3 and high watermark 53309. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,307] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Cancelled in-flight FETCH request with correlation id 27556 due to node 6 being disconnected (elapsed time since creation: 146ms, elapsed time since send: 146ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,308] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,308] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Client requested connection close from node 6 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,308] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=4, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=231292958, epoch=28264), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 23:53:45,308] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,309] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Error sending fetch request (sessionId=511520663, epoch=27556) to node 6: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 6 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 23:53:45,310] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,310] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=4, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=511520663, epoch=27556), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 6 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 23:53:45,310] INFO [Broker id=4] Follower financial_transactions-2 starts at leader epoch 2 from offset 53173 with partition epoch 3 and high watermark 53173. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,311] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,312] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,312] INFO [Broker id=4] Follower financial_transactions-6 starts at leader epoch 2 from offset 53206 with partition epoch 3 and high watermark 53206. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,312] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,312] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,313] INFO [Broker id=4] Follower financial_transactions-10 starts at leader epoch 1 from offset 53252 with partition epoch 3 and high watermark 53252. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,313] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,313] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,314] INFO [Broker id=4] Follower financial_transactions-16 starts at leader epoch 2 from offset 53208 with partition epoch 3 and high watermark 53208. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,314] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,314] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,314] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,315] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,315] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,316] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,316] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,317] INFO [Broker id=4] Follower financial_transactions-3 starts at leader epoch 1 from offset 53622 with partition epoch 3 and high watermark 53622. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,317] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,317] INFO [Broker id=4] Follower financial_transactions-7 starts at leader epoch 1 from offset 53438 with partition epoch 3 and high watermark 53438. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,318] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,318] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,318] INFO [Broker id=4] Follower financial_transactions-11 starts at leader epoch 2 from offset 53486 with partition epoch 3 and high watermark 53486. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,319] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,336] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, financial_transactions-18, financial_transactions-12, financial_transactions-11, __consumer_offsets-8, __consumer_offsets-21, financial_transactions-15, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, financial_transactions-4, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, financial_transactions-0, financial_transactions-6, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, financial_transactions-16, __consumer_offsets-15, financial_transactions-10, __consumer_offsets-24, financial_transactions-19, financial_transactions-7, __consumer_offsets-17, financial_transactions-17, financial_transactions-1, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, financial_transactions-14, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, financial_transactions-8, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, financial_transactions-2, financial_transactions-13, __consumer_offsets-32, financial_transactions-5, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:53:45,337] INFO [ReplicaAlterLogDirsManager on broker 4] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, financial_transactions-18, financial_transactions-12, financial_transactions-11, __consumer_offsets-8, __consumer_offsets-21, financial_transactions-15, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, financial_transactions-4, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, financial_transactions-0, financial_transactions-6, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, financial_transactions-16, __consumer_offsets-15, financial_transactions-10, __consumer_offsets-24, financial_transactions-19, financial_transactions-7, __consumer_offsets-17, financial_transactions-17, financial_transactions-1, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, financial_transactions-14, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, financial_transactions-8, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, financial_transactions-2, financial_transactions-13, __consumer_offsets-32, financial_transactions-5, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 23:53:45,348] INFO [Broker id=4] Stopped fetchers as part of controlled shutdown for 71 partitions (state.change.logger)
[2025-05-20 23:53:45,350] INFO [ReplicaFetcherThread-0-6]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:53:45,352] INFO [ReplicaFetcherThread-0-6]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:53:45,352] INFO [ReplicaFetcherThread-0-6]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:53:45,355] INFO [ReplicaFetcherThread-0-5]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:53:45,355] INFO [ReplicaFetcherThread-0-5]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:53:45,355] INFO [ReplicaFetcherThread-0-5]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:53:45,357] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,357] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,358] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,358] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,358] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,358] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,359] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,358] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,359] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,359] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,359] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,360] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,360] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,360] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,361] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,361] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,361] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,361] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,362] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,362] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,362] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,363] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,362] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,363] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,363] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,363] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,364] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,364] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,364] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,365] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,365] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,365] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,366] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,366] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,366] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,367] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,367] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,367] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,368] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,368] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,369] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,368] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,369] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,369] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,370] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,370] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,370] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,371] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,371] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,371] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,371] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,372] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,372] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,372] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,373] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,372] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,373] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,374] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,373] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,374] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,375] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,374] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,375] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,375] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,375] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,376] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,376] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,376] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,376] INFO [GroupCoordinator 4]: Unloading group metadata for schema-registry with generation 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,377] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,377] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,377] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,378] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,378] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,378] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,378] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,379] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,379] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,378] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,379] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,380] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,380] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,380] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,380] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,381] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,381] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,381] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,382] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,382] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,382] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,383] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,383] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,383] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,383] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,384] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,384] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,384] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,385] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,385] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,385] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,386] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,386] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,386] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,386] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,386] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,387] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,387] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,387] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,387] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,388] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,388] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,388] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,388] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,388] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,389] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,389] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,389] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,389] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,390] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,390] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,390] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,391] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,390] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,391] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,391] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,391] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,392] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,392] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,392] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,393] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,393] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,393] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,393] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,394] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,393] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,394] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,394] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,394] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,395] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,395] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,395] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,395] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,396] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,396] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,396] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,396] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,397] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,397] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,397] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,397] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,398] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,400] INFO [data-plane Kafka Request Handler on Broker 4], shutting down (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 23:53:45,402] INFO [data-plane Kafka Request Handler on Broker 4], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 23:53:45,403] INFO [ExpirationReaper-4-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,404] INFO [ExpirationReaper-4-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,404] INFO [ExpirationReaper-4-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,406] INFO [KafkaApi-4] Shutdown complete. (kafka.server.KafkaApis)
[2025-05-20 23:53:45,409] INFO [TransactionCoordinator id=4] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 23:53:45,410] INFO [Transaction State Manager 4]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2025-05-20 23:53:45,411] INFO [TxnMarkerSenderThread-4]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 23:53:45,411] INFO [TxnMarkerSenderThread-4]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 23:53:45,411] INFO [TxnMarkerSenderThread-4]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 23:53:45,413] INFO [TransactionCoordinator id=4] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 23:53:45,414] INFO [GroupCoordinator 4]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,415] INFO [ExpirationReaper-4-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,416] INFO [ExpirationReaper-4-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,416] INFO [ExpirationReaper-4-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,417] INFO [ExpirationReaper-4-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,418] INFO [ExpirationReaper-4-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,418] INFO [ExpirationReaper-4-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,419] INFO [GroupCoordinator 4]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,421] INFO [AssignmentsManager id=4]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:45,422] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,423] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,423] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,424] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 23:53:45,424] INFO [AssignmentsManager id=4]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:45,425] INFO [ReplicaManager broker=4] Shutting down (kafka.server.ReplicaManager)
[2025-05-20 23:53:45,425] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 23:53:45,426] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 23:53:45,426] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 23:53:45,427] INFO [ReplicaFetcherManager on broker 4] shutting down (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:53:45,429] INFO [ReplicaFetcherManager on broker 4] shutdown completed (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:53:45,429] INFO [ReplicaAlterLogDirsManager on broker 4] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 23:53:45,430] INFO [ReplicaAlterLogDirsManager on broker 4] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 23:53:45,430] INFO [ExpirationReaper-4-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,430] INFO [ExpirationReaper-4-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,430] INFO [ExpirationReaper-4-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,431] INFO [ExpirationReaper-4-RemoteFetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,432] INFO [ExpirationReaper-4-RemoteFetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,432] INFO [ExpirationReaper-4-RemoteFetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,435] INFO [ExpirationReaper-4-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,435] INFO [ExpirationReaper-4-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,435] INFO [ExpirationReaper-4-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,436] INFO [ExpirationReaper-4-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,437] INFO [ExpirationReaper-4-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,437] INFO [ExpirationReaper-4-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,438] INFO [ExpirationReaper-4-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,438] INFO [ExpirationReaper-4-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,438] INFO [ExpirationReaper-4-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,450] INFO [AddPartitionsToTxnSenderThread-4]: Shutting down (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 23:53:45,451] INFO [AddPartitionsToTxnSenderThread-4]: Stopped (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 23:53:45,451] INFO [AddPartitionsToTxnSenderThread-4]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 23:53:45,452] INFO [ReplicaManager broker=4] Shut down completely (kafka.server.ReplicaManager)
[2025-05-20 23:53:45,453] INFO [broker-4-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,453] INFO [broker-4-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,453] INFO [broker-4-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,454] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 23:53:45,455] INFO [broker-4-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,455] INFO [broker-4-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,455] INFO [broker-4-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,456] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 23:53:45,457] INFO Shutting down. (kafka.log.LogManager)
[2025-05-20 23:53:45,458] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
[2025-05-20 23:53:45,459] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 23:53:45,459] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 23:53:45,459] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 23:53:45,480] INFO [ProducerStateManager partition=financial_transactions-15] Wrote producer snapshot at offset 53502 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,536] INFO [ProducerStateManager partition=financial_transactions-0] Wrote producer snapshot at offset 53808 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,603] INFO [ProducerStateManager partition=financial_transactions-3] Wrote producer snapshot at offset 53622 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,624] INFO [ProducerStateManager partition=financial_transactions-10] Wrote producer snapshot at offset 53252 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,640] INFO [ProducerStateManager partition=financial_transactions-12] Wrote producer snapshot at offset 53441 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,652] INFO [ProducerStateManager partition=financial_transactions-13] Wrote producer snapshot at offset 53728 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,685] INFO [ProducerStateManager partition=financial_transactions-19] Wrote producer snapshot at offset 53309 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,696] INFO [ProducerStateManager partition=financial_transactions-5] Wrote producer snapshot at offset 53395 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,711] INFO [ProducerStateManager partition=financial_transactions-6] Wrote producer snapshot at offset 53206 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,754] INFO [ProducerStateManager partition=financial_transactions-17] Wrote producer snapshot at offset 53580 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,766] INFO [ProducerStateManager partition=financial_transactions-7] Wrote producer snapshot at offset 53438 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,792] INFO [ProducerStateManager partition=__consumer_offsets-29] Wrote producer snapshot at offset 4 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,846] INFO [ProducerStateManager partition=financial_transactions-4] Wrote producer snapshot at offset 53258 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,888] INFO [ProducerStateManager partition=financial_transactions-18] Wrote producer snapshot at offset 53605 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,902] INFO [ProducerStateManager partition=financial_transactions-9] Wrote producer snapshot at offset 53169 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,915] INFO [ProducerStateManager partition=financial_transactions-1] Wrote producer snapshot at offset 53591 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,929] INFO [ProducerStateManager partition=financial_transactions-14] Wrote producer snapshot at offset 53468 with 0 producer ids in 6 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,954] INFO [ProducerStateManager partition=financial_transactions-11] Wrote producer snapshot at offset 53486 with 0 producer ids in 6 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,990] INFO [ProducerStateManager partition=_schemas-0] Wrote producer snapshot at offset 4 with 0 producer ids in 6 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:46,016] INFO [ProducerStateManager partition=financial_transactions-8] Wrote producer snapshot at offset 53130 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:46,035] INFO [ProducerStateManager partition=financial_transactions-2] Wrote producer snapshot at offset 53173 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:46,053] INFO [ProducerStateManager partition=financial_transactions-16] Wrote producer snapshot at offset 53208 with 0 producer ids in 6 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:46,143] INFO Shutdown complete. (kafka.log.LogManager)
[2025-05-20 23:53:46,144] INFO [broker-4-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,145] INFO [broker-4-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,145] INFO [broker-4-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,145] INFO [broker-4-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,146] INFO [broker-4-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,146] INFO [broker-4-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,146] INFO [broker-4-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,147] INFO [broker-4-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,147] INFO [broker-4-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,147] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,147] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,147] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,148] INFO [SocketServer listenerType=BROKER, nodeId=4] Shutting down socket server (kafka.network.SocketServer)
[2025-05-20 23:53:46,156] INFO [SocketServer listenerType=BROKER, nodeId=4] Shutdown completed (kafka.network.SocketServer)
[2025-05-20 23:53:46,157] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
[2025-05-20 23:53:46,157] INFO [BrokerLifecycleManager id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:46,158] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 23:53:46,159] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 23:53:46,159] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 23:53:46,160] INFO [SharedServer id=4] Stopping SharedServer (kafka.server.SharedServer)
[2025-05-20 23:53:46,160] INFO [MetadataLoader id=4] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:46,161] INFO [SnapshotGenerator id=4] close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:46,161] INFO [SnapshotGenerator id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:46,162] INFO [MetadataLoader id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:46,162] INFO [SnapshotGenerator id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:46,163] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 23:53:46,193] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 23:53:46,193] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 23:53:46,195] INFO [kafka-4-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 23:53:46,195] INFO [RaftManager id=4] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 23:53:46,195] INFO [RaftManager id=4] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 23:53:46,196] INFO [RaftManager id=4] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 23:53:46,196] INFO [kafka-4-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 23:53:46,196] INFO [kafka-4-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 23:53:46,202] INFO [kafka-4-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 23:53:46,202] INFO [kafka-4-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 23:53:46,202] INFO [kafka-4-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 23:53:46,206] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 8082 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:46,208] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 23:53:46,209] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 23:53:46,209] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 23:53:46,210] INFO App info kafka.server for 4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 23:53:46,210] INFO [BrokerServer id=4] shut down completed (kafka.server.BrokerServer)
[2025-05-20 23:53:46,211] INFO [BrokerServer id=4] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
[2025-05-20 23:53:46,212] INFO App info kafka.server for 4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 12:15:19,195] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-21 12:15:19,568] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-21 12:15:19,591] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-21 12:15:19,599] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:24,436] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-21 12:15:24,645] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-21 12:15:24,661] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:24,974] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:25,026] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:25,096] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-21 12:15:25,112] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-21 12:15:25,114] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2025-05-21 12:15:25,119] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:25,251] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
[2025-05-21 12:15:25,254] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:25,257] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:25,258] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000002740.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 12:15:25,272] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000008082.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 12:15:25,277] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 19ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:25,720] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 8082 with 0 producer ids in 13 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:25,763] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 8082 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:25,771] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 8082 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:25,771] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=8082, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000008082.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:25,777] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 6ms for snapshot load and 0ms for segment recovery from offset 8082 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:25,872] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-21 12:15:25,926] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 12:15:25,933] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 12:15:26,383] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 12:15:26,551] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=16, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from null (org.apache.kafka.raft.QuorumState)
[2025-05-21 12:15:26,555] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 12:15:26,566] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 12:15:26,660] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:26,661] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2025-05-21 12:15:26,712] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:26,721] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:15:26,724] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:15:26,724] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:15:26,726] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:15:26,746] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-21 12:15:26,747] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-21 12:15:26,771] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:26,772] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:26,779] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:26,881] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,122] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,160] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 12:15:27,188] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@135280854 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 12:15:27,194] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,221] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,224] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,359] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,471] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,501] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,507] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,534] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,571] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,580] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,593] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,622] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,634] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,638] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,651] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,652] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,689] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,712] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,743] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,747] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,749] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,766] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,747] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,885] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,962] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,968] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,992] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,099] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,204] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,306] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,407] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,407] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,408] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,417] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,424] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,508] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,611] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,613] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-21 12:15:28,709] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-21 12:15:28,715] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-21 12:15:28,716] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,723] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-21 12:15:28,752] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,757] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,794] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,794] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,809] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,812] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,811] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,811] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,823] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,822] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,849] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,842] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,857] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,851] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,863] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,864] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,941] INFO [BrokerLifecycleManager id=4] Incarnation yokuzlkwQASkxka_dDy-9A of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:15:28,941] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,943] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,951] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,968] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:29,008] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,013] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,067] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,080] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,089] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,110] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,124] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,123] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-21 12:15:29,132] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-21 12:15:29,140] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-21 12:15:29,174] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,219] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,226] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,231] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,281] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,332] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,341] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,333] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,358] INFO [RaftManager id=4] Completed transition to Unattached(epoch=17, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=16, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-21 12:15:29,367] INFO [RaftManager id=4] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,373] WARN [RaftManager id=4] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,471] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,491] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=17, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=17, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-21 12:15:29,499] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,541] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,555] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,581] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,601] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,688] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,791] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,892] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,992] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,093] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,194] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,205] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:30,209] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:30,259] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:30,298] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,322] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=8087, metadata=Optional.empty)] for the first time for epoch 17 (org.apache.kafka.raft.FollowerState)
[2025-05-21 12:15:30,338] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 8087 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,354] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 8087 (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:15:30,934] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 8086, but the high water mark is 8092 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,937] INFO [MetadataLoader id=4] initializeNewPublishers: The loader is still catching up because we have loaded up to offset 8086, but the high water mark is 8092 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,958] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 8092 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:31,011] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 8091 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:31,016] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 8091 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:31,017] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 8091 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:31,019] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=8091, epoch=17) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-21 12:15:31,024] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,063] INFO Skipping recovery of 71 logs from /tmp/kafka-logs since clean shutdown file was found (kafka.log.LogManager)
[2025-05-21 12:15:31,098] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Loading producer state till offset 53728 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,100] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53728 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,100] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:15:31,104] INFO [ProducerStateManager partition=financial_transactions-13] Loading producer state from snapshot file 'SnapshotFile(offset=53728, file=/tmp/kafka-logs/financial_transactions-13/00000000000000053728.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,107] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-21 12:15:31,108] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-21 12:15:31,110] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Producer state recovery took 6ms for snapshot load and 0ms for segment recovery from offset 53728 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,124] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-13, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53728) with 1 segments, local-log-start-offset 0 and log-end-offset 53728 in 33ms (1/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,132] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:15:31,160] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Loading producer state till offset 53605 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,161] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53605 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,168] INFO [ProducerStateManager partition=financial_transactions-18] Loading producer state from snapshot file 'SnapshotFile(offset=53605, file=/tmp/kafka-logs/financial_transactions-18/00000000000000053605.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,170] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 53605 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,172] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-18, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53605) with 1 segments, local-log-start-offset 0 and log-end-offset 53605 in 42ms (2/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,204] INFO Deleted producer state snapshot /tmp/kafka-logs/__consumer_offsets-29/00000000000000000002.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 12:15:31,204] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 4 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,204] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 4 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,205] INFO [ProducerStateManager partition=__consumer_offsets-29] Loading producer state from snapshot file 'SnapshotFile(offset=4, file=/tmp/kafka-logs/__consumer_offsets-29/00000000000000000004.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,205] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 4 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,211] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-29, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=29, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=4) with 1 segments, local-log-start-offset 0 and log-end-offset 4 in 32ms (3/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,257] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Loading producer state till offset 53441 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,262] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53441 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,262] INFO [ProducerStateManager partition=financial_transactions-12] Loading producer state from snapshot file 'SnapshotFile(offset=53441, file=/tmp/kafka-logs/financial_transactions-12/00000000000000053441.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,272] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Producer state recovery took 10ms for snapshot load and 0ms for segment recovery from offset 53441 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,285] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-12, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53441) with 1 segments, local-log-start-offset 0 and log-end-offset 53441 in 74ms (4/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,294] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,301] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-38, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=38, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (5/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,333] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Loading producer state till offset 53208 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,333] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53208 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,341] INFO [ProducerStateManager partition=financial_transactions-16] Loading producer state from snapshot file 'SnapshotFile(offset=53208, file=/tmp/kafka-logs/financial_transactions-16/00000000000000053208.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,349] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Producer state recovery took 8ms for snapshot load and 0ms for segment recovery from offset 53208 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,355] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-16, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53208) with 1 segments, local-log-start-offset 0 and log-end-offset 53208 in 52ms (6/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,367] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,375] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-27, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=27, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (7/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,386] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,388] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-39, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=39, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (8/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,395] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,399] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-42, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=42, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (9/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,405] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,413] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-9, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (10/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,424] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,427] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-22, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=22, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (11/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,446] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,448] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-26, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=26, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 19ms (12/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,453] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,455] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-40, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=40, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (13/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,459] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,461] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-47, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=47, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (14/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,466] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,468] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-15, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (15/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,475] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,480] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-10, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (16/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,513] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Loading producer state till offset 53486 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,515] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53486 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,516] INFO [ProducerStateManager partition=financial_transactions-11] Loading producer state from snapshot file 'SnapshotFile(offset=53486, file=/tmp/kafka-logs/financial_transactions-11/00000000000000053486.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,517] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53486 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,520] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-11, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53486) with 1 segments, local-log-start-offset 0 and log-end-offset 53486 in 29ms (17/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,538] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,541] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-2, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 17ms (18/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,547] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,551] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-18, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (19/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,570] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,573] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-46, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=46, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 19ms (20/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,591] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Loading producer state till offset 53591 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,591] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53591 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,592] INFO [ProducerStateManager partition=financial_transactions-1] Loading producer state from snapshot file 'SnapshotFile(offset=53591, file=/tmp/kafka-logs/financial_transactions-1/00000000000000053591.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,593] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53591 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,596] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-1, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53591) with 1 segments, local-log-start-offset 0 and log-end-offset 53591 in 23ms (21/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,601] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,602] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-12, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (22/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,608] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,613] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-23, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=23, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (23/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,621] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,632] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-14, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (24/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,640] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,645] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-5, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (25/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,649] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,651] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-41, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=41, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (26/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,657] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Loading producer state till offset 53580 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,657] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53580 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,657] INFO [ProducerStateManager partition=financial_transactions-17] Loading producer state from snapshot file 'SnapshotFile(offset=53580, file=/tmp/kafka-logs/financial_transactions-17/00000000000000053580.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,658] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53580 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,671] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-17, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53580) with 1 segments, local-log-start-offset 0 and log-end-offset 53580 in 19ms (27/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,677] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,680] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-1, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (28/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,684] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,687] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-19, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (29/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,695] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,697] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-3, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (30/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,712] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,716] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-16, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 17ms (31/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,741] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,765] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-25, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=25, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 47ms (32/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,781] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Loading producer state till offset 53502 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,787] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53502 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,797] INFO [ProducerStateManager partition=financial_transactions-15] Loading producer state from snapshot file 'SnapshotFile(offset=53502, file=/tmp/kafka-logs/financial_transactions-15/00000000000000053502.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,798] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Producer state recovery took 10ms for snapshot load and 0ms for segment recovery from offset 53502 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,802] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-15, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53502) with 1 segments, local-log-start-offset 0 and log-end-offset 53502 in 34ms (33/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,819] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Loading producer state till offset 53622 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,820] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53622 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,824] INFO [ProducerStateManager partition=financial_transactions-3] Loading producer state from snapshot file 'SnapshotFile(offset=53622, file=/tmp/kafka-logs/financial_transactions-3/00000000000000053622.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,826] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 53622 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,829] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-3, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53622) with 1 segments, local-log-start-offset 0 and log-end-offset 53622 in 27ms (34/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,840] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Loading producer state till offset 53252 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,846] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53252 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,846] INFO [ProducerStateManager partition=financial_transactions-10] Loading producer state from snapshot file 'SnapshotFile(offset=53252, file=/tmp/kafka-logs/financial_transactions-10/00000000000000053252.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,847] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53252 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,849] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-10, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53252) with 1 segments, local-log-start-offset 0 and log-end-offset 53252 in 19ms (35/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,856] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Loading producer state till offset 53309 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,857] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53309 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,858] INFO [ProducerStateManager partition=financial_transactions-19] Loading producer state from snapshot file 'SnapshotFile(offset=53309, file=/tmp/kafka-logs/financial_transactions-19/00000000000000053309.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,859] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53309 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,866] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-19, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53309) with 1 segments, local-log-start-offset 0 and log-end-offset 53309 in 16ms (36/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,874] INFO Deleted producer state snapshot /tmp/kafka-logs/_schemas-0/00000000000000000002.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 12:15:31,874] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 4 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,875] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 4 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,876] INFO [ProducerStateManager partition=_schemas-0] Loading producer state from snapshot file 'SnapshotFile(offset=4, file=/tmp/kafka-logs/_schemas-0/00000000000000000004.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,879] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 4 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,881] INFO Completed load of Log(dir=/tmp/kafka-logs/_schemas-0, topicId=RrE8eovWRKu4kLR3MRJ0fA, topic=_schemas, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=4) with 1 segments, local-log-start-offset 0 and log-end-offset 4 in 13ms (37/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,889] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,893] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-28, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=28, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (38/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,903] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,905] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-13, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (39/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,908] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,911] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-24, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=24, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (40/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,916] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,918] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-32, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=32, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (41/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,923] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Loading producer state till offset 53468 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,925] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53468 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,926] INFO [ProducerStateManager partition=financial_transactions-14] Loading producer state from snapshot file 'SnapshotFile(offset=53468, file=/tmp/kafka-logs/financial_transactions-14/00000000000000053468.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,927] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53468 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,931] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-14, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53468) with 1 segments, local-log-start-offset 0 and log-end-offset 53468 in 12ms (42/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,939] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Loading producer state till offset 53173 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,940] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53173 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,943] INFO [ProducerStateManager partition=financial_transactions-2] Loading producer state from snapshot file 'SnapshotFile(offset=53173, file=/tmp/kafka-logs/financial_transactions-2/00000000000000053173.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,946] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 0ms for segment recovery from offset 53173 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,950] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-2, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53173) with 1 segments, local-log-start-offset 0 and log-end-offset 53173 in 16ms (43/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,959] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,961] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-31, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=31, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (44/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,964] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,966] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-21, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=21, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (45/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,980] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 53808 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,981] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53808 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,982] INFO [ProducerStateManager partition=financial_transactions-0] Loading producer state from snapshot file 'SnapshotFile(offset=53808, file=/tmp/kafka-logs/financial_transactions-0/00000000000000053808.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,982] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 53808 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,984] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-0, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53808) with 1 segments, local-log-start-offset 0 and log-end-offset 53808 in 16ms (46/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,991] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Loading producer state till offset 53169 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,991] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53169 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,992] INFO [ProducerStateManager partition=financial_transactions-9] Loading producer state from snapshot file 'SnapshotFile(offset=53169, file=/tmp/kafka-logs/financial_transactions-9/00000000000000053169.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,994] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 53169 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,997] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-9, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53169) with 1 segments, local-log-start-offset 0 and log-end-offset 53169 in 13ms (47/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,007] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,014] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-6, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 17ms (48/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,020] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,022] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-11, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (49/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,027] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Loading producer state till offset 53395 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,028] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53395 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,029] INFO [ProducerStateManager partition=financial_transactions-5] Loading producer state from snapshot file 'SnapshotFile(offset=53395, file=/tmp/kafka-logs/financial_transactions-5/00000000000000053395.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:32,030] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 53395 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,032] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-5, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53395) with 1 segments, local-log-start-offset 0 and log-end-offset 53395 in 10ms (50/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,036] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,038] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-30, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=30, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (51/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,046] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Loading producer state till offset 53206 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,046] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53206 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,047] INFO [ProducerStateManager partition=financial_transactions-6] Loading producer state from snapshot file 'SnapshotFile(offset=53206, file=/tmp/kafka-logs/financial_transactions-6/00000000000000053206.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:32,047] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 53206 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,049] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-6, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53206) with 1 segments, local-log-start-offset 0 and log-end-offset 53206 in 9ms (52/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,053] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,055] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-43, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=43, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (53/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,071] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,077] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-7, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 20ms (54/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,083] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,086] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-33, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=33, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (55/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,089] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,090] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-36, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=36, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (56/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,093] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,096] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-45, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=45, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (57/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,104] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,107] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-0, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (58/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,109] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,113] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-20, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=20, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (59/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,117] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,119] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-4, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (60/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,131] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,133] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-8, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (61/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,138] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,141] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-49, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=49, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (62/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,150] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,155] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-34, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=34, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (63/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,159] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,162] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-48, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=48, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (64/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,167] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Loading producer state till offset 53258 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,167] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53258 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,168] INFO [ProducerStateManager partition=financial_transactions-4] Loading producer state from snapshot file 'SnapshotFile(offset=53258, file=/tmp/kafka-logs/financial_transactions-4/00000000000000053258.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:32,169] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53258 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,171] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-4, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53258) with 1 segments, local-log-start-offset 0 and log-end-offset 53258 in 8ms (65/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,177] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,180] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-44, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=44, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (66/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,186] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,188] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-37, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=37, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (67/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,199] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Loading producer state till offset 53438 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,201] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53438 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,201] INFO [ProducerStateManager partition=financial_transactions-7] Loading producer state from snapshot file 'SnapshotFile(offset=53438, file=/tmp/kafka-logs/financial_transactions-7/00000000000000053438.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:32,203] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53438 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,204] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-7, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53438) with 1 segments, local-log-start-offset 0 and log-end-offset 53438 in 15ms (68/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,207] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,208] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-35, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=35, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 3ms (69/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,213] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,215] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-17, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (70/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,220] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Loading producer state till offset 53130 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,221] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53130 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,221] INFO [ProducerStateManager partition=financial_transactions-8] Loading producer state from snapshot file 'SnapshotFile(offset=53130, file=/tmp/kafka-logs/financial_transactions-8/00000000000000053130.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:32,222] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53130 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,223] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-8, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53130) with 1 segments, local-log-start-offset 0 and log-end-offset 53130 in 8ms (71/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,228] INFO Loaded 71 logs in 1198ms (kafka.log.LogManager)
[2025-05-21 12:15:32,230] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-21 12:15:32,231] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-21 12:15:32,238] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-21 12:15:32,360] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 12:15:32,365] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 12:15:32,366] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,366] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 12:15:32,373] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,374] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 12:15:32,380] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 12:15:32,380] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 12:15:32,385] INFO [Broker id=4] Transitioning 71 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:32,387] INFO [Broker id=4] Creating new partition financial_transactions-13 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,402] INFO [Partition financial_transactions-13 broker=4] Log loaded for partition financial_transactions-13 with initial high watermark 53728 (kafka.cluster.Partition)
[2025-05-21 12:15:32,406] INFO [Broker id=4] Follower financial_transactions-13 starts at leader epoch 3 from offset 53728 with partition epoch 3 and high watermark 53728. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:32,407] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,414] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,415] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,417] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,422] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,423] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,423] INFO [Broker id=4] Creating new partition financial_transactions-17 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,426] INFO [Partition financial_transactions-17 broker=4] Log loaded for partition financial_transactions-17 with initial high watermark 53580 (kafka.cluster.Partition)
[2025-05-21 12:15:32,427] INFO [Broker id=4] Follower financial_transactions-17 starts at leader epoch 1 from offset 53580 with partition epoch 3 and high watermark 53580. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,429] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,430] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,431] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,432] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,433] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,434] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,435] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,437] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,438] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,438] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,440] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,440] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,441] INFO [Broker id=4] Creating new partition financial_transactions-0 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,444] INFO [Partition financial_transactions-0 broker=4] Log loaded for partition financial_transactions-0 with initial high watermark 53808 (kafka.cluster.Partition)
[2025-05-21 12:15:32,444] INFO [Broker id=4] Follower financial_transactions-0 starts at leader epoch 1 from offset 53808 with partition epoch 3 and high watermark 53808. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,447] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,448] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,454] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,457] INFO [Broker id=4] Creating new partition financial_transactions-4 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,463] INFO [Partition financial_transactions-4 broker=4] Log loaded for partition financial_transactions-4 with initial high watermark 53258 (kafka.cluster.Partition)
[2025-05-21 12:15:32,464] INFO [Broker id=4] Follower financial_transactions-4 starts at leader epoch 2 from offset 53258 with partition epoch 3 and high watermark 53258. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,464] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,466] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,467] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,468] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,469] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,470] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,470] INFO [Broker id=4] Creating new partition financial_transactions-8 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,471] INFO [Partition financial_transactions-8 broker=4] Log loaded for partition financial_transactions-8 with initial high watermark 53130 (kafka.cluster.Partition)
[2025-05-21 12:15:32,472] INFO [Broker id=4] Follower financial_transactions-8 starts at leader epoch 3 from offset 53130 with partition epoch 3 and high watermark 53130. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:32,473] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,476] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,477] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,478] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,480] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,482] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,483] INFO [Broker id=4] Creating new partition financial_transactions-12 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,486] INFO [Partition financial_transactions-12 broker=4] Log loaded for partition financial_transactions-12 with initial high watermark 53441 (kafka.cluster.Partition)
[2025-05-21 12:15:32,486] INFO [Broker id=4] Follower financial_transactions-12 starts at leader epoch 1 from offset 53441 with partition epoch 3 and high watermark 53441. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,487] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,488] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,488] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,489] INFO [Broker id=4] Creating new partition financial_transactions-14 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,490] INFO [Partition financial_transactions-14 broker=4] Log loaded for partition financial_transactions-14 with initial high watermark 53468 (kafka.cluster.Partition)
[2025-05-21 12:15:32,490] INFO [Broker id=4] Follower financial_transactions-14 starts at leader epoch 2 from offset 53468 with partition epoch 3 and high watermark 53468. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,491] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,492] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,493] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,493] INFO [Broker id=4] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-21 12:15:32,495] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 4 (kafka.cluster.Partition)
[2025-05-21 12:15:32,496] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 11 from offset 4 with partition epoch 18 and high watermark 4. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,497] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,498] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,498] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,499] INFO [Broker id=4] Creating new partition financial_transactions-18 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,500] INFO [Partition financial_transactions-18 broker=4] Log loaded for partition financial_transactions-18 with initial high watermark 53605 (kafka.cluster.Partition)
[2025-05-21 12:15:32,501] INFO [Broker id=4] Follower financial_transactions-18 starts at leader epoch 1 from offset 53605 with partition epoch 3 and high watermark 53605. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,502] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,504] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,505] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,505] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,507] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,507] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,507] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,508] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,509] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,509] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,510] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,511] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,511] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,514] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,515] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,516] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,518] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,518] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,519] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,520] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 4 (kafka.cluster.Partition)
[2025-05-21 12:15:32,521] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 11 from offset 4 with partition epoch 18 and high watermark 4. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,521] INFO [Broker id=4] Creating new partition financial_transactions-1 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,523] INFO [Partition financial_transactions-1 broker=4] Log loaded for partition financial_transactions-1 with initial high watermark 53591 (kafka.cluster.Partition)
[2025-05-21 12:15:32,523] INFO [Broker id=4] Follower financial_transactions-1 starts at leader epoch 3 from offset 53591 with partition epoch 3 and high watermark 53591. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:32,524] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,525] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,525] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,526] INFO [Broker id=4] Creating new partition financial_transactions-5 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,527] INFO [Partition financial_transactions-5 broker=4] Log loaded for partition financial_transactions-5 with initial high watermark 53395 (kafka.cluster.Partition)
[2025-05-21 12:15:32,528] INFO [Broker id=4] Follower financial_transactions-5 starts at leader epoch 2 from offset 53395 with partition epoch 3 and high watermark 53395. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,528] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,530] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,530] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,531] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,532] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,532] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,533] INFO [Broker id=4] Creating new partition financial_transactions-9 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,534] INFO [Partition financial_transactions-9 broker=4] Log loaded for partition financial_transactions-9 with initial high watermark 53169 (kafka.cluster.Partition)
[2025-05-21 12:15:32,535] INFO [Broker id=4] Follower financial_transactions-9 starts at leader epoch 2 from offset 53169 with partition epoch 3 and high watermark 53169. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,536] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,537] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,537] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,538] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,539] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,540] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,540] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,541] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,542] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,542] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,544] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,546] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,547] INFO [Broker id=4] Creating new partition financial_transactions-15 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,549] INFO [Partition financial_transactions-15 broker=4] Log loaded for partition financial_transactions-15 with initial high watermark 53502 (kafka.cluster.Partition)
[2025-05-21 12:15:32,550] INFO [Broker id=4] Follower financial_transactions-15 starts at leader epoch 2 from offset 53502 with partition epoch 3 and high watermark 53502. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,550] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,552] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,552] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,553] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,554] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,554] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,555] INFO [Broker id=4] Creating new partition financial_transactions-19 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,556] INFO [Partition financial_transactions-19 broker=4] Log loaded for partition financial_transactions-19 with initial high watermark 53309 (kafka.cluster.Partition)
[2025-05-21 12:15:32,556] INFO [Broker id=4] Follower financial_transactions-19 starts at leader epoch 2 from offset 53309 with partition epoch 3 and high watermark 53309. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,556] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,557] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,558] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,558] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,559] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,559] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,560] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,561] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,562] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,562] INFO [Broker id=4] Creating new partition financial_transactions-2 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,563] INFO [Partition financial_transactions-2 broker=4] Log loaded for partition financial_transactions-2 with initial high watermark 53173 (kafka.cluster.Partition)
[2025-05-21 12:15:32,565] INFO [Broker id=4] Follower financial_transactions-2 starts at leader epoch 2 from offset 53173 with partition epoch 3 and high watermark 53173. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,567] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,569] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,569] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,570] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,571] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,571] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,572] INFO [Broker id=4] Creating new partition financial_transactions-6 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,573] INFO [Partition financial_transactions-6 broker=4] Log loaded for partition financial_transactions-6 with initial high watermark 53206 (kafka.cluster.Partition)
[2025-05-21 12:15:32,573] INFO [Broker id=4] Follower financial_transactions-6 starts at leader epoch 2 from offset 53206 with partition epoch 3 and high watermark 53206. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,574] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,575] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,575] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,575] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,576] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,577] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,578] INFO [Broker id=4] Creating new partition financial_transactions-10 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,579] INFO [Partition financial_transactions-10 broker=4] Log loaded for partition financial_transactions-10 with initial high watermark 53252 (kafka.cluster.Partition)
[2025-05-21 12:15:32,579] INFO [Broker id=4] Follower financial_transactions-10 starts at leader epoch 1 from offset 53252 with partition epoch 3 and high watermark 53252. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,580] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,581] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,581] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,581] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,582] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,582] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,582] INFO [Broker id=4] Creating new partition financial_transactions-16 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,583] INFO [Partition financial_transactions-16 broker=4] Log loaded for partition financial_transactions-16 with initial high watermark 53208 (kafka.cluster.Partition)
[2025-05-21 12:15:32,583] INFO [Broker id=4] Follower financial_transactions-16 starts at leader epoch 2 from offset 53208 with partition epoch 3 and high watermark 53208. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,584] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,585] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,586] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,586] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,587] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,588] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,588] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,589] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,590] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,590] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,596] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,597] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,597] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,599] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,599] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,600] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,600] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,601] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,601] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,602] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,602] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,603] INFO [Broker id=4] Creating new partition financial_transactions-3 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,604] INFO [Partition financial_transactions-3 broker=4] Log loaded for partition financial_transactions-3 with initial high watermark 53622 (kafka.cluster.Partition)
[2025-05-21 12:15:32,604] INFO [Broker id=4] Follower financial_transactions-3 starts at leader epoch 1 from offset 53622 with partition epoch 3 and high watermark 53622. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,605] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,605] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,606] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,606] INFO [Broker id=4] Creating new partition financial_transactions-7 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,608] INFO [Partition financial_transactions-7 broker=4] Log loaded for partition financial_transactions-7 with initial high watermark 53438 (kafka.cluster.Partition)
[2025-05-21 12:15:32,609] INFO [Broker id=4] Follower financial_transactions-7 starts at leader epoch 1 from offset 53438 with partition epoch 3 and high watermark 53438. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,609] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,611] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,611] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,611] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,613] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,614] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,614] INFO [Broker id=4] Creating new partition financial_transactions-11 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,616] INFO [Partition financial_transactions-11 broker=4] Log loaded for partition financial_transactions-11 with initial high watermark 53486 (kafka.cluster.Partition)
[2025-05-21 12:15:32,616] INFO [Broker id=4] Follower financial_transactions-11 starts at leader epoch 2 from offset 53486 with partition epoch 3 and high watermark 53486. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,617] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,617] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,618] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,621] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-6, __consumer_offsets-35, financial_transactions-11, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:15:32,621] INFO [Broker id=4] Stopped fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-21 12:15:32,630] INFO [Broker id=4] Started fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-21 12:15:32,653] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,663] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,663] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,664] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,664] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,664] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,665] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,665] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,665] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,666] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,666] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,669] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,669] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,669] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,670] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,670] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,670] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,671] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,670] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,671] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,671] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,672] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,672] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,673] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,673] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,674] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,673] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,674] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,675] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,676] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,675] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,677] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,676] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,677] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,678] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,678] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,678] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,679] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,680] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,680] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,680] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,681] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,681] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,681] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,682] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,682] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,683] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,683] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,684] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,684] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,684] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,685] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,685] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,685] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,686] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,686] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,686] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,686] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,686] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,687] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,687] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,687] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,688] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,688] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,690] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,690] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,690] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,690] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,691] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,691] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,691] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,692] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,692] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,693] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,693] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,693] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,693] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,694] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,694] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,694] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,694] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,695] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,695] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,696] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,695] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,698] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,698] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,699] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,699] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,699] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,700] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,699] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,700] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,701] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,701] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,701] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,702] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,702] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,703] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,703] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,703] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,704] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,704] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,704] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,704] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,704] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,705] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,705] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,705] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,706] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,706] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,706] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,707] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,707] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,707] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,708] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,708] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,708] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,703] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,709] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,712] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,713] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,714] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,714] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,715] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,715] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,713] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,716] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,716] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,717] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,717] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,718] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,719] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,721] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,718] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,722] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,722] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,723] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,723] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,722] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,724] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,732] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,732] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,732] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,733] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,733] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,734] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,734] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,735] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-21 12:15:32,735] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,736] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,743] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-21 12:15:32,752] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-21 12:15:32,755] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-21 12:15:32,759] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:32,763] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-21 12:15:32,752] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 8091 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:32,849] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:15:32,850] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-21 12:15:32,852] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-21 12:15:32,852] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-21 12:15:32,853] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2025-05-21 12:15:32,854] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-21 12:15:32,856] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-21 12:15:32,861] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-21 12:15:32,862] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-21 12:15:32,862] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-21 12:15:32,862] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-21 12:15:32,863] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-21 12:15:32,864] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 12:15:32,864] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 12:15:32,864] INFO Kafka startTimeMs: 1747829732863 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 12:15:32,866] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-21 12:15:33,334] INFO [Broker id=4] Transitioning 71 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:33,336] INFO [Broker id=4] Follower financial_transactions-13 starts at leader epoch 4 from offset 53728 with partition epoch 4 and high watermark 53728. Current leader is 6. Previous leader Some(6) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:15:33,337] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 11 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,338] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 11 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,339] INFO [Broker id=4] Follower financial_transactions-17 starts at leader epoch 2 from offset 53580 with partition epoch 4 and high watermark 53580. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,340] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 8 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,345] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 8 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,349] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,350] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,351] INFO [Broker id=4] Follower financial_transactions-0 starts at leader epoch 2 from offset 53808 with partition epoch 4 and high watermark 53808. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,353] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 11 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,353] INFO [Broker id=4] Follower financial_transactions-4 starts at leader epoch 3 from offset 53258 with partition epoch 4 and high watermark 53258. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,354] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 12 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,355] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 12 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,356] INFO [Broker id=4] Follower financial_transactions-8 starts at leader epoch 4 from offset 53130 with partition epoch 4 and high watermark 53130. Current leader is 6. Previous leader Some(6) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:15:33,357] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 8 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,360] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 9 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:15:33,362] INFO [Broker id=4] Follower financial_transactions-12 starts at leader epoch 2 from offset 53441 with partition epoch 4 and high watermark 53441. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,366] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 11 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,366] INFO [Broker id=4] Follower financial_transactions-14 starts at leader epoch 3 from offset 53468 with partition epoch 4 and high watermark 53468. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,367] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 12 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,368] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 12 from offset 4 with partition epoch 19 and high watermark 4. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,369] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,369] INFO [Broker id=4] Follower financial_transactions-18 starts at leader epoch 2 from offset 53605 with partition epoch 4 and high watermark 53605. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,370] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,370] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 9 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:15:33,371] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,371] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 9 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:15:33,371] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 12 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,372] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,373] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 12 from offset 4 with partition epoch 19 and high watermark 4. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,373] INFO [Broker id=4] Follower financial_transactions-1 starts at leader epoch 4 from offset 53591 with partition epoch 4 and high watermark 53591. Current leader is 6. Previous leader Some(6) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:15:33,376] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 9 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:15:33,378] INFO [Broker id=4] Follower financial_transactions-5 starts at leader epoch 3 from offset 53395 with partition epoch 4 and high watermark 53395. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,379] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 8 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,381] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 11 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,384] INFO [Broker id=4] Follower financial_transactions-9 starts at leader epoch 3 from offset 53169 with partition epoch 4 and high watermark 53169. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,385] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 9 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:15:33,386] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,387] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 9 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:15:33,388] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 9 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:15:33,389] INFO [Broker id=4] Follower financial_transactions-15 starts at leader epoch 3 from offset 53502 with partition epoch 4 and high watermark 53502. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,392] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 11 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,393] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 11 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,395] INFO [Broker id=4] Follower financial_transactions-19 starts at leader epoch 3 from offset 53309 with partition epoch 4 and high watermark 53309. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,396] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 12 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,397] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,398] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,399] INFO [Broker id=4] Follower financial_transactions-2 starts at leader epoch 3 from offset 53173 with partition epoch 4 and high watermark 53173. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,401] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 9 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:15:33,401] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 11 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,402] INFO [Broker id=4] Follower financial_transactions-6 starts at leader epoch 3 from offset 53206 with partition epoch 4 and high watermark 53206. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,402] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,403] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,404] INFO [Broker id=4] Follower financial_transactions-10 starts at leader epoch 2 from offset 53252 with partition epoch 4 and high watermark 53252. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,404] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,405] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 8 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,405] INFO [Broker id=4] Follower financial_transactions-16 starts at leader epoch 3 from offset 53208 with partition epoch 4 and high watermark 53208. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,406] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 8 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,407] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,409] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,409] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 9 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:15:33,410] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 12 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,410] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 8 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,410] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,411] INFO [Broker id=4] Follower financial_transactions-3 starts at leader epoch 2 from offset 53622 with partition epoch 4 and high watermark 53622. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,412] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 12 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,413] INFO [Broker id=4] Follower financial_transactions-7 starts at leader epoch 2 from offset 53438 with partition epoch 4 and high watermark 53438. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,414] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,416] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 8 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,416] INFO [Broker id=4] Follower financial_transactions-11 starts at leader epoch 3 from offset 53486 with partition epoch 4 and high watermark 53486. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,417] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 12 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,417] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-6, __consumer_offsets-35, financial_transactions-11, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:15:33,418] INFO [Broker id=4] Stopped fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-21 12:15:33,447] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,453] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(financial_transactions-13 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),4,53728), __consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),11,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),11,0), financial_transactions-17 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,53580), __consumer_offsets-9 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),8,0), __consumer_offsets-42 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),8,0), __consumer_offsets-21 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-17 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), financial_transactions-0 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,53808), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),11,0), financial_transactions-4 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53258), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), financial_transactions-8 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),4,53130), __consumer_offsets-38 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),8,0), __consumer_offsets-1 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),9,0), financial_transactions-12 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,53441), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),11,0), financial_transactions-14 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53468), __consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), _schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,4), __consumer_offsets-45 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), financial_transactions-18 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,53605), __consumer_offsets-12 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-41 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),9,0), __consumer_offsets-24 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-20 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),9,0), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), __consumer_offsets-0 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,4), financial_transactions-1 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),4,53591), __consumer_offsets-25 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),9,0), financial_transactions-5 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53395), __consumer_offsets-8 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),8,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),11,0), financial_transactions-9 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53169), __consumer_offsets-4 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),9,0), __consumer_offsets-33 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-15 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),9,0), __consumer_offsets-48 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),9,0), financial_transactions-15 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53502), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),11,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),11,0), financial_transactions-19 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53309), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), __consumer_offsets-19 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-32 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), financial_transactions-2 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53173), __consumer_offsets-28 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),9,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),11,0), financial_transactions-6 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53206), __consumer_offsets-40 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-3 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), financial_transactions-10 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,53252), __consumer_offsets-36 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-47 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),8,0), financial_transactions-16 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53208), __consumer_offsets-14 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),8,0), __consumer_offsets-43 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-10 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-22 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),9,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), __consumer_offsets-31 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),8,0), __consumer_offsets-27 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), financial_transactions-3 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,53622), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), financial_transactions-7 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,53438), __consumer_offsets-6 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-35 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),8,0), financial_transactions-11 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53486), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:15:33,453] INFO [Broker id=4] Started fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-21 12:15:33,454] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,455] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,456] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,457] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,457] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,457] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,458] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,458] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,458] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,459] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,459] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,459] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,460] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,460] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,460] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,460] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,461] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,461] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,461] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,462] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,462] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,463] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,463] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,463] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,464] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,464] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,464] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,465] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,465] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,465] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,465] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,466] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,466] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,466] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,467] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,467] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,467] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,467] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,468] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,468] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,468] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,469] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,469] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,469] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,470] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,470] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,470] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,470] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,471] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,471] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,472] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,472] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,472] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,472] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,473] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,473] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,473] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,473] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,474] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,474] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,474] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,474] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,475] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,475] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,475] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,476] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,476] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,476] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,477] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,477] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,477] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,478] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,478] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,478] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,479] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,479] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,479] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,480] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,480] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,480] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,481] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,481] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,481] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,482] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,482] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,482] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,482] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,483] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,483] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,483] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,484] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,484] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,484] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,485] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,485] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,485] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,485] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,486] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,493] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,494] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,495] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,495] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,495] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,496] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,496] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,496] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,497] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,497] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,497] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,497] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,498] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,498] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,499] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,499] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,499] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,500] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,500] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,500] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,501] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,501] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,501] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,501] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,502] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,501] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,502] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,502] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,503] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,502] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,503] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,503] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,504] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,504] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,505] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,505] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,505] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,505] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,506] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,506] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,507] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,507] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,507] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,507] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,507] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,508] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,508] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,509] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,509] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,509] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,509] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,510] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,510] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,510] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,512] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,512] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,512] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,513] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,513] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,514] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,514] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,515] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,515] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,515] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,515] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,510] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,516] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,516] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,516] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,517] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,517] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,517] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,518] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,519] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,518] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,519] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,520] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,520] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,520] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,520] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,521] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,521] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,521] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,522] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,522] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,522] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,523] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,522] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,523] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,524] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,523] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,524] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,525] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,524] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,525] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,526] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,526] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,526] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,527] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,527] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,527] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,528] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,529] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,529] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,529] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,531] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,530] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,532] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,532] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,531] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,534] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,534] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,535] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,535] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,536] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,535] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,536] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,537] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,537] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,538] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,537] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,538] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,539] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,539] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,539] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,540] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,540] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,540] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,541] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,541] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,541] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,542] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,542] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,543] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,543] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,543] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,543] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,544] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,544] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,546] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,548] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,551] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,552] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,542] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,553] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,554] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,555] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,555] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,556] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,556] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,645] INFO [Broker id=4] Transitioning 2 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:33,646] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[6, 5], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:15:33,647] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=17, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:33,836] INFO [Broker id=4] Transitioning 27 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:33,837] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 5], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:33,839] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 5], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:33,840] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 4], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:33,841] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:33,841] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:33,843] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:33,843] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=5, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:33,844] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:33,845] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:33,849] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:33,851] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:33,854] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[6, 4], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:15:33,855] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 4], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:33,857] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:33,858] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=12, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=5, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:33,858] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:33,859] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:33,860] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:33,861] INFO [Broker id=4] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:33,861] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:33,862] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=18, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:33,863] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:33,863] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:33,864] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:33,864] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:33,865] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:33,866] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:33,867] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,867] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,868] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,869] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,869] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,869] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,870] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,870] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,871] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,872] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,872] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,872] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,868] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,874] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,875] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,875] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,876] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,876] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,877] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,878] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,879] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,880] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,881] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,879] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,881] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,882] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,882] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,883] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,884] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,885] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,885] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,885] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,886] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,886] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,887] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,887] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,888] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,886] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,888] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,889] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,889] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,889] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,890] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,890] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,889] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,892] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,891] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,893] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,893] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,893] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,894] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,893] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,895] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,895] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,896] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,896] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,895] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,897] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,898] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,899] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,190] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:34,191] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=17, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:34,334] INFO [Broker id=4] Transitioning 27 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:34,335] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[6, 5, 4], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:15:34,337] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:34,338] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:34,339] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:34,339] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:34,339] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,342] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,343] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:34,348] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:34,349] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,350] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,351] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,352] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[6, 4, 5], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:15:34,354] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:34,354] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:34,357] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=12, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:34,357] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:34,358] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,358] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,360] INFO [Broker id=4] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,362] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,364] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=18, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:34,367] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,368] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:34,370] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,372] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:34,374] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,377] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,378] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,378] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,378] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,379] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,383] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,384] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,392] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,392] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,393] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,393] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,393] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,394] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,394] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,393] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,394] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,395] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,396] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,397] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,399] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,400] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,400] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,401] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,401] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,401] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,401] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,402] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,403] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,405] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,406] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,406] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,407] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,407] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,407] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,408] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,406] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,414] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,414] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,419] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,410] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,421] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,422] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,423] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,423] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,425] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,424] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,426] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,426] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,427] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,426] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,427] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,427] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,427] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,428] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,429] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,429] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,430] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,695] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:34,697] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:34,698] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,700] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,703] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,834] INFO [Broker id=4] Transitioning 42 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:34,837] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:34,838] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,838] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:34,839] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:34,842] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,843] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,843] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,844] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,844] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,844] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 5], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:34,845] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 5], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:34,845] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,846] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,847] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,847] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=10, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:34,848] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,848] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:34,849] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,849] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:34,850] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,850] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,850] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:34,851] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,852] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:34,852] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5, 4], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,853] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,854] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[6, 5], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:15:34,854] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,855] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=3, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:34,855] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 5], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:34,856] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,856] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,857] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:34,857] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=5, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:34,857] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:34,858] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,858] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,859] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:34,859] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:34,860] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,860] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,860] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,861] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,862] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,862] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,862] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,863] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,862] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,863] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,863] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,864] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,864] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,864] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,864] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,865] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,865] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,865] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,866] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,866] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,866] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,867] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,867] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,866] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,867] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,868] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,868] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,868] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,868] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,869] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,869] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,869] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,869] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,870] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,870] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,870] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,871] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,871] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,872] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,871] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,872] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,873] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,873] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,873] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,873] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,874] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,874] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,874] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,875] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,874] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,875] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,876] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,875] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,876] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,877] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,876] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,877] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,877] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,877] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,878] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,878] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,878] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,878] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,879] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,879] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,879] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,880] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,879] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,880] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,880] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,880] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,881] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,881] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,881] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,882] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,882] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,882] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,883] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,884] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,883] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,884] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,884] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,884] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,885] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,885] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,885] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,886] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,886] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,886] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,887] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,887] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,887] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,887] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,231] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:35,232] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:35,233] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,233] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,233] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,334] INFO [Broker id=4] Transitioning 41 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:35,335] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:35,336] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:35,336] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:35,337] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:35,337] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:35,338] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:35,341] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,342] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5, 4], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,343] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:35,344] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:35,345] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:35,346] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:35,346] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5, 4], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,347] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,348] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=10, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:35,348] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5, 4], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,349] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:35,349] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=6, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:35,349] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:35,350] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,353] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,355] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:35,357] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:35,357] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:35,358] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:35,358] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[6, 5, 4], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:15:35,359] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5, 4], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,359] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=3, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:35,359] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:35,360] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=6, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:35,360] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:35,360] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:35,361] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:35,361] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:35,361] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5, 4], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,362] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:35,362] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:35,362] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:35,362] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:35,363] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,363] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:35,364] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,364] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,365] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,365] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,365] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,366] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,366] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,366] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,367] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,367] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,367] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,367] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,368] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,367] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,368] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,368] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,368] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,369] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,370] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,370] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,371] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,372] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,372] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,373] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,372] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,374] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,374] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,374] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,374] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,375] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,375] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,376] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,377] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,377] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,377] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,378] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,378] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,378] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,379] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,379] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,379] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,379] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,380] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,381] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,381] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,380] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,381] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,382] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,382] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,382] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,383] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,382] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,384] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,383] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,387] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,387] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,388] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,388] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,389] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,389] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,388] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,390] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,391] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,389] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,392] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,392] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,393] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,393] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,394] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,394] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,394] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,395] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,395] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,396] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,397] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,397] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,397] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,398] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,398] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,399] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,399] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,399] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,397] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,400] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,401] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,401] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,402] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,420] INFO [Broker id=4] Transitioning 24 partition(s) to local leaders. (state.change.logger)
[2025-05-21 12:20:30,425] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-16, _schemas-0, __consumer_offsets-13, financial_transactions-16, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-29, __consumer_offsets-30, financial_transactions-1, __consumer_offsets-26, financial_transactions-5, __consumer_offsets-7, __consumer_offsets-39, __consumer_offsets-5, __consumer_offsets-37, financial_transactions-8, financial_transactions-9, __consumer_offsets-2, __consumer_offsets-34) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:20:30,431] INFO [Broker id=4] Leader financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 5 from offset 53728 with partition epoch 7, high watermark 53728, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,438] INFO [Broker id=4] Leader __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 22, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,443] INFO [Broker id=4] Leader _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) starts at leader epoch 13 from offset 6 with partition epoch 22, high watermark 6, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,447] INFO [Broker id=4] Leader __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 22, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:20:30,451] INFO [Broker id=4] Leader financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 4 from offset 53208 with partition epoch 7, high watermark 53208, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:20:30,455] INFO [Broker id=4] Leader __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 22, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:20:30,458] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-4 has an older epoch (3) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,459] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-4 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,459] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-14 has an older epoch (3) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,459] INFO [Broker id=4] Leader __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 22, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:20:30,460] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-14 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,460] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-21 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,460] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-21 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,461] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-17 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,461] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-17 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,461] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-45 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,462] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-45 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,462] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-12 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,463] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-12 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,463] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-24 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,463] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-24 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,464] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-15 has an older epoch (3) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,464] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-15 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,464] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-19 has an older epoch (3) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,465] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-19 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,465] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-19 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,465] INFO [Broker id=4] Leader __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 22, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:20:30,465] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-19 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,466] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-32 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,466] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-32 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,466] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-2 has an older epoch (3) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,467] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-2 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,467] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-6 has an older epoch (3) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,468] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-6 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,468] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-40 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,468] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-40 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,469] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-3 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,469] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-3 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,469] INFO [Broker id=4] Leader __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 22, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,469] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-36 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,470] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-36 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,470] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-43 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,471] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-43 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,471] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-10 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,471] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-10 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,472] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-0 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,472] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-0 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,472] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-27 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,473] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-27 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,473] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-6 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,474] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-6 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,474] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-11 has an older epoch (3) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,474] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-11 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,474] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-33 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,475] INFO [Broker id=4] Leader __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 22, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,475] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-33 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,480] INFO [Broker id=4] Leader __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 22, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,485] INFO [Broker id=4] Leader __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 5 with partition epoch 22, high watermark 5, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,489] INFO [Broker id=4] Leader __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 22, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:20:30,493] INFO [Broker id=4] Leader financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 5 from offset 53591 with partition epoch 7, high watermark 53591, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,498] INFO [Broker id=4] Leader __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 22, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,502] INFO [Broker id=4] Leader financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 4 from offset 53395 with partition epoch 7, high watermark 53395, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:20:30,506] INFO [Broker id=4] Leader __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 22, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:20:30,511] INFO [Broker id=4] Leader __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 22, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,515] INFO [Broker id=4] Leader __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 22, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,519] INFO [Broker id=4] Leader __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 22, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:20:30,523] INFO [Broker id=4] Leader financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 5 from offset 53130 with partition epoch 7, high watermark 53130, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,526] INFO [Broker id=4] Leader financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 4 from offset 53169 with partition epoch 7, high watermark 53169, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:20:30,531] INFO [Broker id=4] Leader __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 22, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,534] INFO [Broker id=4] Leader __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 22, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:20:30,538] INFO [Broker id=4] Transitioning 23 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:20:30,539] INFO [Broker id=4] Follower financial_transactions-14 starts at leader epoch 4 from offset 53468 with partition epoch 7 and high watermark 53468. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,539] INFO [Broker id=4] Follower financial_transactions-15 starts at leader epoch 4 from offset 53502 with partition epoch 7 and high watermark 53502. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,539] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,540] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,540] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,540] INFO [Broker id=4] Follower financial_transactions-19 starts at leader epoch 4 from offset 53309 with partition epoch 7 and high watermark 53309. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,541] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,541] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,541] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,542] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,542] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,542] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,543] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,543] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,543] INFO [Broker id=4] Follower financial_transactions-2 starts at leader epoch 4 from offset 53173 with partition epoch 7 and high watermark 53173. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,544] INFO [Broker id=4] Follower financial_transactions-4 starts at leader epoch 4 from offset 53258 with partition epoch 7 and high watermark 53258. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,544] INFO [Broker id=4] Follower financial_transactions-6 starts at leader epoch 4 from offset 53206 with partition epoch 7 and high watermark 53206. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,544] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,544] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,545] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,545] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,546] INFO [Broker id=4] Follower financial_transactions-11 starts at leader epoch 4 from offset 53486 with partition epoch 7 and high watermark 53486. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,546] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,547] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(financial_transactions-14, financial_transactions-15, __consumer_offsets-45, __consumer_offsets-43, __consumer_offsets-12, financial_transactions-19, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-0, __consumer_offsets-27, financial_transactions-2, financial_transactions-4, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-6, __consumer_offsets-3, __consumer_offsets-36, financial_transactions-11, __consumer_offsets-33) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:20:30,548] INFO [Broker id=4] Stopped fetchers as part of become-follower for 23 partitions (state.change.logger)
[2025-05-21 12:20:30,554] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,554] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(financial_transactions-14 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,53468), financial_transactions-15 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,53502), __consumer_offsets-45 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-43 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-12 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), financial_transactions-19 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,53309), __consumer_offsets-10 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-24 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-21 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-19 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-17 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-32 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-0 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-27 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), financial_transactions-2 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,53173), financial_transactions-4 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,53258), financial_transactions-6 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,53206), __consumer_offsets-40 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-6 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-3 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-36 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), financial_transactions-11 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,53486), __consumer_offsets-33 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:20:30,554] INFO [Broker id=4] Started fetchers as part of become-follower for 23 partitions (state.change.logger)
[2025-05-21 12:20:30,554] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,555] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,555] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,555] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,556] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,556] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,557] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,557] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,557] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,558] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,558] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,558] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,558] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,559] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,559] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,559] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,559] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,560] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,560] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,560] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,560] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,561] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,561] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,561] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,562] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,562] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,562] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,562] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,563] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,563] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,563] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,564] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,566] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 16 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,567] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,567] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 13 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,568] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,568] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 46 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,568] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,569] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 11 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,569] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,569] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 44 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,569] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,570] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 23 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,570] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,570] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 49 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,571] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,571] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 18 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,571] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,571] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 29 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,572] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,572] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 30 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,572] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-16 in 4 milliseconds for epoch 13, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,572] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,573] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 26 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,573] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,573] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-13 in 5 milliseconds for epoch 12, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,573] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 7 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,574] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-46 in 5 milliseconds for epoch 12, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,574] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,574] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-11 in 5 milliseconds for epoch 12, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,574] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 39 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,575] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,575] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-44 in 5 milliseconds for epoch 12, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,575] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 5 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,576] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,576] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-23 in 6 milliseconds for epoch 13, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,576] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 37 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,577] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-49 in 6 milliseconds for epoch 13, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,577] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,577] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-18 in 6 milliseconds for epoch 13, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,577] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 2 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,578] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,578] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 34 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,579] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,579] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,580] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,581] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,581] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,582] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,582] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,582] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,583] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,583] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,583] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,583] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,584] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,584] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,584] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,585] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,585] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,585] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,585] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,586] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,586] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,586] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,587] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,587] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,587] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,587] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,587] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,588] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,588] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,588] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,589] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,589] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,589] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,592] INFO Loaded member MemberMetadata(memberId=sr-1-99122467-ec1a-4fc4-88f0-59fda8f60096, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 1. (kafka.coordinator.group.GroupMetadata$)
[2025-05-21 12:20:30,592] INFO Loaded member MemberMetadata(memberId=sr-1-3027da35-fa63-4fb3-9cba-2acfe293f9a1, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 3. (kafka.coordinator.group.GroupMetadata$)
[2025-05-21 12:20:30,592] INFO Loaded member MemberMetadata(memberId=sr-1-960e2a90-cf63-43a4-b2ba-89aa2499d218, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.8, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 5. (kafka.coordinator.group.GroupMetadata$)
[2025-05-21 12:20:30,594] INFO [GroupCoordinator 4]: Loading group metadata for schema-registry with generation 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,595] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-29 in 23 milliseconds for epoch 13, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,596] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-30 in 23 milliseconds for epoch 12, of which 22 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,596] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-26 in 23 milliseconds for epoch 13, of which 23 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,596] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-7 in 22 milliseconds for epoch 12, of which 22 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,597] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-39 in 22 milliseconds for epoch 13, of which 21 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,597] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-5 in 21 milliseconds for epoch 13, of which 21 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,597] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-37 in 20 milliseconds for epoch 12, of which 20 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,597] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-2 in 19 milliseconds for epoch 13, of which 19 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,598] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-34 in 19 milliseconds for epoch 12, of which 19 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,598] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,598] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,598] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,599] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,599] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,599] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,599] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,599] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,599] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,600] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,600] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,600] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,600] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,600] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,601] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,601] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:25:29,500] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:55,686] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 5 (__consumer_offsets-29) (reason: Removing member sr-1-960e2a90-cf63-43a4-b2ba-89aa2499d218 on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:55,690] INFO [GroupCoordinator 4]: Group schema-registry with generation 6 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:55,702] INFO [GroupCoordinator 4]: Member MemberMetadata(memberId=sr-1-960e2a90-cf63-43a4-b2ba-89aa2499d218, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.8, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:57,804] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-21 12:47:57,824] INFO [BrokerServer id=4] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
[2025-05-21 12:47:57,826] INFO [BrokerServer id=4] shutting down (kafka.server.BrokerServer)
[2025-05-21 12:47:57,831] INFO [BrokerLifecycleManager id=4] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:47:57,893] INFO [Broker id=4] Transitioning 36 partition(s) to local leaders. (state.change.logger)
[2025-05-21 12:47:57,908] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, financial_transactions-0, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, financial_transactions-16, __consumer_offsets-41, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:47:57,910] INFO [BrokerLifecycleManager id=4] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:47:57,913] INFO [Broker id=4] Skipped the become-leader state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=4, leaderEpoch=5, isr=[5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 255874, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,913] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-9 has an older epoch (8) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:57,915] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-9 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:57,915] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-42 has an older epoch (8) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:57,916] INFO [Broker id=4] Leader __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 21, high watermark 0, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:57,918] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-42 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:57,918] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-17 has an older epoch (2) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:57,919] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-17 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:57,920] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-38 has an older epoch (8) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:57,925] INFO [Broker id=4] Leader __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 21, high watermark 0, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:57,927] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-38 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:57,934] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-18 has an older epoch (2) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:57,936] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-18 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:57,940] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[5, 4], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,942] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[5, 4], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,944] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[4, 5], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,946] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[4, 5], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,947] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[5, 4], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,948] INFO [Broker id=4] Leader financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 3 from offset 257197 with partition epoch 7, high watermark 257197, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:47:57,953] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[4, 5], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,953] INFO [Broker id=4] Leader __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 21, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:57,963] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[4, 5], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,964] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[5, 4], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,965] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[4, 5], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,965] INFO [Broker id=4] Skipped the become-leader state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=4, leaderEpoch=5, isr=[4, 5], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 255525, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,966] INFO [Broker id=4] Leader __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 21, high watermark 0, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,008] INFO [Broker id=4] Leader financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 3 from offset 255590 with partition epoch 7, high watermark 255590, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:47:58,013] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[4, 5], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,026] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[4, 5], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,014] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Node 6 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,034] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Cancelled in-flight FETCH request with correlation id 68534 due to node 6 being disconnected (elapsed time since creation: 77ms, elapsed time since send: 77ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,036] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Client requested connection close from node 6 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,032] INFO [BrokerLifecycleManager id=4] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:47:58,040] INFO [BrokerLifecycleManager id=4] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,042] INFO [BrokerLifecycleManager id=4] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:47:58,043] INFO [broker-4-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,030] INFO [Broker id=4] Skipped the become-leader state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[4, 5], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 6, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,044] INFO [SocketServer listenerType=BROKER, nodeId=4] Stopping socket server request processors (kafka.network.SocketServer)
[2025-05-21 12:47:58,039] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Error sending fetch request (sessionId=25873741, epoch=68534) to node 6: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 6 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 12:47:58,081] INFO [SocketServer listenerType=BROKER, nodeId=4] Stopped socket server request processors (kafka.network.SocketServer)
[2025-05-21 12:47:58,059] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Node 5 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,111] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Cancelled in-flight FETCH request with correlation id 67967 due to node 5 being disconnected (elapsed time since creation: 4ms, elapsed time since send: 4ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,113] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,048] INFO [broker-4-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,047] INFO [Broker id=4] Skipped the become-leader state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=4, leaderEpoch=4, isr=[5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 255412, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,046] INFO [broker-4-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,115] INFO [Broker id=4] Leader __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 21, high watermark 0, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,114] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=393574748, epoch=67967) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 12:47:58,116] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=4, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={financial_transactions-11=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=256046, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[4], lastFetchedEpoch=Optional[4])}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=393574748, epoch=67967), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 12:47:58,118] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 12:47:58,095] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=4, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=0e8v3fGFR_uwy9DAR-lNZA:financial_transactions-18, replaced=, metadata=(sessionId=25873741, epoch=68534), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 6 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 12:47:58,122] INFO [Broker id=4] Leader __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 21, high watermark 0, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,131] INFO [Broker id=4] Leader __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 21, high watermark 0, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,144] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[4, 5], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,145] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[5, 4], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,145] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[5, 4], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 6, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,146] INFO [Broker id=4] Skipped the become-leader state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=4, leaderEpoch=5, isr=[5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 255371, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,147] INFO [Broker id=4] Leader __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 21, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,154] INFO [Broker id=4] Skipped the become-leader state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=4, leaderEpoch=4, isr=[5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 254990, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,155] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[5, 4], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,156] INFO [Broker id=4] Leader financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 3 from offset 255844 with partition epoch 7, high watermark 255844, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:47:58,163] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[4, 5], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,164] INFO [Broker id=4] Skipped the become-leader state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=4, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 255435, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,165] INFO [Broker id=4] Leader __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 21, high watermark 0, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,169] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[5, 4], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,170] INFO [Broker id=4] Transitioning 35 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:47:58,171] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:58,171] INFO [Broker id=4] Follower financial_transactions-17 starts at leader epoch 3 from offset 255866 with partition epoch 7 and high watermark 255866. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:47:58,171] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:58,172] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 9 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,173] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 9 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,173] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:58,174] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:58,174] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:58,175] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:58,175] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:58,175] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:58,176] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:58,176] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:58,177] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 9 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,177] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:58,178] INFO [Broker id=4] Follower financial_transactions-10 starts at leader epoch 3 from offset 256290 with partition epoch 7 and high watermark 256290. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:47:58,178] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:58,178] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 9 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,179] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:58,179] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:58,180] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 9 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,180] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:58,181] INFO [Broker id=4] Follower financial_transactions-18 starts at leader epoch 3 from offset 255590 with partition epoch 7 and high watermark 255590. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:47:58,181] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:58,182] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:58,182] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:58,182] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 9 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,183] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:58,183] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:58,184] INFO [Broker id=4] Follower financial_transactions-3 starts at leader epoch 3 from offset 255715 with partition epoch 7 and high watermark 255715. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:47:58,184] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 9 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,184] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:58,185] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 9 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,185] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:58,185] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:58,187] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-47, __consumer_offsets-14, financial_transactions-17, financial_transactions-18, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-31, financial_transactions-3, __consumer_offsets-8, __consumer_offsets-38, __consumer_offsets-35, financial_transactions-10) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:47:58,187] INFO [Broker id=4] Stopped fetchers as part of become-follower for 12 partitions (state.change.logger)
[2025-05-21 12:47:58,188] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,188] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),9,0), __consumer_offsets-14 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),9,0), financial_transactions-17 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,255866), financial_transactions-18 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,255590), __consumer_offsets-9 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),9,0), __consumer_offsets-42 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),9,0), __consumer_offsets-31 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),9,0), financial_transactions-3 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,255715), __consumer_offsets-8 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),9,0), __consumer_offsets-38 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),9,0), __consumer_offsets-35 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),9,0), financial_transactions-10 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),3,256290)) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:47:58,190] INFO [Broker id=4] Started fetchers as part of become-follower for 12 partitions (state.change.logger)
[2025-05-21 12:47:58,189] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:47:58,191] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,191] INFO [ReplicaFetcherThread-0-6]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,192] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:47:58,192] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,193] INFO [ReplicaFetcherThread-0-6]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,193] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:47:58,193] INFO [ReplicaFetcherThread-0-6]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,193] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,195] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:47:58,195] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,196] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:47:58,196] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,196] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:47:58,197] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,197] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:47:58,197] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,198] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:47:58,199] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 15 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,200] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,200] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Node 5 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,200] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 48 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,201] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-15 in 1 milliseconds for epoch 10, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,201] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Connection to node 5 (kafka-broker-2/172.19.0.3:19092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,201] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,202] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,202] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds for epoch 10, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,202] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 28 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,202] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=393574748, epoch=INITIAL) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to kafka-broker-2:19092 (id: 5 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 12:47:58,203] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=4, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={financial_transactions-17=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=255866, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[2]), financial_transactions-10=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=256290, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[2]), financial_transactions-18=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=255590, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[2]), financial_transactions-3=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=255715, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[2]), __consumer_offsets-9=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[9], lastFetchedEpoch=Optional.empty), __consumer_offsets-42=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[9], lastFetchedEpoch=Optional.empty), __consumer_offsets-38=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[9], lastFetchedEpoch=Optional.empty), __consumer_offsets-47=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[9], lastFetchedEpoch=Optional.empty), __consumer_offsets-14=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[9], lastFetchedEpoch=Optional.empty), __consumer_offsets-31=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[9], lastFetchedEpoch=Optional.empty), __consumer_offsets-8=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[9], lastFetchedEpoch=Optional.empty), __consumer_offsets-35=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[9], lastFetchedEpoch=Optional.empty)}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=393574748, epoch=INITIAL), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to kafka-broker-2:19092 (id: 5 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 12:47:58,203] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,204] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 25 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,205] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,205] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-28 in 1 milliseconds for epoch 10, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,206] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 41 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,206] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-25 in 1 milliseconds for epoch 10, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,206] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,207] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 22 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,207] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds for epoch 10, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,207] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,208] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 4 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,208] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-22 in 0 milliseconds for epoch 10, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,208] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,209] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 20 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,209] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds for epoch 10, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,210] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,210] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 1 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,211] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-20 in 1 milliseconds for epoch 10, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,211] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,211] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,212] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,211] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds for epoch 10, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,212] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,213] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,213] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,213] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,214] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,214] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,214] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,215] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,215] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,215] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,216] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,216] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,216] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,217] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,217] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,217] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,218] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,217] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,218] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,219] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,218] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,219] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,219] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,220] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,220] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,220] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,220] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,221] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,221] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,222] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,221] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,222] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,223] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,223] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,223] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,224] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,225] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,225] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,225] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,225] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,226] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,227] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,227] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,226] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,227] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,228] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,228] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,228] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,229] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,229] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,230] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,231] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,231] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,230] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,232] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,233] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,233] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,232] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,233] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,234] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,234] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,234] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,235] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,235] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,235] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,235] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,235] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,236] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,236] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,237] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,238] INFO [data-plane Kafka Request Handler on Broker 4], shutting down (kafka.server.KafkaRequestHandlerPool)
[2025-05-21 12:47:58,239] INFO [data-plane Kafka Request Handler on Broker 4], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2025-05-21 12:47:58,241] INFO [ExpirationReaper-4-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,242] INFO [ExpirationReaper-4-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,242] INFO [ExpirationReaper-4-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,243] INFO [KafkaApi-4] Shutdown complete. (kafka.server.KafkaApis)
[2025-05-21 12:47:58,248] INFO [TransactionCoordinator id=4] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 12:47:58,249] INFO [Transaction State Manager 4]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2025-05-21 12:47:58,249] INFO [TxnMarkerSenderThread-4]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 12:47:58,250] INFO [TxnMarkerSenderThread-4]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 12:47:58,250] INFO [TxnMarkerSenderThread-4]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 12:47:58,253] INFO [TransactionCoordinator id=4] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 12:47:58,254] INFO [GroupCoordinator 4]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,255] INFO [ExpirationReaper-4-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,256] INFO [ExpirationReaper-4-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,256] INFO [ExpirationReaper-4-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,257] INFO [ExpirationReaper-4-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,258] INFO [ExpirationReaper-4-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,258] INFO [ExpirationReaper-4-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,259] INFO [GroupCoordinator 4]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,260] INFO [AssignmentsManager id=4]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,260] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,261] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,261] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,262] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 12:47:58,262] INFO [AssignmentsManager id=4]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,264] INFO [ReplicaManager broker=4] Shutting down (kafka.server.ReplicaManager)
[2025-05-21 12:47:58,265] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 12:47:58,265] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 12:47:58,265] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 12:47:58,266] INFO [ReplicaFetcherManager on broker 4] shutting down (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:47:58,267] INFO [ReplicaFetcherThread-0-5]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,268] INFO [ReplicaFetcherThread-0-5]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,268] INFO [ReplicaFetcherThread-0-5]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,271] INFO [ReplicaFetcherManager on broker 4] shutdown completed (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:47:58,271] INFO [ReplicaAlterLogDirsManager on broker 4] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-21 12:47:58,273] INFO [ReplicaAlterLogDirsManager on broker 4] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-21 12:47:58,273] INFO [ExpirationReaper-4-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,273] INFO [ExpirationReaper-4-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,273] INFO [ExpirationReaper-4-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,274] INFO [ExpirationReaper-4-RemoteFetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,275] INFO [ExpirationReaper-4-RemoteFetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,275] INFO [ExpirationReaper-4-RemoteFetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,275] INFO [ExpirationReaper-4-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,276] INFO [ExpirationReaper-4-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,276] INFO [ExpirationReaper-4-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,277] INFO [ExpirationReaper-4-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,277] INFO [ExpirationReaper-4-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,277] INFO [ExpirationReaper-4-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,278] INFO [ExpirationReaper-4-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,279] INFO [ExpirationReaper-4-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,279] INFO [ExpirationReaper-4-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,286] INFO [AddPartitionsToTxnSenderThread-4]: Shutting down (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 12:47:58,286] INFO [AddPartitionsToTxnSenderThread-4]: Stopped (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 12:47:58,286] INFO [AddPartitionsToTxnSenderThread-4]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 12:47:58,288] INFO [ReplicaManager broker=4] Shut down completely (kafka.server.ReplicaManager)
[2025-05-21 12:47:58,289] INFO [broker-4-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,289] INFO [broker-4-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,289] INFO [broker-4-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,290] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 12:47:58,291] INFO [broker-4-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,291] INFO [broker-4-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,291] INFO [broker-4-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,293] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 12:47:58,294] INFO Shutting down. (kafka.log.LogManager)
[2025-05-21 12:47:58,295] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
[2025-05-21 12:47:58,296] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 12:47:58,296] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 12:47:58,296] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 12:47:58,318] INFO [ProducerStateManager partition=financial_transactions-15] Wrote producer snapshot at offset 256237 with 0 producer ids in 7 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,421] INFO [ProducerStateManager partition=financial_transactions-0] Wrote producer snapshot at offset 257213 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,460] INFO [ProducerStateManager partition=financial_transactions-3] Wrote producer snapshot at offset 255715 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,472] INFO [ProducerStateManager partition=financial_transactions-10] Wrote producer snapshot at offset 256290 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,477] INFO [ProducerStateManager partition=financial_transactions-12] Wrote producer snapshot at offset 255604 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,484] INFO [ProducerStateManager partition=financial_transactions-13] Wrote producer snapshot at offset 255889 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,500] INFO [ProducerStateManager partition=financial_transactions-19] Wrote producer snapshot at offset 255167 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,505] INFO [ProducerStateManager partition=financial_transactions-5] Wrote producer snapshot at offset 255005 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,510] INFO [ProducerStateManager partition=financial_transactions-6] Wrote producer snapshot at offset 255175 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,529] INFO [ProducerStateManager partition=financial_transactions-17] Wrote producer snapshot at offset 255866 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,534] INFO [ProducerStateManager partition=financial_transactions-7] Wrote producer snapshot at offset 255844 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,549] INFO [ProducerStateManager partition=__consumer_offsets-29] Wrote producer snapshot at offset 6 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,581] INFO [ProducerStateManager partition=financial_transactions-4] Wrote producer snapshot at offset 255335 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,597] INFO [ProducerStateManager partition=financial_transactions-18] Wrote producer snapshot at offset 255590 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,606] INFO [ProducerStateManager partition=financial_transactions-9] Wrote producer snapshot at offset 255457 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,612] INFO [ProducerStateManager partition=financial_transactions-1] Wrote producer snapshot at offset 255384 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,617] INFO [ProducerStateManager partition=financial_transactions-14] Wrote producer snapshot at offset 255812 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,630] INFO [ProducerStateManager partition=financial_transactions-11] Wrote producer snapshot at offset 256046 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,644] INFO [ProducerStateManager partition=_schemas-0] Wrote producer snapshot at offset 6 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,653] INFO [ProducerStateManager partition=financial_transactions-8] Wrote producer snapshot at offset 255535 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,662] INFO [ProducerStateManager partition=financial_transactions-2] Wrote producer snapshot at offset 255541 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,671] INFO [ProducerStateManager partition=financial_transactions-16] Wrote producer snapshot at offset 255425 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,717] INFO Shutdown complete. (kafka.log.LogManager)
[2025-05-21 12:47:58,719] INFO [broker-4-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,720] INFO [broker-4-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,720] INFO [broker-4-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,720] INFO [broker-4-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,721] INFO [broker-4-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,721] INFO [broker-4-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,723] INFO [broker-4-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,723] INFO [broker-4-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,723] INFO [broker-4-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,724] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,724] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,724] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,726] INFO [SocketServer listenerType=BROKER, nodeId=4] Shutting down socket server (kafka.network.SocketServer)
[2025-05-21 12:47:58,739] INFO [SocketServer listenerType=BROKER, nodeId=4] Shutdown completed (kafka.network.SocketServer)
[2025-05-21 12:47:58,741] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
[2025-05-21 12:47:58,742] INFO [BrokerLifecycleManager id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,743] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 12:47:58,744] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 12:47:58,744] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 12:47:58,745] INFO [SharedServer id=4] Stopping SharedServer (kafka.server.SharedServer)
[2025-05-21 12:47:58,746] INFO [MetadataLoader id=4] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,747] INFO [SnapshotGenerator id=4] close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,748] INFO [SnapshotGenerator id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,748] INFO [MetadataLoader id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,750] INFO [SnapshotGenerator id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,750] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 12:47:58,761] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 12:47:58,761] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 12:47:58,762] INFO [kafka-4-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 12:47:58,762] INFO [RaftManager id=4] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 12:47:58,763] INFO [RaftManager id=4] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 12:47:58,763] INFO [RaftManager id=4] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 12:47:58,764] INFO [kafka-4-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 12:47:58,764] INFO [kafka-4-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 12:47:58,769] INFO [kafka-4-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 12:47:58,769] INFO [kafka-4-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 12:47:58,769] INFO [kafka-4-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 12:47:58,774] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 12472 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,777] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-21 12:47:58,777] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2025-05-21 12:47:58,778] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-21 12:47:58,778] INFO App info kafka.server for 4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 12:47:58,779] INFO [BrokerServer id=4] shut down completed (kafka.server.BrokerServer)
[2025-05-21 12:47:58,779] INFO [BrokerServer id=4] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
[2025-05-21 12:47:58,780] INFO App info kafka.server for 4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 14:13:10,607] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-21 14:13:10,885] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-21 14:13:10,911] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-21 14:13:10,915] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:17,216] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-21 14:13:17,416] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-21 14:13:17,451] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:18,106] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:18,115] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:18,147] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-21 14:13:18,168] INFO [BrokerServer id=4] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-21 14:13:18,212] INFO [SharedServer id=4] Starting SharedServer (kafka.server.SharedServer)
[2025-05-21 14:13:18,230] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:18,625] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
[2025-05-21 14:13:18,641] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:18,643] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:18,659] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000008082.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:18,660] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000012472.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:18,663] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 4ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:19,645] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 12472 with 0 producer ids in 47 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:19,717] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 12472 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:19,721] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 12472 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:19,722] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=12472, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000012472.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:19,733] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 11ms for snapshot load and 0ms for segment recovery from offset 12472 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:19,789] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-21 14:13:19,816] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 14:13:19,818] INFO [RaftManager id=4] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 14:13:20,143] INFO [RaftManager id=4] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 14:13:20,405] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=17, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from null (org.apache.kafka.raft.QuorumState)
[2025-05-21 14:13:20,424] INFO [kafka-4-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 14:13:20,438] INFO [kafka-4-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 14:13:20,481] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:20,517] INFO [BrokerServer id=4] Starting broker (kafka.server.BrokerServer)
[2025-05-21 14:13:20,551] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:20,578] INFO [RaftManager id=4] Registered the listener org.apache.kafka.image.loader.MetadataLoader@410904613 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 14:13:20,587] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,588] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:20,598] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,605] INFO [broker-4-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:13:20,606] INFO [broker-4-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:13:20,620] INFO [broker-4-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:13:20,630] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:13:20,647] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,681] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,683] INFO [BrokerServer id=4] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-21 14:13:20,695] INFO [BrokerServer id=4] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-21 14:13:20,698] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:20,691] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,704] WARN [RaftManager id=4] Connection to node 1 (kafka-controller-1/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,713] INFO [broker-4-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:20,721] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:20,726] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 14:13:20,801] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:20,910] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,021] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,032] INFO [RaftManager id=4] Completed transition to Unattached(epoch=18, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=17, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-21 14:13:21,052] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:21,060] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:21,145] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:21,153] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,160] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:21,231] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-21 14:13:21,272] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,277] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-21 14:13:21,305] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-21 14:13:21,342] INFO [SocketServer listenerType=BROKER, nodeId=4] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-21 14:13:21,348] INFO [RaftManager id=4] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:21,349] WARN [RaftManager id=4] Connection to node 2 (kafka-controller-2/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:21,364] INFO [broker-4-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:21,376] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,386] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:21,416] INFO [ExpirationReaper-4-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:21,427] INFO [ExpirationReaper-4-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:21,428] INFO [ExpirationReaper-4-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:21,429] INFO [ExpirationReaper-4-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:21,430] INFO [RaftManager id=4] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=18, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=18, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-21 14:13:21,445] INFO [ExpirationReaper-4-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:21,468] INFO [broker-4-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:21,482] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,488] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:21,502] INFO [ExpirationReaper-4-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:21,530] INFO [ExpirationReaper-4-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:21,585] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,688] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,779] INFO [broker-4-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:21,780] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:21,786] INFO [BrokerLifecycleManager id=4] Incarnation J-HmjwbUSNyuNJCywfNwMw of broker 4 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:13:21,792] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,808] INFO [ExpirationReaper-4-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:21,898] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,967] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,970] INFO [BrokerServer id=4] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-21 14:13:21,978] INFO [BrokerServer id=4] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-21 14:13:21,982] INFO [BrokerServer id=4] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-21 14:13:22,071] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,175] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,277] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,380] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,487] INFO [MetadataLoader id=4] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,528] INFO [RaftManager id=4] High watermark set to Optional[LogOffsetMetadata(offset=12477, metadata=Optional.empty)] for the first time for epoch 18 (org.apache.kafka.raft.FollowerState)
[2025-05-21 14:13:22,537] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 12477 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,952] INFO [MetadataLoader id=4] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 12477 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,985] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 12476 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,997] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing MetadataVersionPublisher(id=4) with a snapshot at offset 12476 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:23,013] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 12476 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:23,030] INFO [BrokerMetadataPublisher id=4] Publishing initial metadata at offset OffsetAndEpoch(offset=12476, epoch=18) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-21 14:13:23,033] INFO [NodeToControllerChannelManager id=4 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:23,086] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:23,087] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:23,130] INFO [broker-4-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:23,202] INFO Skipping recovery of 71 logs from /tmp/kafka-logs since clean shutdown file was found (kafka.log.LogManager)
[2025-05-21 14:13:23,311] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-13/00000000000000053728.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:23,326] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Loading producer state till offset 255889 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,328] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255889 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,330] INFO [ProducerStateManager partition=financial_transactions-13] Loading producer state from snapshot file 'SnapshotFile(offset=255889, file=/tmp/kafka-logs/financial_transactions-13/00000000000000255889.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:23,345] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Producer state recovery took 15ms for snapshot load and 0ms for segment recovery from offset 255889 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,374] INFO [BrokerLifecycleManager id=4] Successfully registered broker 4 with broker epoch 12478 (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:13:23,416] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-13, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255889) with 1 segments, local-log-start-offset 0 and log-end-offset 255889 in 142ms (1/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:23,514] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-18/00000000000000053605.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:23,521] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Loading producer state till offset 255590 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,535] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255590 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,544] INFO [ProducerStateManager partition=financial_transactions-18] Loading producer state from snapshot file 'SnapshotFile(offset=255590, file=/tmp/kafka-logs/financial_transactions-18/00000000000000255590.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:23,566] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Producer state recovery took 22ms for snapshot load and 0ms for segment recovery from offset 255590 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,597] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-18, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255590) with 1 segments, local-log-start-offset 0 and log-end-offset 255590 in 152ms (2/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:23,662] INFO Deleted producer state snapshot /tmp/kafka-logs/__consumer_offsets-29/00000000000000000004.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:23,671] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 6 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,672] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 6 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,673] INFO [ProducerStateManager partition=__consumer_offsets-29] Loading producer state from snapshot file 'SnapshotFile(offset=6, file=/tmp/kafka-logs/__consumer_offsets-29/00000000000000000006.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:23,677] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Producer state recovery took 4ms for snapshot load and 0ms for segment recovery from offset 6 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,729] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-29, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=29, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=6) with 1 segments, local-log-start-offset 0 and log-end-offset 6 in 120ms (3/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:23,750] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-12/00000000000000053441.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:23,772] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Loading producer state till offset 255604 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,779] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255604 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,780] INFO [ProducerStateManager partition=financial_transactions-12] Loading producer state from snapshot file 'SnapshotFile(offset=255604, file=/tmp/kafka-logs/financial_transactions-12/00000000000000255604.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:23,784] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Producer state recovery took 4ms for snapshot load and 0ms for segment recovery from offset 255604 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,831] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-12, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255604) with 1 segments, local-log-start-offset 0 and log-end-offset 255604 in 95ms (4/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:23,859] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,081] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-38, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=38, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 228ms (5/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,129] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-16/00000000000000053208.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:24,170] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Loading producer state till offset 255425 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,172] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255425 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,176] INFO [ProducerStateManager partition=financial_transactions-16] Loading producer state from snapshot file 'SnapshotFile(offset=255425, file=/tmp/kafka-logs/financial_transactions-16/00000000000000255425.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:24,187] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Producer state recovery took 11ms for snapshot load and 0ms for segment recovery from offset 255425 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,205] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-16, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255425) with 1 segments, local-log-start-offset 0 and log-end-offset 255425 in 122ms (6/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,261] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,278] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-27, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=27, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 72ms (7/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,356] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,362] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-39, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=39, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 69ms (8/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,420] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,426] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-42, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=42, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 48ms (9/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,441] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,453] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-9, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (10/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,493] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,507] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-22, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=22, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 47ms (11/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,531] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,534] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-26, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=26, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 25ms (12/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,544] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,546] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-40, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=40, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (13/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,571] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,580] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-47, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=47, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 16ms (14/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,608] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,613] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-15, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 26ms (15/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,620] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,622] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-10, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (16/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,628] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-11/00000000000000053486.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:24,630] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Loading producer state till offset 256046 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,633] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 256046 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,635] INFO [ProducerStateManager partition=financial_transactions-11] Loading producer state from snapshot file 'SnapshotFile(offset=256046, file=/tmp/kafka-logs/financial_transactions-11/00000000000000256046.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:24,649] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Producer state recovery took 14ms for snapshot load and 0ms for segment recovery from offset 256046 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,654] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-11, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=256046) with 1 segments, local-log-start-offset 0 and log-end-offset 256046 in 31ms (17/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,686] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,708] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-2, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 51ms (18/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,716] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,732] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-18, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 20ms (19/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,762] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,774] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-46, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=46, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 26ms (20/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,825] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-1/00000000000000053591.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:24,829] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Loading producer state till offset 255384 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,830] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255384 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,830] INFO [ProducerStateManager partition=financial_transactions-1] Loading producer state from snapshot file 'SnapshotFile(offset=255384, file=/tmp/kafka-logs/financial_transactions-1/00000000000000255384.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:24,833] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 0ms for segment recovery from offset 255384 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,839] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-1, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255384) with 1 segments, local-log-start-offset 0 and log-end-offset 255384 in 51ms (21/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,843] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,854] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-12, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (22/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,860] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,863] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-23, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=23, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (23/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,877] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,879] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-14, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (24/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,884] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,885] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-5, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (25/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,927] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,935] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-41, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=41, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 42ms (26/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,984] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-17/00000000000000053580.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:24,989] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Loading producer state till offset 255866 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,993] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255866 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,995] INFO [ProducerStateManager partition=financial_transactions-17] Loading producer state from snapshot file 'SnapshotFile(offset=255866, file=/tmp/kafka-logs/financial_transactions-17/00000000000000255866.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:24,998] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 0ms for segment recovery from offset 255866 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,004] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-17, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255866) with 1 segments, local-log-start-offset 0 and log-end-offset 255866 in 66ms (27/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,038] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,047] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-1, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 30ms (28/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,058] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,062] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-19, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (29/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,084] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,106] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-3, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 33ms (30/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,149] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,156] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-16, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 49ms (31/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,167] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,170] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-25, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=25, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (32/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,196] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-15/00000000000000053502.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,199] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Loading producer state till offset 256237 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,199] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 256237 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,202] INFO [ProducerStateManager partition=financial_transactions-15] Loading producer state from snapshot file 'SnapshotFile(offset=256237, file=/tmp/kafka-logs/financial_transactions-15/00000000000000256237.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,204] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 256237 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,207] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-15, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=256237) with 1 segments, local-log-start-offset 0 and log-end-offset 256237 in 29ms (33/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,213] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-3/00000000000000053622.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,214] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Loading producer state till offset 255715 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,214] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255715 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,215] INFO [ProducerStateManager partition=financial_transactions-3] Loading producer state from snapshot file 'SnapshotFile(offset=255715, file=/tmp/kafka-logs/financial_transactions-3/00000000000000255715.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,216] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 255715 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,218] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-3, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255715) with 1 segments, local-log-start-offset 0 and log-end-offset 255715 in 9ms (34/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,248] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-10/00000000000000053252.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,249] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Loading producer state till offset 256290 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,249] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 256290 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,250] INFO [ProducerStateManager partition=financial_transactions-10] Loading producer state from snapshot file 'SnapshotFile(offset=256290, file=/tmp/kafka-logs/financial_transactions-10/00000000000000256290.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,251] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 256290 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,254] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-10, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=256290) with 1 segments, local-log-start-offset 0 and log-end-offset 256290 in 29ms (35/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,272] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-19/00000000000000053309.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,272] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Loading producer state till offset 255167 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,273] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255167 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,273] INFO [ProducerStateManager partition=financial_transactions-19] Loading producer state from snapshot file 'SnapshotFile(offset=255167, file=/tmp/kafka-logs/financial_transactions-19/00000000000000255167.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,275] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 255167 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,291] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-19, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255167) with 1 segments, local-log-start-offset 0 and log-end-offset 255167 in 34ms (36/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,334] INFO Deleted producer state snapshot /tmp/kafka-logs/_schemas-0/00000000000000000004.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,338] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 6 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,340] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 6 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,341] INFO [ProducerStateManager partition=_schemas-0] Loading producer state from snapshot file 'SnapshotFile(offset=6, file=/tmp/kafka-logs/_schemas-0/00000000000000000006.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,350] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Producer state recovery took 8ms for snapshot load and 0ms for segment recovery from offset 6 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,362] INFO Completed load of Log(dir=/tmp/kafka-logs/_schemas-0, topicId=RrE8eovWRKu4kLR3MRJ0fA, topic=_schemas, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=6) with 1 segments, local-log-start-offset 0 and log-end-offset 6 in 70ms (37/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,379] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,381] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-28, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=28, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (38/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,388] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,398] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-13, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 17ms (39/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,402] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,404] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-24, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=24, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (40/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,424] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,427] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-32, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=32, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 21ms (41/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,457] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-14/00000000000000053468.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,458] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Loading producer state till offset 255812 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,459] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255812 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,460] INFO [ProducerStateManager partition=financial_transactions-14] Loading producer state from snapshot file 'SnapshotFile(offset=255812, file=/tmp/kafka-logs/financial_transactions-14/00000000000000255812.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,464] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Producer state recovery took 4ms for snapshot load and 0ms for segment recovery from offset 255812 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,466] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-14, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255812) with 1 segments, local-log-start-offset 0 and log-end-offset 255812 in 39ms (42/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,482] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-2/00000000000000053173.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,483] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Loading producer state till offset 255541 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,483] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255541 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,484] INFO [ProducerStateManager partition=financial_transactions-2] Loading producer state from snapshot file 'SnapshotFile(offset=255541, file=/tmp/kafka-logs/financial_transactions-2/00000000000000255541.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,487] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 0ms for segment recovery from offset 255541 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,489] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-2, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255541) with 1 segments, local-log-start-offset 0 and log-end-offset 255541 in 21ms (43/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,495] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,504] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-31, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=31, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (44/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,516] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,525] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-21, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=21, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (45/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,538] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-0/00000000000000053808.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,541] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 257213 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,542] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 257213 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,552] INFO [ProducerStateManager partition=financial_transactions-0] Loading producer state from snapshot file 'SnapshotFile(offset=257213, file=/tmp/kafka-logs/financial_transactions-0/00000000000000257213.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,555] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 0ms for segment recovery from offset 257213 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,569] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-0, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=257213) with 1 segments, local-log-start-offset 0 and log-end-offset 257213 in 42ms (46/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,576] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-9/00000000000000053169.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,579] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Loading producer state till offset 255457 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,587] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255457 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,588] INFO [ProducerStateManager partition=financial_transactions-9] Loading producer state from snapshot file 'SnapshotFile(offset=255457, file=/tmp/kafka-logs/financial_transactions-9/00000000000000255457.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,593] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Producer state recovery took 5ms for snapshot load and 0ms for segment recovery from offset 255457 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,602] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-9, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255457) with 1 segments, local-log-start-offset 0 and log-end-offset 255457 in 32ms (47/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,605] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,609] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-6, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (48/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,629] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,650] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-11, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 41ms (49/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,656] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-5/00000000000000053395.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,667] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Loading producer state till offset 255005 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,667] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255005 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,668] INFO [ProducerStateManager partition=financial_transactions-5] Loading producer state from snapshot file 'SnapshotFile(offset=255005, file=/tmp/kafka-logs/financial_transactions-5/00000000000000255005.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,669] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 255005 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,671] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-5, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255005) with 1 segments, local-log-start-offset 0 and log-end-offset 255005 in 20ms (50/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,689] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,691] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-30, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=30, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 19ms (51/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,708] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-6/00000000000000053206.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,711] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Loading producer state till offset 255175 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,712] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255175 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,713] INFO [ProducerStateManager partition=financial_transactions-6] Loading producer state from snapshot file 'SnapshotFile(offset=255175, file=/tmp/kafka-logs/financial_transactions-6/00000000000000255175.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,718] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Producer state recovery took 5ms for snapshot load and 0ms for segment recovery from offset 255175 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,721] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-6, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255175) with 1 segments, local-log-start-offset 0 and log-end-offset 255175 in 28ms (52/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,725] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,732] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-43, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=43, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (53/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,736] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,745] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-7, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (54/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,754] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,764] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-33, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=33, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (55/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,771] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,773] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-36, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=36, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (56/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,788] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,791] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-45, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=45, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 17ms (57/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,802] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,807] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-0, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (58/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,814] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,818] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-20, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=20, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (59/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,833] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,835] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-4, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (60/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,838] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,840] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-8, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (61/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,843] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,847] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-49, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=49, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (62/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,852] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,856] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-34, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=34, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (63/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,871] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,875] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-48, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=48, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (64/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,888] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-4/00000000000000053258.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,888] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Loading producer state till offset 255335 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,889] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255335 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,889] INFO [ProducerStateManager partition=financial_transactions-4] Loading producer state from snapshot file 'SnapshotFile(offset=255335, file=/tmp/kafka-logs/financial_transactions-4/00000000000000255335.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,890] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 255335 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,891] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-4, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255335) with 1 segments, local-log-start-offset 0 and log-end-offset 255335 in 16ms (65/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,895] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,898] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-44, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=44, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (66/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,913] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,923] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-37, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=37, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 20ms (67/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,945] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-7/00000000000000053438.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,945] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Loading producer state till offset 255844 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,946] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255844 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,946] INFO [ProducerStateManager partition=financial_transactions-7] Loading producer state from snapshot file 'SnapshotFile(offset=255844, file=/tmp/kafka-logs/financial_transactions-7/00000000000000255844.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,947] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 255844 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,949] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-7, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255844) with 1 segments, local-log-start-offset 0 and log-end-offset 255844 in 22ms (68/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,953] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,955] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-35, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=35, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (69/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,957] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,959] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-17, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (70/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,967] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-8/00000000000000053130.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,968] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Loading producer state till offset 255535 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,969] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255535 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,970] INFO [ProducerStateManager partition=financial_transactions-8] Loading producer state from snapshot file 'SnapshotFile(offset=255535, file=/tmp/kafka-logs/financial_transactions-8/00000000000000255535.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,973] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 0ms for segment recovery from offset 255535 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,975] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-8, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255535) with 1 segments, local-log-start-offset 0 and log-end-offset 255535 in 16ms (71/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,986] INFO Loaded 71 logs in 2888ms (kafka.log.LogManager)
[2025-05-21 14:13:25,989] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-21 14:13:25,991] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-21 14:13:26,009] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-21 14:13:26,174] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 14:13:26,181] INFO [GroupCoordinator 4]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,185] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 14:13:26,193] INFO [GroupCoordinator 4]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,194] INFO [TransactionCoordinator id=4] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 14:13:26,182] INFO [AddPartitionsToTxnSenderThread-4]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 14:13:26,239] INFO [TxnMarkerSenderThread-4]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 14:13:26,240] INFO [TransactionCoordinator id=4] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 14:13:26,259] INFO [Broker id=4] Transitioning 71 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:26,268] INFO [Broker id=4] Creating new partition financial_transactions-13 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,283] INFO [Partition financial_transactions-13 broker=4] Log loaded for partition financial_transactions-13 with initial high watermark 255889 (kafka.cluster.Partition)
[2025-05-21 14:13:26,287] INFO [Broker id=4] Follower financial_transactions-13 starts at leader epoch 7 from offset 255889 with partition epoch 10 and high watermark 255889. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:13:26,288] INFO [Broker id=4] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,289] INFO [Partition __consumer_offsets-13 broker=4] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,297] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,298] INFO [Broker id=4] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,309] INFO [Partition __consumer_offsets-46 broker=4] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,315] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,318] INFO [Broker id=4] Creating new partition financial_transactions-17 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,325] INFO [Partition financial_transactions-17 broker=4] Log loaded for partition financial_transactions-17 with initial high watermark 255866 (kafka.cluster.Partition)
[2025-05-21 14:13:26,325] INFO [Broker id=4] Follower financial_transactions-17 starts at leader epoch 4 from offset 255866 with partition epoch 9 and high watermark 255866. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 14:13:26,326] INFO [Broker id=4] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,330] INFO [Partition __consumer_offsets-9 broker=4] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,336] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,338] INFO [Broker id=4] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,346] INFO [Partition __consumer_offsets-42 broker=4] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,349] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,350] INFO [Broker id=4] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,351] INFO [Partition __consumer_offsets-21 broker=4] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,352] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,352] INFO [Broker id=4] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,353] INFO [Partition __consumer_offsets-17 broker=4] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,355] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,357] INFO [Broker id=4] Creating new partition financial_transactions-0 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,360] INFO [Partition financial_transactions-0 broker=4] Log loaded for partition financial_transactions-0 with initial high watermark 257197 (kafka.cluster.Partition)
[2025-05-21 14:13:26,361] INFO [Broker id=4] Follower financial_transactions-0 starts at leader epoch 5 from offset 257213 with partition epoch 9 and high watermark 257197. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,362] INFO [Broker id=4] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,367] INFO [Partition __consumer_offsets-30 broker=4] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,367] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,369] INFO [Broker id=4] Creating new partition financial_transactions-4 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,375] INFO [Partition financial_transactions-4 broker=4] Log loaded for partition financial_transactions-4 with initial high watermark 255335 (kafka.cluster.Partition)
[2025-05-21 14:13:26,381] INFO [Broker id=4] Follower financial_transactions-4 starts at leader epoch 5 from offset 255335 with partition epoch 10 and high watermark 255335. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,384] INFO [Broker id=4] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,385] INFO [Partition __consumer_offsets-26 broker=4] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,386] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,386] INFO [Broker id=4] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,391] INFO [Partition __consumer_offsets-5 broker=4] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,391] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,392] INFO [Broker id=4] Creating new partition financial_transactions-8 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,396] INFO [Partition financial_transactions-8 broker=4] Log loaded for partition financial_transactions-8 with initial high watermark 255525 (kafka.cluster.Partition)
[2025-05-21 14:13:26,397] INFO [Broker id=4] Follower financial_transactions-8 starts at leader epoch 7 from offset 255535 with partition epoch 10 and high watermark 255525. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:13:26,406] INFO [Broker id=4] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,414] INFO [Partition __consumer_offsets-38 broker=4] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,414] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,415] INFO [Broker id=4] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,416] INFO [Partition __consumer_offsets-1 broker=4] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,420] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,420] INFO [Broker id=4] Creating new partition financial_transactions-12 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,422] INFO [Partition financial_transactions-12 broker=4] Log loaded for partition financial_transactions-12 with initial high watermark 255590 (kafka.cluster.Partition)
[2025-05-21 14:13:26,422] INFO [Broker id=4] Follower financial_transactions-12 starts at leader epoch 5 from offset 255604 with partition epoch 9 and high watermark 255590. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,423] INFO [Broker id=4] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,426] INFO [Partition __consumer_offsets-34 broker=4] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,428] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,431] INFO [Broker id=4] Creating new partition financial_transactions-14 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,434] INFO [Partition financial_transactions-14 broker=4] Log loaded for partition financial_transactions-14 with initial high watermark 255812 (kafka.cluster.Partition)
[2025-05-21 14:13:26,437] INFO [Broker id=4] Follower financial_transactions-14 starts at leader epoch 5 from offset 255812 with partition epoch 10 and high watermark 255812. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,453] INFO [Broker id=4] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,454] INFO [Partition __consumer_offsets-16 broker=4] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,456] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,457] INFO [Broker id=4] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-21 14:13:26,458] INFO [Partition _schemas-0 broker=4] Log loaded for partition _schemas-0 with initial high watermark 6 (kafka.cluster.Partition)
[2025-05-21 14:13:26,458] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 15 from offset 6 with partition epoch 25 and high watermark 6. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,459] INFO [Broker id=4] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,460] INFO [Partition __consumer_offsets-45 broker=4] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,461] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,462] INFO [Broker id=4] Creating new partition financial_transactions-18 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,464] INFO [Partition financial_transactions-18 broker=4] Log loaded for partition financial_transactions-18 with initial high watermark 255590 (kafka.cluster.Partition)
[2025-05-21 14:13:26,464] INFO [Broker id=4] Follower financial_transactions-18 starts at leader epoch 4 from offset 255590 with partition epoch 9 and high watermark 255590. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 14:13:26,465] INFO [Broker id=4] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,471] INFO [Partition __consumer_offsets-12 broker=4] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,471] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,476] INFO [Broker id=4] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,477] INFO [Partition __consumer_offsets-41 broker=4] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,477] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,478] INFO [Broker id=4] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,480] INFO [Partition __consumer_offsets-24 broker=4] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,481] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,481] INFO [Broker id=4] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,486] INFO [Partition __consumer_offsets-20 broker=4] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,488] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,490] INFO [Broker id=4] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,492] INFO [Partition __consumer_offsets-49 broker=4] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,493] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,494] INFO [Broker id=4] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,498] INFO [Partition __consumer_offsets-0 broker=4] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,500] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,503] INFO [Broker id=4] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,511] INFO [Partition __consumer_offsets-29 broker=4] Log loaded for partition __consumer_offsets-29 with initial high watermark 6 (kafka.cluster.Partition)
[2025-05-21 14:13:26,515] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 15 from offset 6 with partition epoch 25 and high watermark 6. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,518] INFO [Broker id=4] Creating new partition financial_transactions-1 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,522] INFO [Partition financial_transactions-1 broker=4] Log loaded for partition financial_transactions-1 with initial high watermark 255371 (kafka.cluster.Partition)
[2025-05-21 14:13:26,524] INFO [Broker id=4] Follower financial_transactions-1 starts at leader epoch 7 from offset 255384 with partition epoch 10 and high watermark 255371. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:13:26,525] INFO [Broker id=4] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,527] INFO [Partition __consumer_offsets-25 broker=4] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,528] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,529] INFO [Broker id=4] Creating new partition financial_transactions-5 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,534] INFO [Partition financial_transactions-5 broker=4] Log loaded for partition financial_transactions-5 with initial high watermark 254990 (kafka.cluster.Partition)
[2025-05-21 14:13:26,541] INFO [Broker id=4] Follower financial_transactions-5 starts at leader epoch 6 from offset 255005 with partition epoch 10 and high watermark 254990. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:26,542] INFO [Broker id=4] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,543] INFO [Partition __consumer_offsets-8 broker=4] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,547] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,550] INFO [Broker id=4] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,551] INFO [Partition __consumer_offsets-37 broker=4] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,553] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,556] INFO [Broker id=4] Creating new partition financial_transactions-9 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,621] INFO [Partition financial_transactions-9 broker=4] Log loaded for partition financial_transactions-9 with initial high watermark 255435 (kafka.cluster.Partition)
[2025-05-21 14:13:26,635] INFO [Broker id=4] Follower financial_transactions-9 starts at leader epoch 6 from offset 255457 with partition epoch 10 and high watermark 255435. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:26,638] INFO [Broker id=4] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,640] INFO [Partition __consumer_offsets-4 broker=4] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,641] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,641] INFO [Broker id=4] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,644] INFO [Partition __consumer_offsets-33 broker=4] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,645] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,648] INFO [Broker id=4] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,649] INFO [Partition __consumer_offsets-15 broker=4] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,655] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,657] INFO [Broker id=4] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,661] INFO [Partition __consumer_offsets-48 broker=4] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,672] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,672] INFO [Broker id=4] Creating new partition financial_transactions-15 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,686] INFO [Partition financial_transactions-15 broker=4] Log loaded for partition financial_transactions-15 with initial high watermark 256237 (kafka.cluster.Partition)
[2025-05-21 14:13:26,689] INFO [Broker id=4] Follower financial_transactions-15 starts at leader epoch 5 from offset 256237 with partition epoch 10 and high watermark 256237. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,690] INFO [Broker id=4] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,736] INFO [Partition __consumer_offsets-11 broker=4] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,737] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,740] INFO [Broker id=4] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,753] INFO [Partition __consumer_offsets-44 broker=4] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,760] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,765] INFO [Broker id=4] Creating new partition financial_transactions-19 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,770] INFO [Partition financial_transactions-19 broker=4] Log loaded for partition financial_transactions-19 with initial high watermark 255167 (kafka.cluster.Partition)
[2025-05-21 14:13:26,771] INFO [Broker id=4] Follower financial_transactions-19 starts at leader epoch 5 from offset 255167 with partition epoch 10 and high watermark 255167. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,772] INFO [Broker id=4] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,777] INFO [Partition __consumer_offsets-23 broker=4] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,778] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,779] INFO [Broker id=4] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,794] INFO [Partition __consumer_offsets-19 broker=4] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,795] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,795] INFO [Broker id=4] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,806] INFO [Partition __consumer_offsets-32 broker=4] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,806] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,808] INFO [Broker id=4] Creating new partition financial_transactions-2 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,815] INFO [Partition financial_transactions-2 broker=4] Log loaded for partition financial_transactions-2 with initial high watermark 255541 (kafka.cluster.Partition)
[2025-05-21 14:13:26,819] INFO [Broker id=4] Follower financial_transactions-2 starts at leader epoch 5 from offset 255541 with partition epoch 10 and high watermark 255541. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,820] INFO [Broker id=4] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,822] INFO [Partition __consumer_offsets-28 broker=4] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,824] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,831] INFO [Broker id=4] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,847] INFO [Partition __consumer_offsets-7 broker=4] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,864] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,864] INFO [Broker id=4] Creating new partition financial_transactions-6 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,867] INFO [Partition financial_transactions-6 broker=4] Log loaded for partition financial_transactions-6 with initial high watermark 255175 (kafka.cluster.Partition)
[2025-05-21 14:13:26,868] INFO [Broker id=4] Follower financial_transactions-6 starts at leader epoch 5 from offset 255175 with partition epoch 10 and high watermark 255175. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,868] INFO [Broker id=4] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,869] INFO [Partition __consumer_offsets-40 broker=4] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,869] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,870] INFO [Broker id=4] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,871] INFO [Partition __consumer_offsets-3 broker=4] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,871] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,872] INFO [Broker id=4] Creating new partition financial_transactions-10 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,875] INFO [Partition financial_transactions-10 broker=4] Log loaded for partition financial_transactions-10 with initial high watermark 256290 (kafka.cluster.Partition)
[2025-05-21 14:13:26,881] INFO [Broker id=4] Follower financial_transactions-10 starts at leader epoch 4 from offset 256290 with partition epoch 9 and high watermark 256290. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 14:13:26,883] INFO [Broker id=4] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,886] INFO [Partition __consumer_offsets-36 broker=4] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,887] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,889] INFO [Broker id=4] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,891] INFO [Partition __consumer_offsets-47 broker=4] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,892] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,893] INFO [Broker id=4] Creating new partition financial_transactions-16 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,897] INFO [Partition financial_transactions-16 broker=4] Log loaded for partition financial_transactions-16 with initial high watermark 255412 (kafka.cluster.Partition)
[2025-05-21 14:13:26,900] INFO [Broker id=4] Follower financial_transactions-16 starts at leader epoch 6 from offset 255425 with partition epoch 10 and high watermark 255412. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:26,900] INFO [Broker id=4] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,901] INFO [Partition __consumer_offsets-14 broker=4] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,901] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,902] INFO [Broker id=4] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:27,000] INFO [Partition __consumer_offsets-43 broker=4] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:27,000] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:27,003] INFO [Broker id=4] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:27,009] INFO [Partition __consumer_offsets-10 broker=4] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:27,015] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:27,019] INFO [Broker id=4] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:27,020] INFO [Partition __consumer_offsets-22 broker=4] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:27,024] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:27,024] INFO [Broker id=4] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:27,025] INFO [Partition __consumer_offsets-18 broker=4] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:27,026] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,026] INFO [Broker id=4] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:27,028] INFO [Partition __consumer_offsets-31 broker=4] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:27,028] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,029] INFO [Broker id=4] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:27,030] INFO [Partition __consumer_offsets-27 broker=4] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:27,033] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:27,035] INFO [Broker id=4] Creating new partition financial_transactions-3 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:27,042] INFO [Partition financial_transactions-3 broker=4] Log loaded for partition financial_transactions-3 with initial high watermark 255715 (kafka.cluster.Partition)
[2025-05-21 14:13:27,042] INFO [Broker id=4] Follower financial_transactions-3 starts at leader epoch 4 from offset 255715 with partition epoch 9 and high watermark 255715. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 14:13:27,043] INFO [Broker id=4] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:27,044] INFO [Partition __consumer_offsets-39 broker=4] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:27,045] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,045] INFO [Broker id=4] Creating new partition financial_transactions-7 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:27,056] INFO [Partition financial_transactions-7 broker=4] Log loaded for partition financial_transactions-7 with initial high watermark 255844 (kafka.cluster.Partition)
[2025-05-21 14:13:27,064] INFO [Broker id=4] Follower financial_transactions-7 starts at leader epoch 5 from offset 255844 with partition epoch 9 and high watermark 255844. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:27,069] INFO [Broker id=4] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:27,073] INFO [Partition __consumer_offsets-6 broker=4] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:27,074] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:27,075] INFO [Broker id=4] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:27,077] INFO [Partition __consumer_offsets-35 broker=4] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:27,077] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,078] INFO [Broker id=4] Creating new partition financial_transactions-11 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:27,079] INFO [Partition financial_transactions-11 broker=4] Log loaded for partition financial_transactions-11 with initial high watermark 256033 (kafka.cluster.Partition)
[2025-05-21 14:13:27,079] INFO [Broker id=4] Follower financial_transactions-11 starts at leader epoch 5 from offset 256046 with partition epoch 10 and high watermark 256033. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:27,079] INFO [Broker id=4] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:27,080] INFO [Partition __consumer_offsets-2 broker=4] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:27,081] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,096] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-6, __consumer_offsets-35, financial_transactions-11, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:13:27,106] INFO [Broker id=4] Stopped fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-21 14:13:27,134] INFO [Broker id=4] Started fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-21 14:13:27,154] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,173] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,185] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,187] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,205] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,210] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,213] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,214] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,215] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,216] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,217] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,224] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,225] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,227] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,228] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,210] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,231] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,230] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,233] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,234] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,234] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,235] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,235] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,236] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,236] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,236] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,236] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,237] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,237] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,237] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,237] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,238] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,238] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,238] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,238] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,234] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,240] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,240] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,241] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,242] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,242] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,243] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,243] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,244] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,244] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,244] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,245] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,245] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,238] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,246] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,246] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,246] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,247] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,247] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,251] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,252] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,252] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,253] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,253] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,254] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,254] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,256] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,258] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,261] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,262] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,246] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,264] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,264] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,264] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,266] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,267] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,268] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,271] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,263] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,276] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,277] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,279] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,279] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,277] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,282] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,282] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,283] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,283] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,282] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,284] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,284] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,285] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,284] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,294] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,296] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,296] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,297] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,297] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,297] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,298] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,301] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,301] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,302] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,302] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,302] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,303] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,304] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,304] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,304] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,304] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,308] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,311] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,314] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,315] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,315] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,319] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,311] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,319] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,325] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,327] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,329] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,335] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,336] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,336] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,337] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,337] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,338] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,338] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,338] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,339] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,339] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,339] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,340] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,340] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,340] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,340] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,338] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,341] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,341] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,341] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,343] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,341] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,348] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,348] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,349] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,349] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,349] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,349] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,350] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:27,350] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,348] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,350] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,350] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,351] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,351] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:27,355] INFO [DynamicConfigPublisher broker id=4] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-21 14:13:27,372] INFO [DynamicConfigPublisher broker id=4] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-21 14:13:27,387] INFO [MetadataLoader id=4] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=4) with a snapshot at offset 12476 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:27,401] INFO [BrokerLifecycleManager id=4] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:13:27,403] INFO [BrokerServer id=4] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-21 14:13:27,404] INFO [BrokerServer id=4] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-21 14:13:27,404] INFO [BrokerServer id=4] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-21 14:13:27,406] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-21 14:13:27,408] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:27,425] INFO [BrokerServer id=4] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-21 14:13:27,434] INFO [BrokerLifecycleManager id=4] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:13:27,488] INFO [Broker id=4] Transitioning 71 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:27,491] INFO [Broker id=4] Follower financial_transactions-13 starts at leader epoch 8 from offset 255889 with partition epoch 11 and high watermark 255889. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:13:27,492] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 15 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,493] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 15 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,493] INFO [Broker id=4] Follower financial_transactions-17 starts at leader epoch 5 from offset 255866 with partition epoch 10 and high watermark 255866. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:27,493] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 11 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:13:27,494] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 11 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:13:27,494] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,495] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,495] INFO [Broker id=4] Follower financial_transactions-0 starts at leader epoch 6 from offset 257213 with partition epoch 10 and high watermark 257197. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,495] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 15 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,496] INFO [Broker id=4] Follower financial_transactions-4 starts at leader epoch 6 from offset 255335 with partition epoch 11 and high watermark 255335. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,504] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 16 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,509] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 16 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,510] INFO [Broker id=4] Follower financial_transactions-8 starts at leader epoch 8 from offset 255535 with partition epoch 11 and high watermark 255525. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:13:27,527] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 11 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:13:27,528] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 13 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:13:27,528] INFO [Broker id=4] Follower financial_transactions-12 starts at leader epoch 6 from offset 255604 with partition epoch 10 and high watermark 255590. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,529] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 15 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,530] INFO [Broker id=4] Follower financial_transactions-14 starts at leader epoch 6 from offset 255812 with partition epoch 11 and high watermark 255812. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,530] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 16 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,530] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 16 from offset 6 with partition epoch 26 and high watermark 6. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,531] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,531] INFO [Broker id=4] Follower financial_transactions-18 starts at leader epoch 5 from offset 255590 with partition epoch 10 and high watermark 255590. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:27,532] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,532] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 13 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:13:27,532] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,533] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 13 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:13:27,534] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 16 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,534] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,535] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 16 from offset 6 with partition epoch 26 and high watermark 6. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,535] INFO [Broker id=4] Follower financial_transactions-1 starts at leader epoch 8 from offset 255384 with partition epoch 11 and high watermark 255371. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:13:27,536] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 13 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:13:27,536] INFO [Broker id=4] Follower financial_transactions-5 starts at leader epoch 7 from offset 255005 with partition epoch 11 and high watermark 254990. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:13:27,522] INFO [BrokerLifecycleManager id=4] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:13:27,544] INFO [BrokerServer id=4] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-21 14:13:27,559] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-21 14:13:27,541] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 11 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:13:27,572] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 15 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,573] INFO [Broker id=4] Follower financial_transactions-9 starts at leader epoch 7 from offset 255457 with partition epoch 11 and high watermark 255435. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:13:27,571] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-21 14:13:27,576] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 13 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:13:27,591] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,590] INFO [SocketServer listenerType=BROKER, nodeId=4] Enabling request processing. (kafka.network.SocketServer)
[2025-05-21 14:13:27,628] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-21 14:13:27,610] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 13 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:13:27,639] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-21 14:13:27,645] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 13 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:13:27,646] INFO [BrokerServer id=4] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-21 14:13:27,655] INFO [BrokerServer id=4] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-21 14:13:27,660] INFO [BrokerServer id=4] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-21 14:13:27,663] INFO [BrokerServer id=4] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-21 14:13:27,653] INFO [Broker id=4] Follower financial_transactions-15 starts at leader epoch 6 from offset 256237 with partition epoch 11 and high watermark 256237. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,686] INFO [BrokerServer id=4] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-21 14:13:27,692] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 14:13:27,693] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 14:13:27,693] INFO Kafka startTimeMs: 1747836807692 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 14:13:27,697] INFO [KafkaRaftServer nodeId=4] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-21 14:13:27,701] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 15 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,706] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 15 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,712] INFO [Broker id=4] Follower financial_transactions-19 starts at leader epoch 6 from offset 255167 with partition epoch 11 and high watermark 255167. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,717] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 16 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,718] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,718] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,721] INFO [Broker id=4] Follower financial_transactions-2 starts at leader epoch 6 from offset 255541 with partition epoch 11 and high watermark 255541. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,722] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 13 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:13:27,732] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 15 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,739] INFO [Broker id=4] Follower financial_transactions-6 starts at leader epoch 6 from offset 255175 with partition epoch 11 and high watermark 255175. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,744] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,754] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,763] INFO [Broker id=4] Follower financial_transactions-10 starts at leader epoch 5 from offset 256290 with partition epoch 10 and high watermark 256290. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:27,769] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,789] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 11 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:13:27,795] INFO [Broker id=4] Follower financial_transactions-16 starts at leader epoch 7 from offset 255425 with partition epoch 11 and high watermark 255412. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:13:27,800] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 11 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:13:27,801] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,802] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,802] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 13 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:13:27,803] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 16 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,817] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 11 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:13:27,818] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,819] INFO [Broker id=4] Follower financial_transactions-3 starts at leader epoch 5 from offset 255715 with partition epoch 10 and high watermark 255715. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:27,819] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 16 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,820] INFO [Broker id=4] Follower financial_transactions-7 starts at leader epoch 6 from offset 255844 with partition epoch 10 and high watermark 255844. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,820] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,821] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 11 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:13:27,822] INFO [Broker id=4] Follower financial_transactions-11 starts at leader epoch 6 from offset 256046 with partition epoch 11 and high watermark 256033. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,823] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 16 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,826] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-6, __consumer_offsets-35, financial_transactions-11, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:13:27,827] INFO [Broker id=4] Stopped fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-21 14:13:27,907] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,914] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(financial_transactions-13 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,255889), __consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), financial_transactions-17 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,255866), __consumer_offsets-9 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,0), __consumer_offsets-42 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,0), __consumer_offsets-21 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-17 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), financial_transactions-0 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,257213), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), financial_transactions-4 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,255335), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,0), financial_transactions-8 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,255535), __consumer_offsets-38 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,0), __consumer_offsets-1 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), financial_transactions-12 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,255604), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), financial_transactions-14 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,255812), __consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,0), _schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,6), __consumer_offsets-45 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), financial_transactions-18 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,255590), __consumer_offsets-12 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-41 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), __consumer_offsets-24 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-20 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,0), __consumer_offsets-0 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,6), financial_transactions-1 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,255384), __consumer_offsets-25 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), financial_transactions-5 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,255005), __consumer_offsets-8 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), financial_transactions-9 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,255457), __consumer_offsets-4 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), __consumer_offsets-33 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-15 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), __consumer_offsets-48 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), financial_transactions-15 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,256237), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), financial_transactions-19 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,255167), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,0), __consumer_offsets-19 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-32 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), financial_transactions-2 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,255541), __consumer_offsets-28 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), financial_transactions-6 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,255175), __consumer_offsets-40 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-3 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), financial_transactions-10 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,256290), __consumer_offsets-36 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-47 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,0), financial_transactions-16 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,255425), __consumer_offsets-14 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,0), __consumer_offsets-43 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-10 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-22 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,0), __consumer_offsets-31 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,0), __consumer_offsets-27 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), financial_transactions-3 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,255715), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,0), financial_transactions-7 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,255844), __consumer_offsets-6 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-35 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,0), financial_transactions-11 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,256046), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:13:27,918] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,918] INFO [Broker id=4] Started fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-21 14:13:27,921] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,931] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,934] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,935] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,935] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,935] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,936] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,936] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,936] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,936] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,937] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,937] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,937] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,938] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,938] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,938] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,939] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,939] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,939] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,940] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,940] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,940] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,941] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,941] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,941] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,942] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,942] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,943] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,944] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,954] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,959] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,963] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,967] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,968] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,970] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,971] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,972] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,973] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,974] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,974] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,974] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,975] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,980] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,984] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,987] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,987] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,988] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,988] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,988] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,989] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,989] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,989] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,990] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,990] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,991] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,991] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,992] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,992] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,993] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,994] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,996] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,999] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,015] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,026] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,027] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,028] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,029] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,029] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,030] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,036] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,039] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,041] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,047] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,054] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,074] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,086] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,087] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,089] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,090] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,090] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,091] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,091] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,092] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,092] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,092] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,093] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,093] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,094] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,094] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,094] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,095] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,095] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,096] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,097] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,098] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,103] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,104] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,124] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,135] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,141] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,142] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,155] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,180] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,180] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,200] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,203] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,203] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,205] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,219] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,220] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,221] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,221] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,222] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,223] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,228] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,228] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,229] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,233] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,234] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,237] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,237] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,238] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,238] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,240] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,240] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,249] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,256] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,258] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,260] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,261] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,262] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,262] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,263] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,263] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,263] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,263] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,266] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,266] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,266] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,266] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,267] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,267] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,268] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,268] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,268] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,269] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,269] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,270] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,270] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,270] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,271] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,271] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,272] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,273] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,273] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,273] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,277] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,278] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,279] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,280] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,280] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,281] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,281] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,282] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,288] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,288] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,289] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,289] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,290] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,290] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,291] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,292] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,292] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,293] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,293] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,294] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,296] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,297] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,300] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,302] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,302] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,303] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,303] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,303] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,303] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,304] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,304] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,305] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,305] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,306] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,307] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,307] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,307] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,307] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,308] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,308] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,308] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,308] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,309] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,309] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,311] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,312] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,316] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,317] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,318] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,320] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,265] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,322] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,325] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,346] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,349] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,350] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,351] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,351] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,351] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,352] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,353] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,353] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,353] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,353] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,354] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,355] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,355] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,368] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,369] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,369] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,369] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,370] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,370] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,370] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,371] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,372] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,376] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,389] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,390] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,391] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,377] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:28,392] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,395] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,396] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 6], partitionEpoch=12, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 14:13:28,396] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,398] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,399] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,401] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,405] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition financial_transactions-0 with TruncationState(offset=257197, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=0, leaderEpoch=2, endOffset=257211) (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,428] INFO [UnifiedLog partition=financial_transactions-0, dir=/tmp/kafka-logs] Truncating to offset 257197 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,415] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,440] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,447] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,451] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,452] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,454] INFO [UnifiedLog partition=financial_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 257197 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,454] INFO [UnifiedLog partition=financial_transactions-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 257197 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,454] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-0/00000000000000257213.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:28,458] INFO [ProducerStateManager partition=financial_transactions-0] Wrote producer snapshot at offset 257197 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:28,460] INFO [UnifiedLog partition=financial_transactions-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 5ms for segment recovery from offset 257197 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,492] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition financial_transactions-1 with TruncationState(offset=255371, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=1, leaderEpoch=5, endOffset=255371) (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,494] INFO [UnifiedLog partition=financial_transactions-1, dir=/tmp/kafka-logs] Truncating to offset 255371 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,510] INFO [UnifiedLog partition=financial_transactions-1, dir=/tmp/kafka-logs] Loading producer state till offset 255371 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,511] INFO [UnifiedLog partition=financial_transactions-1, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255371 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,511] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-1/00000000000000255384.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:28,517] INFO [ProducerStateManager partition=financial_transactions-1] Wrote producer snapshot at offset 255371 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:28,517] INFO [UnifiedLog partition=financial_transactions-1, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 6ms for segment recovery from offset 255371 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,545] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition financial_transactions-5 with TruncationState(offset=254990, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=5, leaderEpoch=4, endOffset=254990) (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,548] INFO [UnifiedLog partition=financial_transactions-5, dir=/tmp/kafka-logs] Truncating to offset 254990 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,555] INFO [UnifiedLog partition=financial_transactions-5, dir=/tmp/kafka-logs] Loading producer state till offset 254990 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,556] INFO [UnifiedLog partition=financial_transactions-5, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 254990 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,557] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-5/00000000000000255005.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:28,569] INFO [ProducerStateManager partition=financial_transactions-5] Wrote producer snapshot at offset 254990 with 0 producer ids in 11 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:28,576] INFO [UnifiedLog partition=financial_transactions-5, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 18ms for segment recovery from offset 254990 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,600] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition financial_transactions-8 with TruncationState(offset=255525, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=8, leaderEpoch=5, endOffset=255525) (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,601] INFO [UnifiedLog partition=financial_transactions-8, dir=/tmp/kafka-logs] Truncating to offset 255525 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,615] INFO [UnifiedLog partition=financial_transactions-8, dir=/tmp/kafka-logs] Loading producer state till offset 255525 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,619] INFO [UnifiedLog partition=financial_transactions-8, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255525 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,620] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-8/00000000000000255535.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:28,647] INFO [ProducerStateManager partition=financial_transactions-8] Wrote producer snapshot at offset 255525 with 0 producer ids in 18 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:28,651] INFO [UnifiedLog partition=financial_transactions-8, dir=/tmp/kafka-logs] Producer state recovery took 9ms for snapshot load and 22ms for segment recovery from offset 255525 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,728] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition financial_transactions-9 with TruncationState(offset=255435, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=9, leaderEpoch=4, endOffset=255435) (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,736] INFO [UnifiedLog partition=financial_transactions-9, dir=/tmp/kafka-logs] Truncating to offset 255435 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,758] INFO [UnifiedLog partition=financial_transactions-9, dir=/tmp/kafka-logs] Loading producer state till offset 255435 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,761] INFO [UnifiedLog partition=financial_transactions-9, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255435 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,762] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-9/00000000000000255457.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:28,799] INFO [ProducerStateManager partition=financial_transactions-9] Wrote producer snapshot at offset 255435 with 0 producer ids in 25 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:28,800] INFO [UnifiedLog partition=financial_transactions-9, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 35ms for segment recovery from offset 255435 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,819] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition financial_transactions-12 with TruncationState(offset=255590, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=12, leaderEpoch=2, endOffset=255590) (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,822] INFO [UnifiedLog partition=financial_transactions-12, dir=/tmp/kafka-logs] Truncating to offset 255590 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,829] INFO [UnifiedLog partition=financial_transactions-12, dir=/tmp/kafka-logs] Loading producer state till offset 255590 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,831] INFO [UnifiedLog partition=financial_transactions-12, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255590 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,832] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-12/00000000000000255604.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:28,851] INFO [ProducerStateManager partition=financial_transactions-12] Wrote producer snapshot at offset 255590 with 0 producer ids in 13 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:28,853] INFO [UnifiedLog partition=financial_transactions-12, dir=/tmp/kafka-logs] Producer state recovery took 5ms for snapshot load and 16ms for segment recovery from offset 255590 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,896] INFO [Broker id=4] Transitioning 40 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:28,908] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:28,910] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:28,912] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:28,919] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:28,925] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:28,936] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=11, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 14:13:28,939] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:28,941] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:28,955] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:28,957] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:28,959] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:28,974] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=11, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:28,975] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:28,978] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:28,980] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:28,986] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:28,987] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 6], partitionEpoch=12, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 14:13:28,988] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:28,989] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:28,990] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=11, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:28,993] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:29,006] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:29,008] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:29,009] INFO [Broker id=4] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:29,009] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,010] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,010] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:29,012] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,012] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:29,013] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:29,013] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,013] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:29,014] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 6], partitionEpoch=12, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 14:13:29,014] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:29,014] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=12, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 14:13:29,018] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:29,020] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:29,020] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=12, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 14:13:29,026] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:29,027] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,029] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,036] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,040] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,042] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,042] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,042] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,043] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,043] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,043] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,044] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,044] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,044] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,047] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,047] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,047] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,048] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,048] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,049] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,049] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,050] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,050] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,052] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,052] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,053] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,054] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,054] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,055] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,056] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,046] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,056] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,057] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,058] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,057] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,058] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,059] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,059] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,059] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,060] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,059] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,060] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,061] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,061] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,061] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,062] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,062] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,062] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,063] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,063] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,063] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,061] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,070] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,071] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,072] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,072] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,063] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,073] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,074] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,074] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,075] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,074] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,076] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,076] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,076] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,076] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,077] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,077] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,078] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,078] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,078] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,079] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,078] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,086] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,087] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,088] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,089] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,089] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,090] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,091] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,091] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,091] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,079] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,094] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,095] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,095] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,096] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,097] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,098] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,173] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:29,174] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=11, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 14:13:29,562] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:29,563] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 14:13:29,678] INFO [Broker id=4] Transitioning 39 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:29,679] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:29,685] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:29,687] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:29,687] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:29,688] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 14:13:29,690] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:29,692] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:29,694] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,705] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,712] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:29,716] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:29,717] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:29,718] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:29,718] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:29,718] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 14:13:29,719] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:29,719] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:29,719] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:29,720] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:29,720] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:29,720] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:29,721] INFO [Broker id=4] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:29,721] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,721] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 14:13:29,722] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,722] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:29,722] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,723] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:29,723] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:29,723] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,724] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:29,724] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 14:13:29,724] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:29,737] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 14:13:29,738] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:29,739] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:29,750] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 14:13:29,753] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:29,753] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,754] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,755] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,762] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,763] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,771] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,772] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,772] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,772] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,773] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,773] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,774] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,774] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,774] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,775] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,775] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,775] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,775] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,776] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,777] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,778] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,779] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,778] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,780] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,779] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,780] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,781] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,781] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,782] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,782] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,782] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,783] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,787] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,781] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,788] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,789] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,790] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,792] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,793] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,789] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,794] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,794] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,794] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,795] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,795] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,795] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,795] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,796] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,796] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,799] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,800] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,801] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,801] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,801] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,794] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,804] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,806] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,804] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,807] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,807] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,809] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,809] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,810] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,810] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,808] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,811] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,812] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,813] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,811] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,813] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,814] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,814] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,815] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,815] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,813] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,816] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,817] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,818] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,818] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,819] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,819] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,820] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,820] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,821] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,821] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,909] INFO [Broker id=4] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:29,910] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:29,910] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,911] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,911] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,989] INFO [Broker id=4] Transitioning 26 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:29,991] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:29,992] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:29,995] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,996] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,995] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition financial_transactions-16 with TruncationState(offset=255412, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=16, leaderEpoch=4, endOffset=255412) (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:29,997] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:29,999] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:30,000] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:30,001] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,002] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,998] INFO [UnifiedLog partition=financial_transactions-16, dir=/tmp/kafka-logs] Truncating to offset 255412 (kafka.log.UnifiedLog)
[2025-05-21 14:13:30,002] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,007] INFO [UnifiedLog partition=financial_transactions-16, dir=/tmp/kafka-logs] Loading producer state till offset 255412 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:30,010] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,011] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:30,010] INFO [UnifiedLog partition=financial_transactions-16, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255412 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:30,013] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-16/00000000000000255425.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:30,022] INFO [ProducerStateManager partition=financial_transactions-16] Wrote producer snapshot at offset 255412 with 0 producer ids in 8 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:30,026] INFO [UnifiedLog partition=financial_transactions-16, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 12ms for segment recovery from offset 255412 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:30,036] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=12, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 14:13:30,038] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:30,039] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,039] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,040] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:30,041] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:30,042] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:30,042] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,042] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=11, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 14:13:30,043] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:30,044] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=11, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,044] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,044] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:30,045] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:30,045] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,046] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,046] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,046] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,047] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,048] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,049] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,053] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,057] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,057] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,057] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,057] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,057] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,058] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,058] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,059] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,059] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,060] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,060] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,061] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,061] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,061] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,062] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,063] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,063] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,063] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,064] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,064] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,065] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,068] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,069] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,073] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,074] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,074] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,076] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,077] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,077] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,083] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,084] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,084] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,085] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,086] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,090] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,091] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,091] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,086] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,096] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,097] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,092] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,098] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,099] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,099] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,102] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,107] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,108] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,117] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,118] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,118] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,120] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,124] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,126] INFO [Broker id=4] Transitioning 2 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:30,128] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=11, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 14:13:30,130] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,158] INFO [Broker id=4] Transitioning 28 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:30,159] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,160] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:30,160] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:30,161] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,161] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:30,162] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,162] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,163] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,164] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:30,164] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:30,165] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,167] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,167] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,169] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,172] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:30,172] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:30,174] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,176] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,182] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:30,183] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:30,184] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:30,185] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,190] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 14:13:30,191] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:30,191] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,192] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,193] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:30,193] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:30,194] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,194] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,195] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,195] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,196] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,196] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,196] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,196] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,197] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,197] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,198] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,201] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,201] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,202] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,203] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,203] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,204] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,207] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,208] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,208] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,211] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,211] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,213] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,213] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,214] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,214] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,214] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,214] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,212] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,215] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,215] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,216] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,216] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,217] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,217] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,217] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,218] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,218] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,219] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,218] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,219] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,220] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,220] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,220] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,221] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,220] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,223] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,224] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,224] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,225] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,225] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,225] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,225] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,225] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,226] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,226] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,226] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,226] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,226] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,224] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,229] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,229] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,230] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,230] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,230] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,230] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,698] INFO [Broker id=4] Transitioning 3 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:30,707] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 14:13:30,710] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,713] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 14:16:06,428] INFO [NodeToControllerChannelManager id=4 name=forwarding] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:16:06,431] INFO [broker-4-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:18:23,601] INFO [Broker id=4] Transitioning 24 partition(s) to local leaders. (state.change.logger)
[2025-05-21 14:18:23,608] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-16, _schemas-0, __consumer_offsets-13, financial_transactions-16, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-29, __consumer_offsets-30, financial_transactions-1, __consumer_offsets-26, financial_transactions-5, __consumer_offsets-7, __consumer_offsets-39, __consumer_offsets-5, __consumer_offsets-37, financial_transactions-8, financial_transactions-9, __consumer_offsets-2, __consumer_offsets-34) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:18:23,616] INFO [Broker id=4] Leader financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 9 from offset 268526 with partition epoch 14, high watermark 268526, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:18:23,624] INFO [Broker id=4] Leader __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 17 from offset 0 with partition epoch 29, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,629] INFO [Broker id=4] Leader _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) starts at leader epoch 17 from offset 8 with partition epoch 29, high watermark 8, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,634] INFO [Broker id=4] Leader __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 29, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:18:23,639] INFO [Broker id=4] Leader financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 8 from offset 267900 with partition epoch 14, high watermark 267900, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:18:23,644] INFO [Broker id=4] Leader __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 29, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:18:23,650] INFO [Broker id=4] Leader __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 29, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:18:23,653] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-9 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,654] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-9 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,654] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-42 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,655] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-42 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,656] INFO [Broker id=4] Leader __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 29, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:18:23,656] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-38 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,657] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-38 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,658] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-1 has an older epoch (13) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,659] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-1 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,659] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-41 has an older epoch (13) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,660] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-41 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,660] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-20 has an older epoch (13) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,660] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-20 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,660] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-25 has an older epoch (13) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,661] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-25 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,662] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-8 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,662] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-8 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,662] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-4 has an older epoch (13) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,663] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-4 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,663] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-15 has an older epoch (13) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,662] INFO [Broker id=4] Leader __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 17 from offset 0 with partition epoch 29, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,664] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-15 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,664] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-48 has an older epoch (13) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,665] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-48 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,665] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-28 has an older epoch (13) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,666] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-28 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,669] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-47 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,669] INFO [Broker id=4] Leader __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 17 from offset 0 with partition epoch 29, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,671] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-47 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,674] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-14 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,675] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-14 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,676] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-22 has an older epoch (13) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,677] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-22 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,678] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-31 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,679] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-31 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,680] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-35 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,680] INFO [Broker id=4] Leader __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 17 from offset 0 with partition epoch 29, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,681] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition __consumer_offsets-35 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,681] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition financial_transactions-7 has an older epoch (6) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,682] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition financial_transactions-7 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,682] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition financial_transactions-3 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,683] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition financial_transactions-3 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,686] INFO [Broker id=4] Leader __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 17 from offset 7 with partition epoch 29, high watermark 7, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,690] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition financial_transactions-17 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,692] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition financial_transactions-17 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,692] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition financial_transactions-10 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,693] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition financial_transactions-10 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,693] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition financial_transactions-12 has an older epoch (6) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,694] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition financial_transactions-12 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,694] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition financial_transactions-18 has an older epoch (5) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,695] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition financial_transactions-18 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,695] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition financial_transactions-0 has an older epoch (6) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,696] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Partition financial_transactions-0 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,695] INFO [Broker id=4] Leader __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 29, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:18:23,702] INFO [Broker id=4] Leader financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 9 from offset 267918 with partition epoch 14, high watermark 267918, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:18:23,707] INFO [Broker id=4] Leader __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 17 from offset 0 with partition epoch 29, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,713] INFO [Broker id=4] Leader financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 8 from offset 267422 with partition epoch 14, high watermark 267422, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:18:23,718] INFO [Broker id=4] Leader __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 29, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:18:23,722] INFO [Broker id=4] Leader __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 17 from offset 0 with partition epoch 29, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,727] INFO [Broker id=4] Leader __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 17 from offset 0 with partition epoch 29, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,731] INFO [Broker id=4] Leader __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 29, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:18:23,735] INFO [Broker id=4] Leader financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 9 from offset 267896 with partition epoch 14, high watermark 267896, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:18:23,740] INFO [Broker id=4] Leader financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 8 from offset 268043 with partition epoch 14, high watermark 268043, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:18:23,744] INFO [Broker id=4] Leader __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 17 from offset 0 with partition epoch 29, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,749] INFO [Broker id=4] Leader __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 29, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:18:23,754] INFO [Broker id=4] Transitioning 24 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:18:23,754] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 14 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:18:23,755] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 12 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:18:23,755] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 14 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:18:23,755] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 12 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:18:23,755] INFO [Broker id=4] Follower financial_transactions-17 starts at leader epoch 6 from offset 268373 with partition epoch 13 and high watermark 268373. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:18:23,756] INFO [Broker id=4] Follower financial_transactions-18 starts at leader epoch 6 from offset 267868 with partition epoch 13 and high watermark 267868. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:18:23,756] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 12 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:18:23,756] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 14 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:18:23,756] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 12 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:18:23,757] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 14 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:18:23,757] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 14 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:18:23,757] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 12 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:18:23,757] INFO [Broker id=4] Follower financial_transactions-0 starts at leader epoch 7 from offset 269651 with partition epoch 13 and high watermark 269651. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:18:23,758] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 14 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:18:23,758] INFO [Broker id=4] Follower financial_transactions-3 starts at leader epoch 6 from offset 268318 with partition epoch 13 and high watermark 268318. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:18:23,758] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 14 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:18:23,759] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 12 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:18:23,759] INFO [Broker id=4] Follower financial_transactions-7 starts at leader epoch 7 from offset 268422 with partition epoch 13 and high watermark 268422. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:18:23,759] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 12 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:18:23,759] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 12 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:18:23,760] INFO [Broker id=4] Follower financial_transactions-10 starts at leader epoch 6 from offset 268988 with partition epoch 13 and high watermark 268988. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:18:23,760] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 14 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:18:23,760] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 14 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:18:23,760] INFO [Broker id=4] Follower financial_transactions-12 starts at leader epoch 7 from offset 268072 with partition epoch 13 and high watermark 268072. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:18:23,762] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, financial_transactions-0, __consumer_offsets-28, __consumer_offsets-38, financial_transactions-10, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-47, __consumer_offsets-14, financial_transactions-18, __consumer_offsets-41, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-31, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-8, financial_transactions-7, __consumer_offsets-35, __consumer_offsets-4) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:18:23,762] INFO [Broker id=4] Stopped fetchers as part of become-follower for 24 partitions (state.change.logger)
[2025-05-21 14:18:23,765] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,766] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),14,0), __consumer_offsets-48 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),14,0), financial_transactions-17 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,268373), __consumer_offsets-9 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), __consumer_offsets-42 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), financial_transactions-0 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,269651), __consumer_offsets-28 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),14,0), __consumer_offsets-38 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), financial_transactions-10 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,268988), __consumer_offsets-1 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),14,0), financial_transactions-12 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,268072), __consumer_offsets-47 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), __consumer_offsets-14 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), financial_transactions-18 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,267868), __consumer_offsets-41 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),14,0), __consumer_offsets-22 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),14,0), __consumer_offsets-20 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),14,0), __consumer_offsets-31 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), financial_transactions-3 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,268318), __consumer_offsets-25 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),14,0), __consumer_offsets-8 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), financial_transactions-7 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,268422), __consumer_offsets-35 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), __consumer_offsets-4 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),14,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:18:23,766] INFO [Broker id=4] Started fetchers as part of become-follower for 24 partitions (state.change.logger)
[2025-05-21 14:18:23,766] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,766] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,767] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,767] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,767] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,767] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,768] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,768] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,768] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,769] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,769] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,769] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,769] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,770] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,770] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,770] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,770] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,771] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,771] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,771] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,772] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,772] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,772] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,772] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,773] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,773] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,773] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,773] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,774] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,774] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,774] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,774] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,774] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,775] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,776] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 16 in epoch 17 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,777] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,778] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 13 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,778] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,778] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 46 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,779] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,779] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 11 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,779] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,780] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 44 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,780] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,780] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 23 in epoch 17 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,781] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,781] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 49 in epoch 17 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,781] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,781] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 18 in epoch 17 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,782] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,782] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 29 in epoch 17 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,782] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,783] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 30 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,783] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,783] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 26 in epoch 17 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,783] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,784] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 7 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,784] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,784] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-16 in 6 milliseconds for epoch 17, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,784] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 39 in epoch 17 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,785] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-13 in 7 milliseconds for epoch 16, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,785] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,785] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-46 in 6 milliseconds for epoch 16, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,785] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 5 in epoch 17 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,786] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,786] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-11 in 7 milliseconds for epoch 16, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,786] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 37 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,786] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-44 in 6 milliseconds for epoch 16, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,786] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,787] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-23 in 6 milliseconds for epoch 17, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,787] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 2 in epoch 17 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,788] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,787] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-49 in 6 milliseconds for epoch 17, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,788] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 34 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,788] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-18 in 6 milliseconds for epoch 17, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,788] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,789] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,789] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,789] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,790] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,790] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,790] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,790] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,791] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,791] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,791] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,792] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,792] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,792] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,792] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,793] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,793] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,793] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,793] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,793] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,794] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,794] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,794] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,794] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,795] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,795] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,795] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,795] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,796] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,796] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,796] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,796] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,797] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,797] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,797] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,804] INFO Loaded member MemberMetadata(memberId=sr-1-99122467-ec1a-4fc4-88f0-59fda8f60096, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 1. (kafka.coordinator.group.GroupMetadata$)
[2025-05-21 14:18:23,804] INFO Loaded member MemberMetadata(memberId=sr-1-3027da35-fa63-4fb3-9cba-2acfe293f9a1, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 3. (kafka.coordinator.group.GroupMetadata$)
[2025-05-21 14:18:23,805] INFO Loaded member MemberMetadata(memberId=sr-1-960e2a90-cf63-43a4-b2ba-89aa2499d218, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.8, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 5. (kafka.coordinator.group.GroupMetadata$)
[2025-05-21 14:18:23,805] INFO Loaded member MemberMetadata(memberId=sr-1-11f3f728-c325-475e-a3a2-6b3e8884ce37, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 7. (kafka.coordinator.group.GroupMetadata$)
[2025-05-21 14:18:23,806] INFO [GroupCoordinator 4]: Loading group metadata for schema-registry with generation 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,807] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-29 in 24 milliseconds for epoch 17, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,808] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-30 in 25 milliseconds for epoch 16, of which 25 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,808] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-26 in 24 milliseconds for epoch 17, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,808] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-7 in 24 milliseconds for epoch 16, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,809] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-39 in 24 milliseconds for epoch 17, of which 23 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,809] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-5 in 23 milliseconds for epoch 17, of which 23 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,809] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-37 in 22 milliseconds for epoch 16, of which 22 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,809] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-2 in 21 milliseconds for epoch 17, of which 21 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,810] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-34 in 21 milliseconds for epoch 16, of which 20 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,810] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,810] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,810] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,810] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,811] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,811] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,811] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,811] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,811] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,812] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,812] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,812] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,812] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,813] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,813] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,813] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,813] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:23:21,575] INFO [RaftManager id=4] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:26:06,752] INFO [NodeToControllerChannelManager id=4 name=forwarding] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:45:30,292] INFO [GroupCoordinator 4]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 7 (__consumer_offsets-29) (reason: Removing member sr-1-11f3f728-c325-475e-a3a2-6b3e8884ce37 on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:30,297] INFO [GroupCoordinator 4]: Group schema-registry with generation 8 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:30,309] INFO [GroupCoordinator 4]: Member MemberMetadata(memberId=sr-1-11f3f728-c325-475e-a3a2-6b3e8884ce37, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:31,878] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-21 14:45:31,893] INFO [BrokerServer id=4] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
[2025-05-21 14:45:31,893] INFO [BrokerServer id=4] shutting down (kafka.server.BrokerServer)
[2025-05-21 14:45:31,895] INFO [BrokerLifecycleManager id=4] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:45:31,947] INFO [Broker id=4] Transitioning 36 partition(s) to local leaders. (state.change.logger)
[2025-05-21 14:45:31,951] INFO [BrokerLifecycleManager id=4] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:45:31,969] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, financial_transactions-0, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, financial_transactions-16, __consumer_offsets-41, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:45:31,970] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-9 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,970] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-9 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,971] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-42 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,971] INFO [Broker id=4] Skipped the become-leader state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=4, leaderEpoch=9, isr=[5, 4], partitionEpoch=15, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 305468, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:31,972] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-42 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,973] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-17 has an older epoch (6) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,973] INFO [Broker id=4] Leader __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 28, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:31,973] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-17 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,978] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-38 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,978] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-38 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,979] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-18 has an older epoch (6) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,980] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-18 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,981] INFO [Broker id=4] Leader __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 28, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:31,992] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:31,993] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:31,994] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:31,995] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:31,997] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 17. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,000] INFO [Broker id=4] Leader financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 8 from offset 306847 with partition epoch 14, high watermark 306847, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,034] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,037] INFO [Broker id=4] Leader __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 28, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,083] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-47 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,089] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-47 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,100] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-14 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,112] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-14 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,119] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-31 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,134] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-31 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,135] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-8 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,136] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-8 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,136] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-35 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,137] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition __consumer_offsets-35 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,137] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-3 has an older epoch (6) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,140] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-3 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,142] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-10 has an older epoch (6) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,144] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Partition financial_transactions-10 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,149] INFO [BrokerLifecycleManager id=4] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:45:32,182] INFO [BrokerLifecycleManager id=4] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:32,186] INFO [BrokerLifecycleManager id=4] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:45:32,172] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Node 6 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:45:32,188] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Cancelled in-flight FETCH request with correlation id 21952 due to node 6 being disconnected (elapsed time since creation: 24ms, elapsed time since send: 24ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:45:32,189] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Client requested connection close from node 6 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:45:32,150] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 17. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,187] INFO [broker-4-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,190] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,191] INFO [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Error sending fetch request (sessionId=313972793, epoch=21952) to node 6: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 6 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 14:45:32,224] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Node 5 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:45:32,250] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Cancelled in-flight FETCH request with correlation id 33802 due to node 5 being disconnected (elapsed time since creation: 83ms, elapsed time since send: 83ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:45:32,193] INFO [broker-4-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,193] INFO [broker-4-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,192] INFO [SocketServer listenerType=BROKER, nodeId=4] Stopping socket server request processors (kafka.network.SocketServer)
[2025-05-21 14:45:32,272] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 14:45:32,191] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 17. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,250] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:45:32,279] INFO [Broker id=4] Skipped the become-leader state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=4, leaderEpoch=9, isr=[5, 4], partitionEpoch=15, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 304749, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,249] WARN [ReplicaFetcher replicaId=4, leaderId=6, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=4, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-47, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-14, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-31, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-8, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-35, 0e8v3fGFR_uwy9DAR-lNZA:financial_transactions-10, 0e8v3fGFR_uwy9DAR-lNZA:financial_transactions-3, replaced=, metadata=(sessionId=313972793, epoch=21952), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 6 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 14:45:32,279] INFO [Broker id=4] Leader __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 28, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,279] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=340145623, epoch=33802) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 14:45:32,281] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=4, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={financial_transactions-19=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=304475, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[6], lastFetchedEpoch=Optional[6]), financial_transactions-2=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=305058, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[6], lastFetchedEpoch=Optional[6])}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=340145623, epoch=33802), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 14:45:32,301] INFO [Broker id=4] Leader financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 8 from offset 305079 with partition epoch 14, high watermark 305062, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,307] INFO [SocketServer listenerType=BROKER, nodeId=4] Stopped socket server request processors (kafka.network.SocketServer)
[2025-05-21 14:45:32,312] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,313] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 17. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,316] INFO [Broker id=4] Skipped the become-leader state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 17. Current high watermark 8, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,317] INFO [Broker id=4] Skipped the become-leader state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=4, leaderEpoch=8, isr=[5, 4], partitionEpoch=15, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 304604, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,320] INFO [Broker id=4] Leader __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 28, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,334] INFO [Broker id=4] Leader __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 28, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,345] INFO [Broker id=4] Leader __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 28, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,354] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 17. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,356] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 17. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,361] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 17. Current high watermark 8, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,366] INFO [Broker id=4] Skipped the become-leader state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=4, leaderEpoch=9, isr=[5, 4], partitionEpoch=15, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 304782, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,368] INFO [Broker id=4] Leader __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 28, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,376] INFO [Broker id=4] Skipped the become-leader state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=4, leaderEpoch=8, isr=[5, 4], partitionEpoch=15, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 304275, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,378] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 17. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,379] INFO [Broker id=4] Leader financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 8 from offset 305027 with partition epoch 14, high watermark 305027, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,392] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,394] INFO [Broker id=4] Skipped the become-leader state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=4, leaderEpoch=8, isr=[5, 4], partitionEpoch=15, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 304857, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,395] INFO [Broker id=4] Leader __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 28, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,405] INFO [Broker id=4] Skipped the become-leader state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 17. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,406] INFO [Broker id=4] Transitioning 35 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:45:32,407] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:45:32,408] INFO [Broker id=4] Follower financial_transactions-17 starts at leader epoch 7 from offset 305279 with partition epoch 14 and high watermark 305279. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,409] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:45:32,409] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 13 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:45:32,410] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 13 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:45:32,412] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,414] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,420] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,422] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,425] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:45:32,427] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:45:32,430] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:45:32,431] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,431] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 13 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:45:32,434] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,434] INFO [Broker id=4] Follower financial_transactions-10 starts at leader epoch 7 from offset 305673 with partition epoch 14 and high watermark 305673. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,436] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,440] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 13 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:45:32,445] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:45:32,450] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,451] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 13 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:45:32,452] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,463] INFO [Broker id=4] Follower financial_transactions-18 starts at leader epoch 7 from offset 304723 with partition epoch 14 and high watermark 304723. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,465] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,468] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,469] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,483] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 13 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:45:32,484] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,485] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,486] INFO [Broker id=4] Follower financial_transactions-3 starts at leader epoch 7 from offset 305161 with partition epoch 14 and high watermark 305161. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,488] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 13 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:45:32,492] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,493] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 13 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:45:32,494] INFO [Broker id=4] Skipped the become-follower state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:45:32,494] INFO [Broker id=4] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,501] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions Set(__consumer_offsets-47, __consumer_offsets-14, financial_transactions-17, financial_transactions-18, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-31, financial_transactions-3, __consumer_offsets-8, __consumer_offsets-38, __consumer_offsets-35, financial_transactions-10) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:45:32,502] INFO [Broker id=4] Stopped fetchers as part of become-follower for 12 partitions (state.change.logger)
[2025-05-21 14:45:32,505] INFO [ReplicaFetcherManager on broker 4] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-47 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), __consumer_offsets-14 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), financial_transactions-17 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,305279), financial_transactions-18 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,304723), __consumer_offsets-9 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), __consumer_offsets-42 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), __consumer_offsets-31 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), financial_transactions-3 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,305161), __consumer_offsets-8 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), __consumer_offsets-38 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), __consumer_offsets-35 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), financial_transactions-10 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,305673)) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:45:32,505] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,506] INFO [Broker id=4] Started fetchers as part of become-follower for 12 partitions (state.change.logger)
[2025-05-21 14:45:32,507] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:45:32,508] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,509] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:45:32,508] INFO [ReplicaFetcherThread-0-6]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,520] INFO [ReplicaFetcherThread-0-6]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,519] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,521] INFO [ReplicaFetcherThread-0-6]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,535] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:45:32,536] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,538] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:45:32,539] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,540] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:45:32,543] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,547] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:45:32,548] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,549] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:45:32,549] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,550] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:45:32,552] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 15 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,560] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,555] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Node 5 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:45:32,562] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-15 in 1 milliseconds for epoch 15, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,562] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 48 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,570] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Connection to node 5 (kafka-broker-2/172.19.0.11:19092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:45:32,571] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,573] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:45:32,574] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 28 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,577] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,578] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 25 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,574] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-48 in 1 milliseconds for epoch 15, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,583] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,574] INFO [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=340145623, epoch=INITIAL) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to kafka-broker-2:19092 (id: 5 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 14:45:32,588] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 41 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,587] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-28 in 9 milliseconds for epoch 15, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,590] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,590] WARN [ReplicaFetcher replicaId=4, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=4, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={financial_transactions-17=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=305279, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[7], lastFetchedEpoch=Optional[6]), financial_transactions-10=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=305673, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[7], lastFetchedEpoch=Optional[6]), financial_transactions-18=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=304723, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[7], lastFetchedEpoch=Optional[6]), financial_transactions-3=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=305161, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[7], lastFetchedEpoch=Optional[6]), __consumer_offsets-9=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[13], lastFetchedEpoch=Optional.empty), __consumer_offsets-42=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[13], lastFetchedEpoch=Optional.empty), __consumer_offsets-38=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[13], lastFetchedEpoch=Optional.empty), __consumer_offsets-47=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[13], lastFetchedEpoch=Optional.empty), __consumer_offsets-14=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[13], lastFetchedEpoch=Optional.empty), __consumer_offsets-31=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[13], lastFetchedEpoch=Optional.empty), __consumer_offsets-8=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[13], lastFetchedEpoch=Optional.empty), __consumer_offsets-35=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[13], lastFetchedEpoch=Optional.empty)}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=340145623, epoch=INITIAL), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to kafka-broker-2:19092 (id: 5 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 14:45:32,591] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 22 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,590] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-25 in 3 milliseconds for epoch 15, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,594] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,595] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-41 in 4 milliseconds for epoch 15, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,596] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 4 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,607] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-22 in 12 milliseconds for epoch 15, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,608] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,610] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 20 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,611] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds for epoch 15, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,612] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,614] INFO [GroupCoordinator 4]: Elected as the group coordinator for partition 1 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,614] INFO [GroupMetadataManager brokerId=4] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,615] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,616] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,617] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,614] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds for epoch 15, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,618] INFO [GroupMetadataManager brokerId=4] Finished loading offsets and group metadata from __consumer_offsets-1 in 3 milliseconds for epoch 15, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,618] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,619] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,621] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,621] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,626] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,627] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,628] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,628] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,630] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,632] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,632] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,633] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,636] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,631] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,639] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,640] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,641] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,642] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,643] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,646] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,650] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,652] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,655] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,661] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,661] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,664] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,665] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,665] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,668] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,670] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,670] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,671] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,674] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,674] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,676] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,678] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,678] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,680] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,683] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,684] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,685] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,688] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,689] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,689] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,689] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,689] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,690] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,690] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,691] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,690] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,691] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,691] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,692] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,693] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,694] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,694] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,695] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,693] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,698] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,698] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,699] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,698] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,706] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,706] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,708] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,710] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,711] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,710] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,715] INFO [Broker id=4] Transitioning 71 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:45:32,718] INFO [Broker id=4] Follower financial_transactions-13 starts at leader epoch 11 from offset 305495 with partition epoch 17 and high watermark 305484. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,719] INFO [Broker id=4] Follower __consumer_offsets-13 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,718] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,721] INFO [Broker id=4] Follower __consumer_offsets-46 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,724] INFO [Broker id=4] Follower financial_transactions-17 starts at leader epoch 8 from offset 305279 with partition epoch 16 and high watermark 305279. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:45:32,725] INFO [Broker id=4] Follower __consumer_offsets-9 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,726] INFO [Broker id=4] Follower __consumer_offsets-42 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,730] INFO [Broker id=4] Follower __consumer_offsets-21 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,732] INFO [Broker id=4] Follower __consumer_offsets-17 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,733] INFO [Broker id=4] Follower financial_transactions-0 starts at leader epoch 10 from offset 306864 with partition epoch 16 and high watermark 306847. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:45:32,735] INFO [Broker id=4] Follower __consumer_offsets-30 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,736] INFO [Broker id=4] Follower financial_transactions-4 starts at leader epoch 7 from offset 304585 with partition epoch 16 and high watermark 304585. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,738] INFO [Broker id=4] Follower __consumer_offsets-26 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,740] INFO [Broker id=4] Follower __consumer_offsets-5 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,742] INFO [Broker id=4] Follower financial_transactions-8 starts at leader epoch 11 from offset 304771 with partition epoch 17 and high watermark 304757. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,743] INFO [Broker id=4] Follower __consumer_offsets-38 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,744] INFO [Broker id=4] Follower __consumer_offsets-1 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,745] INFO [Broker id=4] Follower financial_transactions-12 starts at leader epoch 10 from offset 305079 with partition epoch 16 and high watermark 305062. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:45:32,745] INFO [Broker id=4] Follower __consumer_offsets-34 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,747] INFO [Broker id=4] Follower financial_transactions-14 starts at leader epoch 7 from offset 305196 with partition epoch 16 and high watermark 305191. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,748] INFO [Broker id=4] Follower __consumer_offsets-16 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,749] INFO [Broker id=4] Follower _schemas-0 starts at leader epoch 19 from offset 8 with partition epoch 32 and high watermark 8. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,751] INFO [Broker id=4] Follower __consumer_offsets-45 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,754] INFO [Broker id=4] Follower financial_transactions-18 starts at leader epoch 8 from offset 304723 with partition epoch 16 and high watermark 304723. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:45:32,762] INFO [Broker id=4] Follower __consumer_offsets-12 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,763] INFO [Broker id=4] Follower __consumer_offsets-41 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,764] INFO [Broker id=4] Follower __consumer_offsets-24 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,765] INFO [Broker id=4] Follower __consumer_offsets-20 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,765] INFO [Broker id=4] Follower __consumer_offsets-49 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,766] INFO [Broker id=4] Follower __consumer_offsets-0 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,766] INFO [Broker id=4] Follower __consumer_offsets-29 starts at leader epoch 19 from offset 8 with partition epoch 32 and high watermark 8. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,767] INFO [Broker id=4] Follower financial_transactions-1 starts at leader epoch 11 from offset 304797 with partition epoch 17 and high watermark 304786. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,768] INFO [Broker id=4] Follower __consumer_offsets-25 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,769] INFO [Broker id=4] Follower financial_transactions-5 starts at leader epoch 10 from offset 304293 with partition epoch 17 and high watermark 304281. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:45:32,771] INFO [Broker id=4] Follower __consumer_offsets-8 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,774] INFO [Broker id=4] Follower __consumer_offsets-37 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,775] INFO [Broker id=4] Follower financial_transactions-9 starts at leader epoch 10 from offset 304868 with partition epoch 17 and high watermark 304859. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:45:32,776] INFO [Broker id=4] Follower __consumer_offsets-4 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,777] INFO [Broker id=4] Follower __consumer_offsets-33 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,778] INFO [Broker id=4] Follower __consumer_offsets-15 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,784] INFO [Broker id=4] Follower __consumer_offsets-48 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,788] INFO [Broker id=4] Follower financial_transactions-15 starts at leader epoch 7 from offset 305572 with partition epoch 16 and high watermark 305563. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,789] INFO [Broker id=4] Follower __consumer_offsets-11 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,789] INFO [Broker id=4] Follower __consumer_offsets-44 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,790] INFO [Broker id=4] Follower financial_transactions-19 starts at leader epoch 7 from offset 304475 with partition epoch 16 and high watermark 304471. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,791] INFO [Broker id=4] Follower __consumer_offsets-23 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,792] INFO [Broker id=4] Follower __consumer_offsets-19 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,794] INFO [Broker id=4] Follower __consumer_offsets-32 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,794] INFO [Broker id=4] Follower financial_transactions-2 starts at leader epoch 7 from offset 305058 with partition epoch 16 and high watermark 305054. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,795] INFO [Broker id=4] Follower __consumer_offsets-28 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,795] INFO [Broker id=4] Follower __consumer_offsets-7 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,796] INFO [Broker id=4] Follower financial_transactions-6 starts at leader epoch 7 from offset 304616 with partition epoch 16 and high watermark 304608. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,798] INFO [Broker id=4] Follower __consumer_offsets-40 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,799] INFO [Broker id=4] Follower __consumer_offsets-3 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,801] INFO [Broker id=4] Follower financial_transactions-10 starts at leader epoch 8 from offset 305673 with partition epoch 16 and high watermark 305673. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:45:32,802] INFO [Broker id=4] Follower __consumer_offsets-36 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,803] INFO [Broker id=4] Follower __consumer_offsets-47 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,805] INFO [Broker id=4] Follower financial_transactions-16 starts at leader epoch 10 from offset 304634 with partition epoch 17 and high watermark 304618. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:45:32,806] INFO [Broker id=4] Follower __consumer_offsets-14 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,807] INFO [Broker id=4] Follower __consumer_offsets-43 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,808] INFO [Broker id=4] Follower __consumer_offsets-10 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,809] INFO [Broker id=4] Follower __consumer_offsets-22 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,809] INFO [Broker id=4] Follower __consumer_offsets-18 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,810] INFO [Broker id=4] Follower __consumer_offsets-31 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,811] INFO [Broker id=4] Follower __consumer_offsets-27 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,812] INFO [Broker id=4] Follower financial_transactions-3 starts at leader epoch 8 from offset 305161 with partition epoch 16 and high watermark 305161. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:45:32,812] INFO [Broker id=4] Follower __consumer_offsets-39 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,815] INFO [Broker id=4] Follower financial_transactions-7 starts at leader epoch 10 from offset 305027 with partition epoch 16 and high watermark 305027. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:45:32,816] INFO [Broker id=4] Follower __consumer_offsets-6 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,816] INFO [Broker id=4] Follower __consumer_offsets-35 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,817] INFO [Broker id=4] Follower financial_transactions-11 starts at leader epoch 7 from offset 305371 with partition epoch 16 and high watermark 305362. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,818] INFO [Broker id=4] Follower __consumer_offsets-2 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,822] INFO [ReplicaFetcherManager on broker 4] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, financial_transactions-18, financial_transactions-12, financial_transactions-11, __consumer_offsets-8, __consumer_offsets-21, financial_transactions-15, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, financial_transactions-4, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, financial_transactions-0, financial_transactions-6, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, financial_transactions-16, __consumer_offsets-15, financial_transactions-10, __consumer_offsets-24, financial_transactions-19, financial_transactions-7, __consumer_offsets-17, financial_transactions-17, financial_transactions-1, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, financial_transactions-14, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, financial_transactions-8, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, financial_transactions-2, financial_transactions-13, __consumer_offsets-32, financial_transactions-5, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:45:32,825] INFO [ReplicaAlterLogDirsManager on broker 4] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, financial_transactions-18, financial_transactions-12, financial_transactions-11, __consumer_offsets-8, __consumer_offsets-21, financial_transactions-15, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, financial_transactions-4, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, financial_transactions-0, financial_transactions-6, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, financial_transactions-16, __consumer_offsets-15, financial_transactions-10, __consumer_offsets-24, financial_transactions-19, financial_transactions-7, __consumer_offsets-17, financial_transactions-17, financial_transactions-1, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, financial_transactions-14, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, financial_transactions-8, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, financial_transactions-2, financial_transactions-13, __consumer_offsets-32, financial_transactions-5, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-21 14:45:32,833] INFO [Broker id=4] Stopped fetchers as part of controlled shutdown for 71 partitions (state.change.logger)
[2025-05-21 14:45:32,839] INFO [ReplicaFetcherThread-0-5]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,840] INFO [ReplicaFetcherThread-0-5]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,840] INFO [ReplicaFetcherThread-0-5]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,843] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,843] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,844] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,845] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,846] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,858] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,854] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,860] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,860] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,861] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,861] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,862] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,862] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,866] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,868] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,868] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,876] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,877] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,878] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,878] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,879] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,878] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,880] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,883] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,884] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,884] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,884] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,885] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,883] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,885] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,888] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,888] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,889] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,890] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,890] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,890] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,889] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,891] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,894] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,895] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,892] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,900] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,900] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,901] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,901] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,902] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,899] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,904] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,904] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,905] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,907] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,907] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,909] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,907] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,911] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,911] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,916] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,921] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,922] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,916] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,924] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,924] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,928] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,934] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,932] INFO [GroupCoordinator 4]: Unloading group metadata for schema-registry with generation 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,936] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,939] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,939] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,940] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,940] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,941] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,941] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,944] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,944] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,944] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,946] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,946] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,949] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,950] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,950] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,950] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,951] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,951] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,952] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,954] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,955] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,954] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,956] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,955] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,956] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,957] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,957] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,957] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,960] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,960] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,961] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,964] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,965] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,969] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,970] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,971] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,971] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,971] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,971] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,972] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,972] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,972] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,973] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,973] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,973] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,973] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,974] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,974] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,974] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,976] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,979] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,980] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,981] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,981] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,983] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,986] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,986] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,986] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,987] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,988] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,987] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,988] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,989] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,991] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,998] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,999] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,999] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,998] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:33,000] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:33,001] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:33,000] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:33,002] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:33,003] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:33,003] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:33,004] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:33,005] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:33,006] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:33,007] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:33,007] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:33,009] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:33,009] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:33,009] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:33,010] INFO [GroupCoordinator 4]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:33,010] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:33,010] INFO [GroupMetadataManager brokerId=4] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:33,011] INFO [GroupMetadataManager brokerId=4] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:33,013] INFO [data-plane Kafka Request Handler on Broker 4], shutting down (kafka.server.KafkaRequestHandlerPool)
[2025-05-21 14:45:33,017] INFO [data-plane Kafka Request Handler on Broker 4], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2025-05-21 14:45:33,018] INFO [ExpirationReaper-4-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,024] INFO [ExpirationReaper-4-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,024] INFO [ExpirationReaper-4-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,026] INFO [KafkaApi-4] Shutdown complete. (kafka.server.KafkaApis)
[2025-05-21 14:45:33,032] INFO [TransactionCoordinator id=4] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 14:45:33,036] INFO [Transaction State Manager 4]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2025-05-21 14:45:33,036] INFO [TxnMarkerSenderThread-4]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 14:45:33,037] INFO [TxnMarkerSenderThread-4]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 14:45:33,037] INFO [TxnMarkerSenderThread-4]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 14:45:33,042] INFO [TransactionCoordinator id=4] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 14:45:33,043] INFO [GroupCoordinator 4]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:33,045] INFO [ExpirationReaper-4-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,049] INFO [ExpirationReaper-4-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,049] INFO [ExpirationReaper-4-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,052] INFO [ExpirationReaper-4-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,053] INFO [ExpirationReaper-4-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,053] INFO [ExpirationReaper-4-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,055] INFO [GroupCoordinator 4]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:33,058] INFO [AssignmentsManager id=4]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,065] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:33,066] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:33,066] INFO [broker-4-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:33,068] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 14:45:33,069] INFO [AssignmentsManager id=4]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,070] INFO [ReplicaManager broker=4] Shutting down (kafka.server.ReplicaManager)
[2025-05-21 14:45:33,071] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 14:45:33,072] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 14:45:33,072] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 14:45:33,074] INFO [ReplicaFetcherManager on broker 4] shutting down (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:45:33,075] INFO [ReplicaFetcherManager on broker 4] shutdown completed (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:45:33,076] INFO [ReplicaAlterLogDirsManager on broker 4] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-21 14:45:33,077] INFO [ReplicaAlterLogDirsManager on broker 4] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-21 14:45:33,078] INFO [ExpirationReaper-4-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,079] INFO [ExpirationReaper-4-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,079] INFO [ExpirationReaper-4-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,080] INFO [ExpirationReaper-4-RemoteFetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,081] INFO [ExpirationReaper-4-RemoteFetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,081] INFO [ExpirationReaper-4-RemoteFetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,083] INFO [ExpirationReaper-4-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,084] INFO [ExpirationReaper-4-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,084] INFO [ExpirationReaper-4-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,085] INFO [ExpirationReaper-4-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,086] INFO [ExpirationReaper-4-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,086] INFO [ExpirationReaper-4-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,087] INFO [ExpirationReaper-4-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,088] INFO [ExpirationReaper-4-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,088] INFO [ExpirationReaper-4-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,100] INFO [AddPartitionsToTxnSenderThread-4]: Shutting down (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 14:45:33,100] INFO [AddPartitionsToTxnSenderThread-4]: Stopped (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 14:45:33,100] INFO [AddPartitionsToTxnSenderThread-4]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 14:45:33,102] INFO [ReplicaManager broker=4] Shut down completely (kafka.server.ReplicaManager)
[2025-05-21 14:45:33,103] INFO [broker-4-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:33,105] INFO [broker-4-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:33,105] INFO [broker-4-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:33,106] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 14:45:33,107] INFO [broker-4-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:33,108] INFO [broker-4-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:33,108] INFO [broker-4-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:33,109] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 14:45:33,110] INFO Shutting down. (kafka.log.LogManager)
[2025-05-21 14:45:33,113] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
[2025-05-21 14:45:33,114] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 14:45:33,117] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 14:45:33,117] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 14:45:33,149] INFO [ProducerStateManager partition=financial_transactions-15] Wrote producer snapshot at offset 305572 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,192] INFO [ProducerStateManager partition=financial_transactions-0] Wrote producer snapshot at offset 306864 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,310] INFO [ProducerStateManager partition=financial_transactions-3] Wrote producer snapshot at offset 305161 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,344] INFO [ProducerStateManager partition=financial_transactions-10] Wrote producer snapshot at offset 305673 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,350] INFO [ProducerStateManager partition=financial_transactions-12] Wrote producer snapshot at offset 305079 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,356] INFO [ProducerStateManager partition=financial_transactions-13] Wrote producer snapshot at offset 305495 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,375] INFO [ProducerStateManager partition=financial_transactions-19] Wrote producer snapshot at offset 304475 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,380] INFO [ProducerStateManager partition=financial_transactions-5] Wrote producer snapshot at offset 304293 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,386] INFO [ProducerStateManager partition=financial_transactions-6] Wrote producer snapshot at offset 304616 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,412] INFO [ProducerStateManager partition=financial_transactions-17] Wrote producer snapshot at offset 305279 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,418] INFO [ProducerStateManager partition=financial_transactions-7] Wrote producer snapshot at offset 305027 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,437] INFO [ProducerStateManager partition=__consumer_offsets-29] Wrote producer snapshot at offset 8 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,478] INFO [ProducerStateManager partition=financial_transactions-4] Wrote producer snapshot at offset 304585 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,499] INFO [ProducerStateManager partition=financial_transactions-18] Wrote producer snapshot at offset 304723 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,508] INFO [ProducerStateManager partition=financial_transactions-9] Wrote producer snapshot at offset 304868 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,513] INFO [ProducerStateManager partition=financial_transactions-1] Wrote producer snapshot at offset 304797 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,519] INFO [ProducerStateManager partition=financial_transactions-14] Wrote producer snapshot at offset 305196 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,531] INFO [ProducerStateManager partition=financial_transactions-11] Wrote producer snapshot at offset 305371 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,547] INFO [ProducerStateManager partition=_schemas-0] Wrote producer snapshot at offset 8 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,558] INFO [ProducerStateManager partition=financial_transactions-8] Wrote producer snapshot at offset 304771 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,568] INFO [ProducerStateManager partition=financial_transactions-2] Wrote producer snapshot at offset 305058 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,577] INFO [ProducerStateManager partition=financial_transactions-16] Wrote producer snapshot at offset 304634 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,627] INFO Shutdown complete. (kafka.log.LogManager)
[2025-05-21 14:45:33,628] INFO [broker-4-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,630] INFO [broker-4-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,630] INFO [broker-4-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,632] INFO [broker-4-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,632] INFO [broker-4-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,632] INFO [broker-4-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,633] INFO [broker-4-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,634] INFO [broker-4-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,634] INFO [broker-4-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,634] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,635] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,635] INFO [broker-4-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,636] INFO [SocketServer listenerType=BROKER, nodeId=4] Shutting down socket server (kafka.network.SocketServer)
[2025-05-21 14:45:33,646] INFO [SocketServer listenerType=BROKER, nodeId=4] Shutdown completed (kafka.network.SocketServer)
[2025-05-21 14:45:33,647] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
[2025-05-21 14:45:33,648] INFO [BrokerLifecycleManager id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,649] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 14:45:33,650] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 14:45:33,650] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 14:45:33,652] INFO [SharedServer id=4] Stopping SharedServer (kafka.server.SharedServer)
[2025-05-21 14:45:33,653] INFO [MetadataLoader id=4] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,653] INFO [SnapshotGenerator id=4] close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,655] INFO [SnapshotGenerator id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,656] INFO [MetadataLoader id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,657] INFO [SnapshotGenerator id=4] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,658] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 14:45:33,856] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 14:45:33,856] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 14:45:33,857] INFO [kafka-4-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 14:45:33,858] INFO [RaftManager id=4] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 14:45:33,858] INFO [RaftManager id=4] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 14:45:33,859] INFO [RaftManager id=4] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 14:45:33,859] INFO [kafka-4-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 14:45:33,859] INFO [kafka-4-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 14:45:33,863] INFO [kafka-4-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 14:45:33,864] INFO [kafka-4-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 14:45:33,864] INFO [kafka-4-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 14:45:33,868] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 16829 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,871] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-21 14:45:33,871] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2025-05-21 14:45:33,872] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-21 14:45:33,872] INFO App info kafka.server for 4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 14:45:33,873] INFO [BrokerServer id=4] shut down completed (kafka.server.BrokerServer)
[2025-05-21 14:45:33,873] INFO [BrokerServer id=4] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
[2025-05-21 14:45:33,874] INFO App info kafka.server for 4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-22 17:40:21,822] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-22 17:40:22,020] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 4
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 4
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-22 17:40:22,035] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-22 17:40:22,037] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
