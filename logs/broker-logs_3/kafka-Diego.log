[2025-05-20 22:21:35,219] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 22:21:35,683] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 5
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 5
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 22:21:35,709] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 22:21:35,728] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:41,392] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 22:21:41,515] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 22:21:41,519] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:41,742] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:41,814] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:41,859] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 22:21:41,866] INFO [BrokerServer id=5] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-20 22:21:41,869] INFO [SharedServer id=5] Starting SharedServer (kafka.server.SharedServer)
[2025-05-20 22:21:41,886] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:42,029] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:42,030] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:42,033] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 1ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:42,053] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-20 22:21:42,087] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:21:42,091] INFO [RaftManager id=5] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:21:42,102] INFO [RaftManager id=5] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:21:42,166] INFO [RaftManager id=5] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1414) from null (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:42,168] INFO [kafka-5-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:21:42,170] INFO [kafka-5-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:21:42,185] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,211] INFO [BrokerServer id=5] Starting broker (kafka.server.BrokerServer)
[2025-05-20 22:21:42,232] INFO [RaftManager id=5] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1134633876 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:21:42,241] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:42,289] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,341] INFO [broker-5-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:21:42,349] INFO [broker-5-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:21:42,359] INFO [broker-5-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:21:42,372] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:21:42,399] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,447] INFO [BrokerServer id=5] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 22:21:42,454] INFO [BrokerServer id=5] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 22:21:42,483] INFO [broker-5-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:42,515] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,527] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:21:42,527] INFO [RaftManager id=5] Completed transition to Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1414) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:42,560] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:21:42,567] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:21:42,615] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,625] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:21:42,625] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:21:42,728] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,835] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,866] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 22:21:42,894] INFO [SocketServer listenerType=BROKER, nodeId=5] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-20 22:21:42,895] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 22:21:42,906] INFO [SocketServer listenerType=BROKER, nodeId=5] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-20 22:21:42,918] INFO [broker-5-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:42,940] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,074] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:43,076] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,086] INFO [RaftManager id=5] Completed transition to Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:43,095] INFO [ExpirationReaper-5-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,098] INFO [ExpirationReaper-5-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,104] INFO [ExpirationReaper-5-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,108] INFO [ExpirationReaper-5-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,112] INFO [RaftManager id=5] Completed transition to Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:43,129] INFO [ExpirationReaper-5-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,159] INFO [ExpirationReaper-5-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,160] INFO [ExpirationReaper-5-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,182] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-05-20 22:21:43,182] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,199] INFO [broker-5-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:43,201] INFO [BrokerLifecycleManager id=5] Incarnation Qp9IVRc0RoazeG-vLYBhqg of broker 5 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:43,205] INFO [ExpirationReaper-5-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,255] INFO [BrokerServer id=5] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 22:21:43,256] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,272] INFO [BrokerServer id=5] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 22:21:43,279] INFO [BrokerServer id=5] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 22:21:43,380] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,485] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,587] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,688] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,789] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,837] INFO [RaftManager id=5] Completed transition to Unattached(epoch=12, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:43,890] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,991] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,092] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,129] INFO [RaftManager id=5] Completed transition to Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=12, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:44,184] INFO [RaftManager id=5] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=13, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:44,192] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,193] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,204] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,224] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:21:44,227] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,278] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,283] INFO [broker-5-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,290] INFO [broker-5-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,300] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,382] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:21:44,383] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,401] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,433] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,501] INFO [BrokerLifecycleManager id=5] Successfully registered broker 5 with broker epoch 1827 (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:44,502] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,516] INFO [RaftManager id=5] High watermark set to Optional[LogOffsetMetadata(offset=1825, metadata=Optional.empty)] for the first time for epoch 13 (org.apache.kafka.raft.FollowerState)
[2025-05-20 22:21:44,561] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 1825 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,703] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 1824, but the high water mark is 1829 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,719] INFO [MetadataLoader id=5] initializeNewPublishers: The loader is still catching up because we have loaded up to offset 1824, but the high water mark is 1829 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,735] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 1829 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,772] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 1828 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,776] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing MetadataVersionPublisher(id=5) with a snapshot at offset 1828 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,781] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 1828 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,784] INFO [BrokerLifecycleManager id=5] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:44,784] INFO [BrokerMetadataPublisher id=5] Publishing initial metadata at offset OffsetAndEpoch(offset=1828, epoch=13) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-20 22:21:44,790] INFO [BrokerServer id=5] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 22:21:44,817] INFO [BrokerServer id=5] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 22:21:44,828] INFO [BrokerLifecycleManager id=5] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:44,841] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:44,851] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-05-20 22:21:44,882] INFO Loaded 0 logs in 40ms (kafka.log.LogManager)
[2025-05-20 22:21:44,890] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-20 22:21:44,892] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-20 22:21:44,905] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-20 22:21:45,058] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:21:45,075] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:21:45,087] INFO [GroupCoordinator 5]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:45,085] INFO [AddPartitionsToTxnSenderThread-5]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:21:45,097] INFO [GroupCoordinator 5]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:45,099] INFO [TransactionCoordinator id=5] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:21:45,110] INFO [TransactionCoordinator id=5] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:21:45,111] INFO [TxnMarkerSenderThread-5]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:21:45,133] INFO [Broker id=5] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:21:45,138] INFO [Broker id=5] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,167] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,175] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,179] INFO [Partition __consumer_offsets-13 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-05-20 22:21:45,182] INFO [Partition __consumer_offsets-13 broker=5] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,185] INFO [Broker id=5] Follower __consumer_offsets-13 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,189] INFO [Broker id=5] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,196] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,199] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,208] INFO [Partition __consumer_offsets-46 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-05-20 22:21:45,209] INFO [Partition __consumer_offsets-46 broker=5] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,210] INFO [Broker id=5] Follower __consumer_offsets-46 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,210] INFO [Broker id=5] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,236] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,239] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,245] INFO [Partition __consumer_offsets-9 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-05-20 22:21:45,247] INFO [Partition __consumer_offsets-9 broker=5] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,248] INFO [Broker id=5] Follower __consumer_offsets-9 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,248] INFO [Broker id=5] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,264] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,279] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,283] INFO [Partition __consumer_offsets-42 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-05-20 22:21:45,288] INFO [Partition __consumer_offsets-42 broker=5] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,290] INFO [Broker id=5] Follower __consumer_offsets-42 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,292] INFO [Broker id=5] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,314] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,316] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,318] INFO [Partition __consumer_offsets-21 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-05-20 22:21:45,318] INFO [Partition __consumer_offsets-21 broker=5] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,319] INFO [Broker id=5] Follower __consumer_offsets-21 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,319] INFO [Broker id=5] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,334] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,336] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,338] INFO [Partition __consumer_offsets-17 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-05-20 22:21:45,345] INFO [Partition __consumer_offsets-17 broker=5] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,347] INFO [Broker id=5] Follower __consumer_offsets-17 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,348] INFO [Broker id=5] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,362] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,364] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,365] INFO [Partition __consumer_offsets-30 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-05-20 22:21:45,365] INFO [Partition __consumer_offsets-30 broker=5] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,366] INFO [Broker id=5] Follower __consumer_offsets-30 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,366] INFO [Broker id=5] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,385] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,388] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,389] INFO [Partition __consumer_offsets-26 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-05-20 22:21:45,400] INFO [Partition __consumer_offsets-26 broker=5] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,402] INFO [Broker id=5] Follower __consumer_offsets-26 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,402] INFO [Broker id=5] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,411] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,416] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,417] INFO [Partition __consumer_offsets-5 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-05-20 22:21:45,417] INFO [Partition __consumer_offsets-5 broker=5] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,418] INFO [Broker id=5] Follower __consumer_offsets-5 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,418] INFO [Broker id=5] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,425] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,428] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,431] INFO [Partition __consumer_offsets-38 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-05-20 22:21:45,432] INFO [Partition __consumer_offsets-38 broker=5] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,439] INFO [Broker id=5] Follower __consumer_offsets-38 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,445] INFO [Broker id=5] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,455] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,457] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,469] INFO [Partition __consumer_offsets-1 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-05-20 22:21:45,476] INFO [Partition __consumer_offsets-1 broker=5] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,479] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,481] INFO [Broker id=5] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,503] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,512] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,514] INFO [Partition __consumer_offsets-34 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-05-20 22:21:45,518] INFO [Partition __consumer_offsets-34 broker=5] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,519] INFO [Broker id=5] Follower __consumer_offsets-34 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,521] INFO [Broker id=5] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,542] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,554] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,560] INFO [Partition __consumer_offsets-16 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-05-20 22:21:45,580] INFO [Partition __consumer_offsets-16 broker=5] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,582] INFO [Broker id=5] Follower __consumer_offsets-16 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,585] INFO [Broker id=5] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-20 22:21:45,610] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,611] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-05-20 22:21:45,612] INFO [Partition _schemas-0 broker=5] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,612] INFO [Partition _schemas-0 broker=5] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,613] INFO [Broker id=5] Follower _schemas-0 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,613] INFO [Broker id=5] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,627] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,629] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,631] INFO [Partition __consumer_offsets-45 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-05-20 22:21:45,633] INFO [Partition __consumer_offsets-45 broker=5] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,633] INFO [Broker id=5] Follower __consumer_offsets-45 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,634] INFO [Broker id=5] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,662] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,664] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,665] INFO [Partition __consumer_offsets-12 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-05-20 22:21:45,669] INFO [Partition __consumer_offsets-12 broker=5] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,669] INFO [Broker id=5] Follower __consumer_offsets-12 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,670] INFO [Broker id=5] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,689] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,698] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,700] INFO [Partition __consumer_offsets-41 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-05-20 22:21:45,701] INFO [Partition __consumer_offsets-41 broker=5] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,707] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,708] INFO [Broker id=5] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,723] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,726] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,730] INFO [Partition __consumer_offsets-24 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-05-20 22:21:45,733] INFO [Partition __consumer_offsets-24 broker=5] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,746] INFO [Broker id=5] Follower __consumer_offsets-24 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,746] INFO [Broker id=5] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,761] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,766] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,772] INFO [Partition __consumer_offsets-20 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-05-20 22:21:45,780] INFO [Partition __consumer_offsets-20 broker=5] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,782] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,785] INFO [Broker id=5] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,798] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,799] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,799] INFO [Partition __consumer_offsets-49 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-05-20 22:21:45,800] INFO [Partition __consumer_offsets-49 broker=5] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,800] INFO [Broker id=5] Follower __consumer_offsets-49 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,801] INFO [Broker id=5] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,809] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,815] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,816] INFO [Partition __consumer_offsets-0 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,819] INFO [Partition __consumer_offsets-0 broker=5] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,820] INFO [Broker id=5] Follower __consumer_offsets-0 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,820] INFO [Broker id=5] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,854] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,856] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,857] INFO [Partition __consumer_offsets-29 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-05-20 22:21:45,857] INFO [Partition __consumer_offsets-29 broker=5] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,858] INFO [Broker id=5] Follower __consumer_offsets-29 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,858] INFO [Broker id=5] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,872] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,876] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,877] INFO [Partition __consumer_offsets-25 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-05-20 22:21:45,877] INFO [Partition __consumer_offsets-25 broker=5] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,879] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,880] INFO [Broker id=5] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,890] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,891] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,892] INFO [Partition __consumer_offsets-8 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-05-20 22:21:45,892] INFO [Partition __consumer_offsets-8 broker=5] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,893] INFO [Broker id=5] Follower __consumer_offsets-8 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,896] INFO [Broker id=5] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,908] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,909] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,911] INFO [Partition __consumer_offsets-37 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-05-20 22:21:45,917] INFO [Partition __consumer_offsets-37 broker=5] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,918] INFO [Broker id=5] Follower __consumer_offsets-37 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,918] INFO [Broker id=5] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,924] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,926] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,928] INFO [Partition __consumer_offsets-4 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-05-20 22:21:45,934] INFO [Partition __consumer_offsets-4 broker=5] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,938] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,938] INFO [Broker id=5] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,945] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,948] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,950] INFO [Partition __consumer_offsets-33 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-05-20 22:21:45,951] INFO [Partition __consumer_offsets-33 broker=5] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,957] INFO [Broker id=5] Follower __consumer_offsets-33 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,958] INFO [Broker id=5] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,964] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,969] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,981] INFO [Partition __consumer_offsets-15 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-05-20 22:21:45,984] INFO [Partition __consumer_offsets-15 broker=5] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,995] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,999] INFO [Broker id=5] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,010] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,018] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,025] INFO [Partition __consumer_offsets-48 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-05-20 22:21:46,032] INFO [Partition __consumer_offsets-48 broker=5] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,033] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,034] INFO [Broker id=5] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,049] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,054] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,054] INFO [Partition __consumer_offsets-11 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-05-20 22:21:46,056] INFO [Partition __consumer_offsets-11 broker=5] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,060] INFO [Broker id=5] Follower __consumer_offsets-11 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,064] INFO [Broker id=5] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,078] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,083] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,085] INFO [Partition __consumer_offsets-44 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-05-20 22:21:46,088] INFO [Partition __consumer_offsets-44 broker=5] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,090] INFO [Broker id=5] Follower __consumer_offsets-44 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,091] INFO [Broker id=5] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,106] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,111] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,113] INFO [Partition __consumer_offsets-23 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-05-20 22:21:46,114] INFO [Partition __consumer_offsets-23 broker=5] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,119] INFO [Broker id=5] Follower __consumer_offsets-23 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,119] INFO [Broker id=5] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,129] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,133] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,133] INFO [Partition __consumer_offsets-19 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-05-20 22:21:46,138] INFO [Partition __consumer_offsets-19 broker=5] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,140] INFO [Broker id=5] Follower __consumer_offsets-19 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,142] INFO [Broker id=5] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,147] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,150] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,155] INFO [Partition __consumer_offsets-32 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-05-20 22:21:46,160] INFO [Partition __consumer_offsets-32 broker=5] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,161] INFO [Broker id=5] Follower __consumer_offsets-32 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,161] INFO [Broker id=5] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,175] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,181] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,181] INFO [Partition __consumer_offsets-28 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-05-20 22:21:46,182] INFO [Partition __consumer_offsets-28 broker=5] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,184] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,187] INFO [Broker id=5] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,201] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,204] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,206] INFO [Partition __consumer_offsets-7 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-05-20 22:21:46,209] INFO [Partition __consumer_offsets-7 broker=5] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,210] INFO [Broker id=5] Follower __consumer_offsets-7 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,213] INFO [Broker id=5] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,225] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,230] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,231] INFO [Partition __consumer_offsets-40 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-05-20 22:21:46,233] INFO [Partition __consumer_offsets-40 broker=5] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,233] INFO [Broker id=5] Follower __consumer_offsets-40 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,234] INFO [Broker id=5] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,252] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,259] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,259] INFO [Partition __consumer_offsets-3 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-05-20 22:21:46,261] INFO [Partition __consumer_offsets-3 broker=5] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,262] INFO [Broker id=5] Follower __consumer_offsets-3 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,263] INFO [Broker id=5] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,273] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,276] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,276] INFO [Partition __consumer_offsets-36 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-05-20 22:21:46,277] INFO [Partition __consumer_offsets-36 broker=5] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,278] INFO [Broker id=5] Follower __consumer_offsets-36 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,278] INFO [Broker id=5] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,284] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,288] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,289] INFO [Partition __consumer_offsets-47 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-05-20 22:21:46,290] INFO [Partition __consumer_offsets-47 broker=5] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,291] INFO [Broker id=5] Follower __consumer_offsets-47 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,292] INFO [Broker id=5] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,302] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,303] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,304] INFO [Partition __consumer_offsets-14 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-05-20 22:21:46,304] INFO [Partition __consumer_offsets-14 broker=5] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,306] INFO [Broker id=5] Follower __consumer_offsets-14 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,307] INFO [Broker id=5] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,319] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,320] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,321] INFO [Partition __consumer_offsets-43 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-05-20 22:21:46,321] INFO [Partition __consumer_offsets-43 broker=5] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,322] INFO [Broker id=5] Follower __consumer_offsets-43 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,324] INFO [Broker id=5] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,333] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,335] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,335] INFO [Partition __consumer_offsets-10 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-05-20 22:21:46,335] INFO [Partition __consumer_offsets-10 broker=5] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,336] INFO [Broker id=5] Follower __consumer_offsets-10 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,337] INFO [Broker id=5] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,345] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,347] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,347] INFO [Partition __consumer_offsets-22 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-05-20 22:21:46,348] INFO [Partition __consumer_offsets-22 broker=5] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,348] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,349] INFO [Broker id=5] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,354] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,356] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,356] INFO [Partition __consumer_offsets-18 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-05-20 22:21:46,356] INFO [Partition __consumer_offsets-18 broker=5] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,357] INFO [Broker id=5] Follower __consumer_offsets-18 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,357] INFO [Broker id=5] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,384] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,385] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,386] INFO [Partition __consumer_offsets-31 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-05-20 22:21:46,386] INFO [Partition __consumer_offsets-31 broker=5] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,387] INFO [Broker id=5] Follower __consumer_offsets-31 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,387] INFO [Broker id=5] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,395] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,397] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,397] INFO [Partition __consumer_offsets-27 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-05-20 22:21:46,397] INFO [Partition __consumer_offsets-27 broker=5] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,398] INFO [Broker id=5] Follower __consumer_offsets-27 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,400] INFO [Broker id=5] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,409] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,410] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,411] INFO [Partition __consumer_offsets-39 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-05-20 22:21:46,411] INFO [Partition __consumer_offsets-39 broker=5] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,411] INFO [Broker id=5] Follower __consumer_offsets-39 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,412] INFO [Broker id=5] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,417] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,418] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,419] INFO [Partition __consumer_offsets-6 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-05-20 22:21:46,419] INFO [Partition __consumer_offsets-6 broker=5] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,420] INFO [Broker id=5] Follower __consumer_offsets-6 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,421] INFO [Broker id=5] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,429] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,431] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,431] INFO [Partition __consumer_offsets-35 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-05-20 22:21:46,432] INFO [Partition __consumer_offsets-35 broker=5] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,433] INFO [Broker id=5] Follower __consumer_offsets-35 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,433] INFO [Broker id=5] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,439] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,441] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,441] INFO [Partition __consumer_offsets-2 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-05-20 22:21:46,443] INFO [Partition __consumer_offsets-2 broker=5] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,444] INFO [Broker id=5] Follower __consumer_offsets-2 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,447] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:21:46,450] INFO [Broker id=5] Stopped fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 22:21:46,468] INFO [Broker id=5] Started fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 22:21:46,531] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,536] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,547] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,550] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,551] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,551] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,551] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,552] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,552] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,553] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,554] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,556] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,558] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,558] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,561] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,562] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,563] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,564] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,566] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,566] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,567] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,567] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,567] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,568] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,568] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,568] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,569] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,570] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,571] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,571] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,571] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,572] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,573] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,575] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,576] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,577] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,578] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,584] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,585] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,585] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,586] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,586] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,586] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,587] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,587] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,587] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,574] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,588] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,588] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,589] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,589] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,589] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,590] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,590] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,591] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,591] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,593] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,599] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,605] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,588] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,606] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,607] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,607] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,608] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,608] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,608] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,606] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,612] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,613] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,615] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,616] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,618] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,619] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,619] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,620] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,620] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,621] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,621] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,621] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,622] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,622] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,613] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,626] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,627] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,627] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,627] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,628] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,628] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,625] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,629] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,630] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,632] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,629] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,633] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,633] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,634] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,635] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,635] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,635] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,636] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,636] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,636] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,637] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,637] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,637] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,637] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,638] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,634] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,639] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,639] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,639] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,640] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,640] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,640] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,639] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,641] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,641] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,641] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,643] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,643] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,643] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,643] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,644] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,644] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,644] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,645] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,645] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,641] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,647] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,647] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,648] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,648] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,648] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,646] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,663] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,665] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,665] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,666] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,666] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,666] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,667] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,664] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,668] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,668] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,668] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,667] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,669] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,669] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,669] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,672] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,688] INFO [DynamicConfigPublisher broker id=5] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 22:21:46,701] INFO [DynamicConfigPublisher broker id=5] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 22:21:46,706] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=5) with a snapshot at offset 1828 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:46,706] INFO [BrokerServer id=5] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 22:21:46,710] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 5
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 5
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 22:21:46,716] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:46,724] INFO [BrokerServer id=5] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 22:21:46,803] INFO [BrokerLifecycleManager id=5] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:46,806] INFO [BrokerServer id=5] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 22:21:46,809] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 22:21:46,816] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 22:21:46,820] INFO [SocketServer listenerType=BROKER, nodeId=5] Enabling request processing. (kafka.network.SocketServer)
[2025-05-20 22:21:46,824] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 22:21:46,830] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 22:21:46,846] INFO [BrokerServer id=5] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 22:21:46,847] INFO [BrokerServer id=5] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 22:21:46,849] INFO [BrokerServer id=5] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 22:21:46,850] INFO [BrokerServer id=5] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 22:21:46,850] INFO [BrokerServer id=5] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-20 22:21:46,851] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:21:46,851] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:21:46,852] INFO Kafka startTimeMs: 1747779706851 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:21:46,854] INFO [KafkaRaftServer nodeId=5] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-20 22:21:47,470] INFO [Broker id=5] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:21:47,475] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,476] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,476] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,480] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,481] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,482] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,483] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,485] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,486] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,487] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,489] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,492] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,499] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,502] INFO [Broker id=5] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,503] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,505] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,506] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,508] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,509] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,510] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,511] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,513] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,513] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,515] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,516] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,517] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,518] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,521] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,524] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,526] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,528] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,530] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,531] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,531] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,532] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,533] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,534] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,535] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,535] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,536] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,537] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,538] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,538] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,539] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,539] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,540] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,541] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,543] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,546] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,565] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,567] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,570] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,571] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,572] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,572] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,573] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,573] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,573] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,573] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,576] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,577] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,579] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,580] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,580] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,580] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,580] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,582] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,582] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,583] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,584] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,586] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,586] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,586] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,586] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,587] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,587] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,587] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,587] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,587] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,588] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,588] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,588] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,575] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,589] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,590] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,590] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,591] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,592] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,594] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,595] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,591] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,597] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,598] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,598] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,599] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,599] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,600] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,599] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,600] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,602] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,602] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,602] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,603] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,603] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,606] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,606] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,607] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,607] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,608] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,603] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,613] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,614] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,615] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,617] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,621] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,621] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,615] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,622] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,623] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,622] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,624] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,624] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,624] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,625] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,623] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,626] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,625] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,627] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,628] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,628] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,629] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,628] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,632] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,634] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,634] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,635] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,635] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,635] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,635] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,636] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,636] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,637] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,638] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,640] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,642] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,643] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,643] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,646] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,647] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,649] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,649] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,629] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,650] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,655] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,655] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,655] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,655] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,655] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,656] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,656] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,656] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,657] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,656] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,657] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,658] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,666] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,668] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,669] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,669] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,670] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,670] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,670] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,671] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,672] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,672] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,665] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,676] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,682] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,682] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,684] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,684] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,685] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,686] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,687] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,688] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,689] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,677] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,690] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,690] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,690] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,691] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,691] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,691] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,691] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,692] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,692] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,692] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,693] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,693] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,693] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,694] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:24:30,905] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 22:24:30,910] INFO [BrokerServer id=5] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
[2025-05-20 22:24:30,911] INFO [BrokerServer id=5] shutting down (kafka.server.BrokerServer)
[2025-05-20 22:24:30,912] INFO [BrokerLifecycleManager id=5] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:30,936] INFO [BrokerLifecycleManager id=5] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:30,936] INFO [BrokerLifecycleManager id=5] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:24:30,937] INFO [BrokerLifecycleManager id=5] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:30,938] INFO [broker-5-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:30,938] INFO [broker-5-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:30,938] INFO [broker-5-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:30,939] INFO [SocketServer listenerType=BROKER, nodeId=5] Stopping socket server request processors (kafka.network.SocketServer)
[2025-05-20 22:24:30,944] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:24:30,950] INFO [SocketServer listenerType=BROKER, nodeId=5] Stopped socket server request processors (kafka.network.SocketServer)
[2025-05-20 22:24:30,952] INFO [data-plane Kafka Request Handler on Broker 5], shutting down (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 22:24:30,954] INFO [data-plane Kafka Request Handler on Broker 5], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 22:24:30,955] INFO [ExpirationReaper-5-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:30,957] INFO [ExpirationReaper-5-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:30,957] INFO [ExpirationReaper-5-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:30,959] INFO [KafkaApi-5] Shutdown complete. (kafka.server.KafkaApis)
[2025-05-20 22:24:30,963] INFO [TransactionCoordinator id=5] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:24:30,964] INFO [Transaction State Manager 5]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2025-05-20 22:24:30,964] INFO [TxnMarkerSenderThread-5]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:24:30,965] INFO [TxnMarkerSenderThread-5]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:24:30,965] INFO [TxnMarkerSenderThread-5]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:24:30,967] INFO [TransactionCoordinator id=5] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:24:30,969] INFO [GroupCoordinator 5]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:24:30,970] INFO [ExpirationReaper-5-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:30,971] INFO [ExpirationReaper-5-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:30,971] INFO [ExpirationReaper-5-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:30,972] INFO [ExpirationReaper-5-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:30,974] INFO [ExpirationReaper-5-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:30,974] INFO [ExpirationReaper-5-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:30,975] INFO [GroupCoordinator 5]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:24:30,986] INFO [AssignmentsManager id=5]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:24:30,988] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:30,993] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:30,993] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:30,995] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:24:30,995] INFO [AssignmentsManager id=5]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:24:30,996] INFO [ReplicaManager broker=5] Shutting down (kafka.server.ReplicaManager)
[2025-05-20 22:24:30,997] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:24:30,997] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:24:30,997] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:24:30,999] INFO [ReplicaFetcherManager on broker 5] shutting down (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:24:31,001] INFO [ReplicaFetcherManager on broker 5] shutdown completed (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:24:31,002] INFO [ReplicaAlterLogDirsManager on broker 5] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:24:31,004] INFO [ReplicaAlterLogDirsManager on broker 5] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:24:31,005] INFO [ExpirationReaper-5-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,007] INFO [ExpirationReaper-5-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,007] INFO [ExpirationReaper-5-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,008] INFO [ExpirationReaper-5-RemoteFetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,011] INFO [ExpirationReaper-5-RemoteFetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,012] INFO [ExpirationReaper-5-RemoteFetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,016] INFO [ExpirationReaper-5-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,017] INFO [ExpirationReaper-5-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,017] INFO [ExpirationReaper-5-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,018] INFO [ExpirationReaper-5-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,019] INFO [ExpirationReaper-5-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,019] INFO [ExpirationReaper-5-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,021] INFO [ExpirationReaper-5-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,022] INFO [ExpirationReaper-5-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,022] INFO [ExpirationReaper-5-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:31,030] INFO [AddPartitionsToTxnSenderThread-5]: Shutting down (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:24:31,031] INFO [AddPartitionsToTxnSenderThread-5]: Stopped (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:24:31,031] INFO [AddPartitionsToTxnSenderThread-5]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:24:31,032] INFO [ReplicaManager broker=5] Shut down completely (kafka.server.ReplicaManager)
[2025-05-20 22:24:31,032] INFO [broker-5-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:31,033] INFO [broker-5-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:31,033] INFO [broker-5-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:31,034] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:24:31,035] INFO [broker-5-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:31,035] INFO [broker-5-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:31,036] INFO [broker-5-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:31,036] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:24:31,037] INFO Shutting down. (kafka.log.LogManager)
[2025-05-20 22:24:31,039] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
[2025-05-20 22:24:31,039] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:24:31,040] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:24:31,041] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:24:31,229] INFO Shutdown complete. (kafka.log.LogManager)
[2025-05-20 22:24:31,230] INFO [broker-5-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,231] INFO [broker-5-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,231] INFO [broker-5-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,231] INFO [broker-5-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,232] INFO [broker-5-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,232] INFO [broker-5-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,232] INFO [broker-5-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,232] INFO [broker-5-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,232] INFO [broker-5-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,233] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,233] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,233] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:31,235] INFO [SocketServer listenerType=BROKER, nodeId=5] Shutting down socket server (kafka.network.SocketServer)
[2025-05-20 22:24:31,245] INFO [SocketServer listenerType=BROKER, nodeId=5] Shutdown completed (kafka.network.SocketServer)
[2025-05-20 22:24:31,247] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
[2025-05-20 22:24:31,247] INFO [BrokerLifecycleManager id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:24:31,248] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:24:31,249] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:24:31,249] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:24:31,251] INFO [SharedServer id=5] Stopping SharedServer (kafka.server.SharedServer)
[2025-05-20 22:24:31,251] INFO [MetadataLoader id=5] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:24:31,252] INFO [SnapshotGenerator id=5] close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:24:31,252] INFO [SnapshotGenerator id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:24:31,252] INFO [MetadataLoader id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:24:31,255] INFO [SnapshotGenerator id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:24:31,256] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:24:31,299] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:24:31,299] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:24:31,300] INFO [kafka-5-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:24:31,300] INFO [RaftManager id=5] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:24:31,301] INFO [RaftManager id=5] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:24:31,301] INFO [RaftManager id=5] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:24:31,301] INFO [kafka-5-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:24:31,301] INFO [kafka-5-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:24:31,306] INFO [kafka-5-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:24:31,307] INFO [kafka-5-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:24:31,307] INFO [kafka-5-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:24:31,312] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2219 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:24:31,315] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:24:31,316] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:24:31,316] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:24:31,317] INFO App info kafka.server for 5 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:24:31,317] INFO [BrokerServer id=5] shut down completed (kafka.server.BrokerServer)
[2025-05-20 22:24:31,318] INFO [BrokerServer id=5] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
[2025-05-20 22:24:31,318] INFO App info kafka.server for 5 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:24:50,446] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 22:24:50,935] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 22:24:50,990] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 22:24:51,005] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:24:56,596] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 22:24:56,783] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 22:24:56,789] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:24:57,031] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:24:57,045] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:24:57,075] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 22:24:57,081] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-20 22:24:57,084] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2025-05-20 22:24:57,093] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:24:57,202] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:57,205] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:57,208] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:57,227] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-20 22:24:57,272] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:24:57,274] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:24:57,277] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:24:57,323] INFO [RaftManager id=6] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1675) from null (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:24:57,325] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:24:57,337] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:24:57,387] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:57,399] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2025-05-20 22:24:57,409] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:24:57,411] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@198480695 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:24:57,495] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:57,521] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:57,551] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:57,561] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:57,575] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:57,603] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:57,666] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=14, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1675) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:24:57,671] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:24:57,674] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:24:57,704] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:57,710] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 22:24:57,715] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 22:24:57,749] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:24:57,751] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:57,753] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:57,752] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:24:57,783] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:24:57,812] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:57,878] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:24:57,878] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:24:57,921] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:57,924] INFO [RaftManager id=6] Completed transition to Unattached(epoch=15, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=14, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:24:58,035] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,151] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:24:58,154] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:24:58,159] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,260] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,321] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 22:24:58,362] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,385] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-20 22:24:58,405] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 22:24:58,417] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-20 22:24:58,435] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:58,532] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,532] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:58,560] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:58,603] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=15, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=15, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:24:58,603] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:58,629] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:58,636] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:58,638] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:58,636] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,649] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:58,661] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:58,662] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:58,668] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:58,683] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-05-20 22:24:58,729] INFO [BrokerLifecycleManager id=6] Incarnation 10m8N77-TP22pD1Mw_VqOw of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:58,730] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:58,733] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:58,745] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,768] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:58,846] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,863] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 22:24:58,864] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,864] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 22:24:58,865] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 22:24:58,947] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:24:58,949] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:58,965] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,999] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:59,067] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,180] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,189] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 2224 (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:59,218] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=2224, metadata=Optional.empty)] for the first time for epoch 15 (org.apache.kafka.raft.FollowerState)
[2025-05-20 22:24:59,280] INFO [MetadataLoader id=6] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 2224 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,290] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 2224 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,578] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 2223, but the high water mark is 2228 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,603] INFO [MetadataLoader id=6] initializeNewPublishers: The loader is still catching up because we have loaded up to offset 2223, but the high water mark is 2228 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,625] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 2228 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,641] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 2227 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,642] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 2227 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,643] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 2227 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,644] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=2227, epoch=15) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-20 22:24:59,649] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:24:59,668] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-05-20 22:24:59,677] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:59,679] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 22:24:59,690] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 22:24:59,702] INFO [BrokerLifecycleManager id=6] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:59,736] INFO Loaded 0 logs in 87ms (kafka.log.LogManager)
[2025-05-20 22:24:59,747] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-20 22:24:59,752] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-20 22:24:59,780] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-20 22:25:00,135] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:25:00,205] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:25:00,217] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:25:00,221] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:00,241] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:00,253] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:25:00,304] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:25:00,306] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:25:00,314] INFO [Broker id=6] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:00,318] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:00,404] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,416] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:00,432] INFO [Partition __consumer_offsets-13 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-05-20 22:25:00,447] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:00,464] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:00,471] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:00,483] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,492] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:00,505] INFO [Partition __consumer_offsets-46 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-05-20 22:25:00,508] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:00,523] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:00,525] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:00,542] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,546] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:00,547] INFO [Partition __consumer_offsets-9 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-05-20 22:25:00,551] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:00,554] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:00,555] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:00,562] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,564] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:00,565] INFO [Partition __consumer_offsets-42 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-05-20 22:25:00,565] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:00,568] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:00,568] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:00,574] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,576] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:00,576] INFO [Partition __consumer_offsets-21 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-05-20 22:25:00,577] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:00,577] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:00,578] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:00,590] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,591] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:00,593] INFO [Partition __consumer_offsets-17 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-05-20 22:25:00,604] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:00,686] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:00,692] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:00,708] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,711] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:00,712] INFO [Partition __consumer_offsets-30 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-05-20 22:25:00,713] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:00,725] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:00,726] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:00,742] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,759] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:00,761] INFO [Partition __consumer_offsets-26 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-05-20 22:25:00,762] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:00,762] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:00,764] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:00,779] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,793] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:00,794] INFO [Partition __consumer_offsets-5 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-05-20 22:25:00,795] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:00,795] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:00,796] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:00,811] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,816] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:00,819] INFO [Partition __consumer_offsets-38 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-05-20 22:25:00,826] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:00,827] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:00,829] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:00,838] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,855] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:00,866] INFO [Partition __consumer_offsets-1 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-05-20 22:25:00,873] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:00,873] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:00,874] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:00,885] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,889] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:00,890] INFO [Partition __consumer_offsets-34 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-05-20 22:25:00,891] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:00,897] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:00,899] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:00,910] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,911] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:00,912] INFO [Partition __consumer_offsets-16 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-05-20 22:25:00,912] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:00,913] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:00,913] INFO [Broker id=6] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-20 22:25:00,923] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,925] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-05-20 22:25:00,926] INFO [Partition _schemas-0 broker=6] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-05-20 22:25:00,926] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:00,928] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:00,929] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:00,939] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,946] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:00,946] INFO [Partition __consumer_offsets-45 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-05-20 22:25:00,951] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:00,962] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:00,965] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:00,970] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,973] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:00,973] INFO [Partition __consumer_offsets-12 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-05-20 22:25:00,975] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:00,977] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:00,980] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:00,988] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,990] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:00,998] INFO [Partition __consumer_offsets-41 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-05-20 22:25:01,000] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,002] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,006] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,018] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,019] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,020] INFO [Partition __consumer_offsets-24 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-05-20 22:25:01,021] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,022] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,022] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,028] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,031] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,033] INFO [Partition __consumer_offsets-20 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-05-20 22:25:01,033] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,034] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,034] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,039] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,040] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,041] INFO [Partition __consumer_offsets-49 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-05-20 22:25:01,041] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,042] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,043] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,053] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,054] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,055] INFO [Partition __consumer_offsets-0 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,056] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,057] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,057] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,063] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,067] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,068] INFO [Partition __consumer_offsets-29 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-05-20 22:25:01,070] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,072] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,074] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,089] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,092] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,093] INFO [Partition __consumer_offsets-25 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-05-20 22:25:01,095] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,095] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,096] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,101] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,107] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,109] INFO [Partition __consumer_offsets-8 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-05-20 22:25:01,110] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,111] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,111] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,116] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,119] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,121] INFO [Partition __consumer_offsets-37 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-05-20 22:25:01,122] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,123] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,124] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,131] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,133] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,134] INFO [Partition __consumer_offsets-4 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-05-20 22:25:01,134] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,137] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,138] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,146] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,147] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,148] INFO [Partition __consumer_offsets-33 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-05-20 22:25:01,148] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,149] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,149] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,157] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,161] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,163] INFO [Partition __consumer_offsets-15 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-05-20 22:25:01,164] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,165] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,165] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,171] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,173] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,174] INFO [Partition __consumer_offsets-48 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-05-20 22:25:01,174] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,175] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,175] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,181] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,185] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,186] INFO [Partition __consumer_offsets-11 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-05-20 22:25:01,186] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,187] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,188] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,195] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,198] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,199] INFO [Partition __consumer_offsets-44 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-05-20 22:25:01,200] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,202] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,203] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,209] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,212] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,215] INFO [Partition __consumer_offsets-23 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-05-20 22:25:01,221] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,226] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,227] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,237] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,239] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,240] INFO [Partition __consumer_offsets-19 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-05-20 22:25:01,241] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,241] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,242] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,280] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,282] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,282] INFO [Partition __consumer_offsets-32 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-05-20 22:25:01,283] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,283] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,284] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,294] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,296] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,296] INFO [Partition __consumer_offsets-28 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-05-20 22:25:01,296] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,297] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,297] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,303] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,305] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,306] INFO [Partition __consumer_offsets-7 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-05-20 22:25:01,306] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,307] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,307] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,312] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,314] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,314] INFO [Partition __consumer_offsets-40 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-05-20 22:25:01,315] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,315] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,316] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,325] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,327] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,328] INFO [Partition __consumer_offsets-3 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-05-20 22:25:01,328] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,329] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,329] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,337] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,339] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,341] INFO [Partition __consumer_offsets-36 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-05-20 22:25:01,342] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,343] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,345] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,350] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,353] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,354] INFO [Partition __consumer_offsets-47 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-05-20 22:25:01,356] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,361] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,367] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,373] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,374] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,375] INFO [Partition __consumer_offsets-14 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-05-20 22:25:01,376] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,376] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,377] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,383] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,385] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,386] INFO [Partition __consumer_offsets-43 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-05-20 22:25:01,386] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,387] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,387] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,394] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,396] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,397] INFO [Partition __consumer_offsets-10 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-05-20 22:25:01,397] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,397] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,398] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,406] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,410] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,411] INFO [Partition __consumer_offsets-22 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-05-20 22:25:01,411] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,412] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,412] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,418] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,420] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,421] INFO [Partition __consumer_offsets-18 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-05-20 22:25:01,422] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,422] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,422] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,427] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,429] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,429] INFO [Partition __consumer_offsets-31 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-05-20 22:25:01,429] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,430] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,431] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,443] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,445] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,447] INFO [Partition __consumer_offsets-27 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-05-20 22:25:01,447] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,448] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,448] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,454] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,455] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,456] INFO [Partition __consumer_offsets-39 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-05-20 22:25:01,456] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,457] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,457] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,463] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,464] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,465] INFO [Partition __consumer_offsets-6 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-05-20 22:25:01,466] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,466] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,467] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,471] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,473] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,473] INFO [Partition __consumer_offsets-35 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-05-20 22:25:01,474] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,474] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,475] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,483] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,485] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:25:01,485] INFO [Partition __consumer_offsets-2 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-05-20 22:25:01,486] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,486] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,488] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:01,490] INFO [Broker id=6] Stopped fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 22:25:01,499] INFO [Broker id=6] Started fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 22:25:01,520] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,521] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,522] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,523] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,529] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,529] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,529] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,536] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,536] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,540] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,542] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,546] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,543] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,555] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,555] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,555] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,556] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,556] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,557] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,557] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,558] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,558] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,559] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,559] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,569] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,579] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,580] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,584] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,584] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,584] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,588] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,590] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,590] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,595] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,602] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,603] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,603] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,604] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,604] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,605] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,608] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,608] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,610] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,611] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,611] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,613] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,621] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,621] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,622] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,622] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,622] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,623] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,630] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,630] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,631] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,634] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,634] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,635] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,637] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,637] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,637] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,638] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,638] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,639] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,641] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,641] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,642] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,645] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,645] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,645] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,646] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,647] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,648] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,649] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,649] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,649] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,652] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,652] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,653] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,654] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,654] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,654] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,656] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,656] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,664] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,673] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,673] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,674] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,675] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,675] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,676] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,679] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,680] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,682] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,686] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,686] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,687] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,688] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,688] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,689] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,691] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,691] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,694] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,699] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,699] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,700] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,701] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,702] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,709] INFO [BrokerLifecycleManager id=6] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:25:01,706] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,714] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,715] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,716] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,721] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,721] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,722] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,722] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,723] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,723] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,723] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,724] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,724] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,721] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,725] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,725] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,726] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,735] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,727] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,741] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,743] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,744] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,748] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,748] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,749] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,751] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,753] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,755] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,768] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,773] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,778] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,780] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,781] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,781] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,782] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,782] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,783] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,784] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,784] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,784] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,785] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,790] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 22:25:01,802] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 22:25:01,807] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 2227 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:25:01,809] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 22:25:01,812] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 22:25:01,817] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:25:01,822] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 22:25:01,894] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:25:01,895] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 22:25:01,898] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 22:25:01,899] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 22:25:01,901] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2025-05-20 22:25:01,910] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 22:25:01,914] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 22:25:01,918] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 22:25:01,918] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 22:25:01,919] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 22:25:01,920] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 22:25:01,920] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-20 22:25:01,921] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:25:01,921] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:25:01,922] INFO Kafka startTimeMs: 1747779901921 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:25:01,923] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-20 22:25:02,469] INFO [Broker id=6] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:02,473] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,474] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,474] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,475] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,475] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,476] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,478] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,479] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,479] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,480] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,483] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,483] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,484] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,486] INFO [Broker id=6] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,487] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,488] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,488] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,489] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,490] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,492] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,492] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,493] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,493] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,494] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,495] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,497] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,499] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,500] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,501] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,501] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,501] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,502] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,503] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,504] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,507] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,507] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,511] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,516] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,518] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,520] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,520] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,521] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,522] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,524] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,525] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,526] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,527] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,528] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,528] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,529] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,530] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,531] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,531] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,532] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,532] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,533] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,532] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,533] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,534] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,534] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,534] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,534] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,535] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,535] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,536] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,538] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,538] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,539] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,540] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,540] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,542] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,544] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,544] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,545] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,545] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,545] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,546] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,547] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,548] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,548] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,548] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,547] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,549] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,550] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,550] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,550] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,551] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,551] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,552] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,553] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,555] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,555] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,556] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,556] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,558] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,558] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,559] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,562] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,560] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,563] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,563] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,564] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,565] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,566] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,564] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,571] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,572] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,571] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,574] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,579] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,577] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,583] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,580] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,588] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,589] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,593] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,593] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,593] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,596] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,597] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,598] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,598] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,598] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,600] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,601] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,602] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,603] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,605] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,597] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,609] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,608] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,610] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,610] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,611] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,613] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,614] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,612] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,616] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,615] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,617] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,618] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,617] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,619] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,619] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,619] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,620] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,620] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,623] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,622] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,625] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,624] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,626] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,626] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,625] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,629] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,629] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,631] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,634] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,635] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,640] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,642] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,634] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,644] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,645] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,643] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,647] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,647] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,648] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,649] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,655] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,656] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,657] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,655] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,659] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,660] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,660] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,661] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,661] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,662] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,661] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,664] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,666] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,666] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,667] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,667] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,667] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,668] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,669] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,670] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,672] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,674] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,675] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,675] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,676] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,677] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,676] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,677] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,678] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,677] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,680] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,681] INFO [Broker id=6] Transitioning 35 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:25:02,683] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, _schemas-0, __consumer_offsets-14, __consumer_offsets-41, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:02,692] INFO [Broker id=6] Leader __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:02,702] INFO [Broker id=6] Leader __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:02,708] INFO [Broker id=6] Leader __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 6, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,716] INFO [Broker id=6] Leader __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 6, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,724] INFO [Broker id=6] Leader __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 6, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,730] INFO [Broker id=6] Leader __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 6, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,736] INFO [Broker id=6] Leader __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:02,743] INFO [Broker id=6] Leader __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:02,747] INFO [Broker id=6] Leader __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 6, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,752] INFO [Broker id=6] Leader __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 6, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,757] INFO [Broker id=6] Leader __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:02,761] INFO [Broker id=6] Leader __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 6, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,767] INFO [Broker id=6] Leader __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 6, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,772] INFO [Broker id=6] Leader __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 6, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,777] INFO [Broker id=6] Leader __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:02,782] INFO [Broker id=6] Leader __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:02,787] INFO [Broker id=6] Leader __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 6, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,791] INFO [Broker id=6] Leader __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:02,795] INFO [Broker id=6] Leader __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 6, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,800] INFO [Broker id=6] Leader _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) starts at leader epoch 3 from offset 0 with partition epoch 6, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,806] INFO [Broker id=6] Leader __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:02,811] INFO [Broker id=6] Leader __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:02,816] INFO [Broker id=6] Leader __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:02,822] INFO [Broker id=6] Leader __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:02,827] INFO [Broker id=6] Leader __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 6, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,837] INFO [Broker id=6] Leader __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 6, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,842] INFO [Broker id=6] Leader __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:02,848] INFO [Broker id=6] Leader __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 6, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,850] INFO [Partition __consumer_offsets-15 broker=6] ISR updated to 6,4  and version updated to 6 (kafka.cluster.Partition)
[2025-05-20 22:25:02,851] INFO [Partition __consumer_offsets-48 broker=6] ISR updated to 6,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-20 22:25:02,852] INFO [Broker id=6] Leader __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:02,856] INFO [Broker id=6] Leader __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 6, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,859] INFO [Broker id=6] Leader __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:02,863] INFO [Broker id=6] Leader __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 6, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,867] INFO [Broker id=6] Leader __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:02,871] INFO [Broker id=6] Leader __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 5, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:02,874] INFO [Broker id=6] Leader __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 6, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,878] INFO [Broker id=6] Transitioning 16 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:02,878] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,878] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,879] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,879] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,879] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,879] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,880] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,880] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,880] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,881] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,881] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,881] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,881] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,882] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,882] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,882] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,884] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 15 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,885] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,887] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 48 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,887] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,888] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 13 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,888] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,889] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 46 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,889] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,889] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 11 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,890] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,890] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 44 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,890] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,892] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 9 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,893] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,893] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 42 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,893] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,893] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 23 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,894] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,894] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 30 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,894] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,894] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 28 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,895] INFO [Partition __consumer_offsets-47 broker=6] ISR updated to 6,4  and version updated to 6 (kafka.cluster.Partition)
[2025-05-20 22:25:02,895] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-15 in 8 milliseconds for epoch 2, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,895] INFO [Partition __consumer_offsets-13 broker=6] ISR updated to 6,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:02,895] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,896] INFO [Partition __consumer_offsets-46 broker=6] ISR updated to 6,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:02,896] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-48 in 8 milliseconds for epoch 2, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,896] INFO [Partition __consumer_offsets-11 broker=6] ISR updated to 6,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:02,896] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 26 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,897] INFO [Partition __consumer_offsets-44 broker=6] ISR updated to 6,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:02,897] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-13 in 8 milliseconds for epoch 3, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,897] INFO [Partition __consumer_offsets-9 broker=6] ISR updated to 6,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-20 22:25:02,897] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,898] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 7 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,898] INFO [Partition __consumer_offsets-42 broker=6] ISR updated to 6,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-20 22:25:02,897] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-46 in 8 milliseconds for epoch 3, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,899] INFO [Partition __consumer_offsets-23 broker=6] ISR updated to 6,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:02,898] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,899] INFO [Partition __consumer_offsets-30 broker=6] ISR updated to 6,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:02,899] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-11 in 9 milliseconds for epoch 3, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,899] INFO [Partition __consumer_offsets-28 broker=6] ISR updated to 6,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-20 22:25:02,899] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 5 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,900] INFO [Partition __consumer_offsets-26 broker=6] ISR updated to 6,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:02,900] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-44 in 8 milliseconds for epoch 3, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,901] INFO [Partition __consumer_offsets-7 broker=6] ISR updated to 6,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:02,901] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-9 in 8 milliseconds for epoch 2, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,900] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,901] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-42 in 8 milliseconds for epoch 2, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,901] INFO [Partition __consumer_offsets-5 broker=6] ISR updated to 6,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:02,902] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-23 in 8 milliseconds for epoch 3, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,901] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 38 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,902] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-30 in 8 milliseconds for epoch 3, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,902] INFO [Partition __consumer_offsets-38 broker=6] ISR updated to 6,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-20 22:25:02,903] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-28 in 7 milliseconds for epoch 2, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,902] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,903] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-26 in 5 milliseconds for epoch 3, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,903] INFO [Partition __consumer_offsets-1 broker=6] ISR updated to 6,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-20 22:25:02,904] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-7 in 5 milliseconds for epoch 3, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,903] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 1 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,904] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-5 in 3 milliseconds for epoch 3, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,904] INFO [Partition __consumer_offsets-34 broker=6] ISR updated to 6,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:02,905] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-38 in 2 milliseconds for epoch 2, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,904] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,906] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 34 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,906] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,906] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds for epoch 2, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,906] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 47 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,907] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-34 in 1 milliseconds for epoch 3, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,907] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,907] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 16 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,908] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,908] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-47 in 1 milliseconds for epoch 2, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,908] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 14 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,908] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,908] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds for epoch 3, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,909] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 41 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,909] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds for epoch 2, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,909] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,910] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 22 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,910] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-41 in 1 milliseconds for epoch 2, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,910] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,910] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 20 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,910] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,910] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-22 in 0 milliseconds for epoch 2, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,911] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 49 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,911] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,911] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds for epoch 2, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,911] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 18 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,912] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,912] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-49 in 1 milliseconds for epoch 3, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,912] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 31 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,913] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,912] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds for epoch 3, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,913] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 29 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,913] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds for epoch 2, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,913] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,914] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 25 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,914] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,914] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds for epoch 3, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,914] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 39 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,915] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-25 in 1 milliseconds for epoch 2, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,915] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,915] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 8 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,916] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,916] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-39 in 1 milliseconds for epoch 3, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,916] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 37 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,916] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,916] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds for epoch 2, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,917] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 35 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,917] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,917] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds for epoch 3, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,917] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 4 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,918] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-35 in 1 milliseconds for epoch 2, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,918] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,918] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 2 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,918] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,918] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds for epoch 2, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,919] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,919] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-2 in 1 milliseconds for epoch 3, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,919] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,920] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,920] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,920] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,920] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,920] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,920] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,921] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,921] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,921] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,921] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,922] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,922] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,922] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,922] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,922] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,923] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,923] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,923] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,923] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,924] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,924] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,924] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,925] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,924] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,925] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,925] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,925] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,926] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,926] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,926] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,926] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,927] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,927] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,927] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,927] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,927] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,928] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,928] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,928] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,928] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,929] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,929] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,929] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,930] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,930] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,931] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,931] INFO [Broker id=6] Transitioning 2 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:25:02,932] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:02,933] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:02,933] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:02,994] INFO [Broker id=6] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:25:02,994] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-47, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:02,995] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:02,996] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:02,997] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:02,998] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:02,999] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:02,999] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,000] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,003] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,006] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,007] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,008] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,008] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,009] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,010] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,010] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,011] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,390] INFO [Broker id=6] Transitioning 16 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:03,391] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,392] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,392] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,393] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,394] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,395] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,396] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,396] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,398] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,398] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,400] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,402] INFO [Partition __consumer_offsets-48 broker=6] ISR updated to 6,5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,403] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,404] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,405] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,405] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,405] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 2 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:03,406] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-45, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-0, __consumer_offsets-27, __consumer_offsets-40, __consumer_offsets-6, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-33) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:03,407] INFO [Broker id=6] Stopped fetchers as part of become-follower for 16 partitions (state.change.logger)
[2025-05-20 22:25:03,432] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,439] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-45 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-43 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-12 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-10 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-24 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-21 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-19 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-17 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-32 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-0 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-27 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-40 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-6 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-3 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-36 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0), __consumer_offsets-33 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),2,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:03,439] INFO [Broker id=6] Started fetchers as part of become-follower for 16 partitions (state.change.logger)
[2025-05-20 22:25:03,440] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,442] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,444] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,445] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,451] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,453] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,456] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,456] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,457] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,457] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,457] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,458] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,458] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,459] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,460] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,460] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,461] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,461] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,461] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,462] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,462] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,463] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,463] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,463] INFO [Partition __consumer_offsets-15 broker=6] ISR updated to 6,4,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,464] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,465] INFO [Partition __consumer_offsets-47 broker=6] ISR updated to 6,4,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,467] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,468] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,469] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,468] INFO [Partition __consumer_offsets-13 broker=6] ISR updated to 6,5,4  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:03,470] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,472] INFO [Partition __consumer_offsets-46 broker=6] ISR updated to 6,5,4  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:03,472] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,472] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,472] INFO [Partition __consumer_offsets-11 broker=6] ISR updated to 6,5,4  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:03,473] INFO [Partition __consumer_offsets-44 broker=6] ISR updated to 6,5,4  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:03,473] INFO [Partition __consumer_offsets-9 broker=6] ISR updated to 6,5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,474] INFO [Partition __consumer_offsets-42 broker=6] ISR updated to 6,5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,474] INFO [Partition __consumer_offsets-23 broker=6] ISR updated to 6,5,4  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:03,474] INFO [Partition __consumer_offsets-30 broker=6] ISR updated to 6,5,4  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:03,475] INFO [Partition __consumer_offsets-28 broker=6] ISR updated to 6,5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,473] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:03,475] INFO [Partition __consumer_offsets-26 broker=6] ISR updated to 6,5,4  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:03,476] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:03,476] INFO [Partition __consumer_offsets-7 broker=6] ISR updated to 6,5,4  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:03,476] INFO [Partition __consumer_offsets-5 broker=6] ISR updated to 6,5,4  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:03,477] INFO [Partition __consumer_offsets-38 broker=6] ISR updated to 6,5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,478] INFO [Partition __consumer_offsets-1 broker=6] ISR updated to 6,5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,478] INFO [Partition __consumer_offsets-34 broker=6] ISR updated to 6,5,4  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:03,489] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,489] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,489] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,490] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,490] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,490] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,490] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,491] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,491] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,491] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,492] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,492] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,492] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,493] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,493] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,494] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,494] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,495] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,491] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,495] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,496] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,496] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,496] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,497] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,497] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,498] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,498] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,499] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,499] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,499] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,500] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,500] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,501] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,501] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,501] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,502] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,503] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,503] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,502] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,504] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,504] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,505] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,505] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,509] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,510] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,510] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,515] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,515] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,518] INFO [Broker id=6] Transitioning 18 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:25:03,520] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-47, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:03,520] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,522] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,522] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,523] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,524] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,525] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,527] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,529] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,529] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,530] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,534] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,535] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,535] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,536] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,537] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,538] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,539] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,540] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,563] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:03,564] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,565] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,565] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,566] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,897] INFO [Broker id=6] Transitioning 10 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:03,898] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,898] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,899] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,899] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,900] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,900] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,901] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,902] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,903] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,903] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,904] INFO [Partition __consumer_offsets-16 broker=6] ISR updated to 6,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,904] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,909] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,913] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,913] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,914] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,914] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,914] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,915] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,915] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,916] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,916] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,917] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,917] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,917] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,918] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,918] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,918] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,918] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,919] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,919] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,919] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,920] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,919] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,920] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,920] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,920] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,921] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,921] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,921] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,922] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,947] INFO [Broker id=6] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:25:03,948] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-16) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:03,948] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,952] INFO [Partition _schemas-0 broker=6] ISR updated to 6,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,952] INFO [Partition __consumer_offsets-14 broker=6] ISR updated to 6,4  and version updated to 6 (kafka.cluster.Partition)
[2025-05-20 22:25:03,953] INFO [Partition __consumer_offsets-41 broker=6] ISR updated to 6,4  and version updated to 6 (kafka.cluster.Partition)
[2025-05-20 22:25:03,953] INFO [Partition __consumer_offsets-22 broker=6] ISR updated to 6,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-20 22:25:03,953] INFO [Partition __consumer_offsets-20 broker=6] ISR updated to 6,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-20 22:25:03,954] INFO [Partition __consumer_offsets-49 broker=6] ISR updated to 6,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,954] INFO [Partition __consumer_offsets-18 broker=6] ISR updated to 6,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,955] INFO [Partition __consumer_offsets-31 broker=6] ISR updated to 6,4  and version updated to 6 (kafka.cluster.Partition)
[2025-05-20 22:25:03,955] INFO [Partition __consumer_offsets-29 broker=6] ISR updated to 6,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,955] INFO [Partition __consumer_offsets-25 broker=6] ISR updated to 6,4  and version updated to 6 (kafka.cluster.Partition)
[2025-05-20 22:25:03,955] INFO [Partition __consumer_offsets-39 broker=6] ISR updated to 6,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,956] INFO [Partition __consumer_offsets-8 broker=6] ISR updated to 6,4  and version updated to 6 (kafka.cluster.Partition)
[2025-05-20 22:25:03,956] INFO [Partition __consumer_offsets-37 broker=6] ISR updated to 6,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,956] INFO [Partition __consumer_offsets-35 broker=6] ISR updated to 6,4  and version updated to 6 (kafka.cluster.Partition)
[2025-05-20 22:25:03,957] INFO [Partition __consumer_offsets-4 broker=6] ISR updated to 6,4  and version updated to 6 (kafka.cluster.Partition)
[2025-05-20 22:25:03,957] INFO [Partition __consumer_offsets-2 broker=6] ISR updated to 6,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,995] INFO [Broker id=6] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:25:03,997] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(_schemas-0, __consumer_offsets-14, __consumer_offsets-41, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:03,998] INFO [Broker id=6] Skipped the become-leader state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,998] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,999] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,999] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,000] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,003] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,004] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,005] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,006] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,006] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,007] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,008] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,009] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,009] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,010] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,010] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,128] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:04,129] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,133] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,134] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,138] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,428] INFO [Broker id=6] Transitioning 6 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:04,428] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,429] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,429] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,434] INFO [Partition __consumer_offsets-16 broker=6] ISR updated to 6,4,5  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,435] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,442] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,444] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,445] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,447] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,447] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,447] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,448] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,449] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,451] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,451] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,453] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,453] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,454] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,454] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,455] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,455] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,456] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,456] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,456] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,457] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,489] INFO [Partition _schemas-0 broker=6] ISR updated to 6,5,4  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,489] INFO [Partition __consumer_offsets-14 broker=6] ISR updated to 6,4,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:04,490] INFO [Partition __consumer_offsets-41 broker=6] ISR updated to 6,4,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:04,491] INFO [Partition __consumer_offsets-22 broker=6] ISR updated to 6,5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:04,492] INFO [Partition __consumer_offsets-20 broker=6] ISR updated to 6,5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:04,493] INFO [Partition __consumer_offsets-49 broker=6] ISR updated to 6,4,5  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,493] INFO [Partition __consumer_offsets-18 broker=6] ISR updated to 6,5,4  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,493] INFO [Partition __consumer_offsets-31 broker=6] ISR updated to 6,4,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:04,493] INFO [Partition __consumer_offsets-29 broker=6] ISR updated to 6,5,4  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,494] INFO [Partition __consumer_offsets-25 broker=6] ISR updated to 6,4,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:04,490] INFO [Broker id=6] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:25:04,494] INFO [Partition __consumer_offsets-39 broker=6] ISR updated to 6,4,5  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,495] INFO [Partition __consumer_offsets-8 broker=6] ISR updated to 6,4,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:04,495] INFO [Partition __consumer_offsets-37 broker=6] ISR updated to 6,4,5  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,495] INFO [Partition __consumer_offsets-35 broker=6] ISR updated to 6,4,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:04,495] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-16) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:04,496] INFO [Partition __consumer_offsets-4 broker=6] ISR updated to 6,4,5  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:04,496] INFO [Partition __consumer_offsets-2 broker=6] ISR updated to 6,4,5  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,496] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,591] INFO [Broker id=6] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:25:04,595] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(_schemas-0, __consumer_offsets-14, __consumer_offsets-41, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:04,596] INFO [Broker id=6] Skipped the become-leader state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,597] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,598] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,602] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,603] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,604] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,611] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,612] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,613] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,614] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,614] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,615] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,616] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,617] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,618] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,619] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,640] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:04,644] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,645] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,646] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,646] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,995] INFO [Broker id=6] Transitioning 8 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:04,997] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,998] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,999] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,000] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,000] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,000] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,001] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,002] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,003] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,007] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,008] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,009] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,008] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,010] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,009] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,010] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,011] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,011] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,012] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,011] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,012] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,013] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,013] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,014] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,014] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,015] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,016] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,016] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,015] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,017] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,018] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,018] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,146] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:05,147] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,148] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,149] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,149] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,497] INFO [Broker id=6] Transitioning 4 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:05,499] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,501] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,502] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,503] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:05,503] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,504] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,504] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,504] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,505] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,505] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,506] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,505] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,506] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:05,507] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,508] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:05,508] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:06,741] INFO [GroupCoordinator 6]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-99122467-ec1a-4fc4-88f0-59fda8f60096 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:06,768] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-99122467-ec1a-4fc4-88f0-59fda8f60096 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:06,792] INFO [GroupCoordinator 6]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:06,861] INFO [GroupCoordinator 6]: Assignment received from leader sr-1-99122467-ec1a-4fc4-88f0-59fda8f60096 for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:01,273] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 1 (__consumer_offsets-29) (reason: Removing member sr-1-99122467-ec1a-4fc4-88f0-59fda8f60096 on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:01,274] INFO [GroupCoordinator 6]: Group schema-registry with generation 2 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:01,276] INFO [GroupCoordinator 6]: Member MemberMetadata(memberId=sr-1-99122467-ec1a-4fc4-88f0-59fda8f60096, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,382] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 22:26:02,387] INFO [BrokerServer id=6] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
[2025-05-20 22:26:02,387] INFO [BrokerServer id=6] shutting down (kafka.server.BrokerServer)
[2025-05-20 22:26:02,388] INFO [BrokerLifecycleManager id=6] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:26:02,440] INFO [BrokerLifecycleManager id=6] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:26:02,466] INFO [Broker id=6] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:26:02,469] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,471] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,472] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 3 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,474] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 3 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,474] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,475] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,476] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,477] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,478] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,479] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 3 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,480] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 4 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,481] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,482] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,482] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 5 from offset 2 with partition epoch 10 and high watermark 2. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,483] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,484] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,486] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 4 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,487] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,489] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 4 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,489] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,490] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,496] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 5 from offset 2 with partition epoch 10 and high watermark 2. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,499] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 4 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,501] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 3 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,502] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,504] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 4 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,504] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,505] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 4 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,505] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 4 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,506] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,506] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,507] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,507] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,507] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,508] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 4 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,508] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,508] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,509] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,509] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,509] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 3 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,510] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 3 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,510] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,511] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,511] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 4 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 22:26:02,512] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,514] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 3 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,515] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,516] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,516] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:26:02,517] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 3 from offset 0 with partition epoch 9 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,517] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 22:26:02,532] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:26:02,535] INFO [ReplicaAlterLogDirsManager on broker 6] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:26:02,538] INFO [BrokerLifecycleManager id=6] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:26:02,539] INFO [BrokerLifecycleManager id=6] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:02,540] INFO [BrokerLifecycleManager id=6] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:26:02,540] INFO [broker-6-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,541] INFO [broker-6-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,541] INFO [broker-6-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,542] INFO [SocketServer listenerType=BROKER, nodeId=6] Stopping socket server request processors (kafka.network.SocketServer)
[2025-05-20 22:26:02,549] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:26:02,557] INFO [SocketServer listenerType=BROKER, nodeId=6] Stopped socket server request processors (kafka.network.SocketServer)
[2025-05-20 22:26:02,563] INFO [Broker id=6] Stopped fetchers as part of controlled shutdown for 51 partitions (state.change.logger)
[2025-05-20 22:26:02,567] INFO [ReplicaFetcherThread-0-5]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:26:02,568] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:26:02,569] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Cancelled in-flight FETCH request with correlation id 118 due to node 5 being disconnected (elapsed time since creation: 217ms, elapsed time since send: 217ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:26:02,570] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=332390813, epoch=118) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Client was shutdown before response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:109)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 22:26:02,575] INFO [ReplicaFetcherThread-0-5]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:26:02,575] INFO [ReplicaFetcherThread-0-5]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:26:02,578] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,578] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,579] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,579] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,579] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,579] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,580] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,580] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,580] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,580] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,581] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,581] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,582] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,581] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,582] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,582] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,582] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,583] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,584] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,583] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,584] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,584] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,585] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,586] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,587] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,587] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,587] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,588] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,588] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,588] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,589] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,589] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,588] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,589] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,590] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,590] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,590] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,591] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,591] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,590] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,591] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,592] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,592] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,593] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,602] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,601] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,603] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,603] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,603] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,604] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,604] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,604] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,604] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,605] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,605] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,605] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,606] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,606] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,607] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,608] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,609] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,610] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,610] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,611] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,609] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,611] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,612] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,613] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,613] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,613] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,613] INFO [GroupCoordinator 6]: Unloading group metadata for schema-registry with generation 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,613] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,614] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,614] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,614] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,615] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,615] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,616] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,616] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,617] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,616] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,617] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,618] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,618] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,618] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,619] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,619] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,619] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,620] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,620] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,621] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,620] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,621] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,621] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,622] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,622] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,623] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,625] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,625] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,626] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,626] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,626] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,627] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,627] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,627] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,628] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,628] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,628] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,628] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,629] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,629] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,630] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,630] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,631] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,632] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,632] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,633] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,633] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,633] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,634] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,634] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,635] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,635] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,635] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,635] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,636] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,637] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,637] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,637] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,637] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,638] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,638] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,640] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,640] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,640] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,643] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,643] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,644] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,645] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,645] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,645] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,646] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,647] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,648] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,650] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,650] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,651] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,652] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,652] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,652] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,653] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,654] INFO [data-plane Kafka Request Handler on Broker 6], shutting down (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 22:26:02,657] INFO [data-plane Kafka Request Handler on Broker 6], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 22:26:02,658] INFO [ExpirationReaper-6-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,659] INFO [ExpirationReaper-6-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,659] INFO [ExpirationReaper-6-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,662] INFO [KafkaApi-6] Shutdown complete. (kafka.server.KafkaApis)
[2025-05-20 22:26:02,667] INFO [TransactionCoordinator id=6] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:26:02,668] INFO [Transaction State Manager 6]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2025-05-20 22:26:02,669] INFO [TxnMarkerSenderThread-6]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:26:02,669] INFO [TxnMarkerSenderThread-6]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:26:02,669] INFO [TxnMarkerSenderThread-6]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:26:02,673] INFO [TransactionCoordinator id=6] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:26:02,674] INFO [GroupCoordinator 6]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,675] INFO [ExpirationReaper-6-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,676] INFO [ExpirationReaper-6-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,676] INFO [ExpirationReaper-6-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,676] INFO [ExpirationReaper-6-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,677] INFO [ExpirationReaper-6-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,677] INFO [ExpirationReaper-6-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,678] INFO [GroupCoordinator 6]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,680] INFO [AssignmentsManager id=6]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:02,681] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,681] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,681] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,682] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:26:02,683] INFO [AssignmentsManager id=6]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:02,684] INFO [ReplicaManager broker=6] Shutting down (kafka.server.ReplicaManager)
[2025-05-20 22:26:02,686] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:26:02,686] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:26:02,686] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:26:02,688] INFO [ReplicaFetcherManager on broker 6] shutting down (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:26:02,689] INFO [ReplicaFetcherManager on broker 6] shutdown completed (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:26:02,690] INFO [ReplicaAlterLogDirsManager on broker 6] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:26:02,691] INFO [ReplicaAlterLogDirsManager on broker 6] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:26:02,691] INFO [ExpirationReaper-6-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,692] INFO [ExpirationReaper-6-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,692] INFO [ExpirationReaper-6-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,694] INFO [ExpirationReaper-6-RemoteFetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,695] INFO [ExpirationReaper-6-RemoteFetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,695] INFO [ExpirationReaper-6-RemoteFetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,696] INFO [ExpirationReaper-6-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,697] INFO [ExpirationReaper-6-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,697] INFO [ExpirationReaper-6-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,698] INFO [ExpirationReaper-6-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,700] INFO [ExpirationReaper-6-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,700] INFO [ExpirationReaper-6-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,702] INFO [ExpirationReaper-6-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,704] INFO [ExpirationReaper-6-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,704] INFO [ExpirationReaper-6-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,749] INFO [AddPartitionsToTxnSenderThread-6]: Shutting down (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:26:02,750] INFO [AddPartitionsToTxnSenderThread-6]: Stopped (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:26:02,750] INFO [AddPartitionsToTxnSenderThread-6]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:26:02,751] INFO [ReplicaManager broker=6] Shut down completely (kafka.server.ReplicaManager)
[2025-05-20 22:26:02,752] INFO [broker-6-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,752] INFO [broker-6-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,752] INFO [broker-6-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,753] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:26:02,754] INFO [broker-6-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,755] INFO [broker-6-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,755] INFO [broker-6-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,756] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:26:02,757] INFO Shutting down. (kafka.log.LogManager)
[2025-05-20 22:26:02,758] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
[2025-05-20 22:26:02,759] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:26:02,759] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:26:02,759] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:26:02,772] INFO [ProducerStateManager partition=__consumer_offsets-29] Wrote producer snapshot at offset 2 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:26:02,885] INFO [ProducerStateManager partition=_schemas-0] Wrote producer snapshot at offset 2 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:26:02,969] INFO Shutdown complete. (kafka.log.LogManager)
[2025-05-20 22:26:02,970] INFO [broker-6-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,971] INFO [broker-6-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,971] INFO [broker-6-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,972] INFO [broker-6-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,972] INFO [broker-6-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,972] INFO [broker-6-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,973] INFO [broker-6-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,973] INFO [broker-6-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,973] INFO [broker-6-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,974] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,974] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,974] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:02,975] INFO [SocketServer listenerType=BROKER, nodeId=6] Shutting down socket server (kafka.network.SocketServer)
[2025-05-20 22:26:02,989] INFO [SocketServer listenerType=BROKER, nodeId=6] Shutdown completed (kafka.network.SocketServer)
[2025-05-20 22:26:02,990] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
[2025-05-20 22:26:02,991] INFO [BrokerLifecycleManager id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:02,992] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:26:02,993] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:26:02,993] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:26:02,994] INFO [SharedServer id=6] Stopping SharedServer (kafka.server.SharedServer)
[2025-05-20 22:26:02,995] INFO [MetadataLoader id=6] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:02,995] INFO [SnapshotGenerator id=6] close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:03,000] INFO [SnapshotGenerator id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:03,001] INFO [MetadataLoader id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:03,002] INFO [SnapshotGenerator id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:03,003] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:26:03,046] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:26:03,046] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:26:03,047] INFO [kafka-6-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:26:03,047] INFO [RaftManager id=6] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:26:03,048] INFO [RaftManager id=6] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:26:03,048] INFO [RaftManager id=6] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:26:03,048] INFO [kafka-6-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:26:03,048] INFO [kafka-6-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:26:03,053] INFO [kafka-6-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:26:03,053] INFO [kafka-6-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:26:03,053] INFO [kafka-6-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:26:03,057] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2740 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:26:03,059] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:26:03,060] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:26:03,060] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:26:03,061] INFO App info kafka.server for 6 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:26:03,061] INFO [BrokerServer id=6] shut down completed (kafka.server.BrokerServer)
[2025-05-20 22:26:03,061] INFO [BrokerServer id=6] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
[2025-05-20 22:26:03,062] INFO App info kafka.server for 6 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 23:12:46,772] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 23:12:47,282] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 23:12:47,326] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 23:12:47,328] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:53,116] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 23:12:53,349] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 23:12:53,366] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:53,597] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:53,631] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:53,677] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 23:12:53,683] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-20 23:12:53,693] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2025-05-20 23:12:53,718] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:53,829] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
[2025-05-20 23:12:53,831] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:53,840] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:53,844] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000002740.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-20 23:12:53,845] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:53,978] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2740 with 0 producer ids in 11 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:12:53,987] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 2740 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:53,988] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2740 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:53,989] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=2740, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000002740.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:12:53,991] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 2740 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:54,012] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-20 23:12:54,031] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 23:12:54,037] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 23:12:54,120] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 23:12:54,206] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=15, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from null (org.apache.kafka.raft.QuorumState)
[2025-05-20 23:12:54,217] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 23:12:54,225] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 23:12:54,313] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:54,315] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2025-05-20 23:12:54,337] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:54,374] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,404] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@773118019 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 23:12:54,408] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,413] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:54,421] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:12:54,442] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:12:54,457] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:12:54,493] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:12:54,509] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,510] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,520] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:54,511] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 23:12:54,523] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 23:12:54,525] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,531] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,531] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:54,534] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,536] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:54,544] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,548] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 23:12:54,622] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:54,628] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,629] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,633] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,633] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,651] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,654] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,728] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:54,819] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,823] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,830] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:54,835] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,840] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,846] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,847] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,938] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,152] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 23:12:55,155] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,170] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,190] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,193] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,198] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,214] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,215] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,228] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-20 23:12:55,257] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 23:12:55,268] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,288] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-20 23:12:55,337] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:55,348] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:55,373] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,404] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:55,409] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:55,421] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:55,425] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:55,429] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:55,428] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:55,426] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:55,444] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:55,444] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:55,475] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,590] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,691] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,696] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,715] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,739] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:55,740] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:55,742] INFO [BrokerLifecycleManager id=6] Incarnation bLNDw_1cQoe4xFVOdFKS2g of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:12:55,751] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:55,795] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,796] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 23:12:55,809] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 23:12:55,811] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 23:12:55,860] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,867] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:55,904] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,914] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:55,988] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,988] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,005] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,038] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,056] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:56,058] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,105] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,109] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,146] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:56,147] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,200] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,206] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,216] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:56,216] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,267] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,284] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:56,285] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,307] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,337] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,361] INFO [RaftManager id=6] Completed transition to Unattached(epoch=16, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=15, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-20 23:12:56,409] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,423] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:56,438] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=16, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=16, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 23:12:56,511] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,528] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,612] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,722] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,835] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,936] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,037] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,082] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:57,082] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:57,133] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:57,138] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,195] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=2744, metadata=Optional.empty)] for the first time for epoch 16 (org.apache.kafka.raft.FollowerState)
[2025-05-20 23:12:57,200] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 2744 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,216] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 2745 (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:12:57,361] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 2744 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,369] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 2743 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,370] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 2743 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,370] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 2743 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,372] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=2743, epoch=16) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-20 23:12:57,377] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,387] INFO Skipping recovery of 51 logs from /tmp/kafka-logs since clean shutdown file was found (kafka.log.LogManager)
[2025-05-20 23:12:57,458] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,461] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,472] INFO [ProducerStateManager partition=__consumer_offsets-29] Loading producer state from snapshot file 'SnapshotFile(offset=2, file=/tmp/kafka-logs/__consumer_offsets-29/00000000000000000002.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:12:57,473] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,496] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-29, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=29, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=2) with 1 segments, local-log-start-offset 0 and log-end-offset 2 in 72ms (1/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,520] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,528] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-38, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=38, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 29ms (2/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,532] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,557] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-27, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=27, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 28ms (3/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,612] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,642] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-39, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=39, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 85ms (4/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,733] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,749] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-42, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=42, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 85ms (5/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,762] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,768] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-9, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (6/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,811] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,818] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-22, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=22, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 47ms (7/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,836] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,846] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-26, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=26, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 27ms (8/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,852] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,854] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-40, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=40, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (9/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,858] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,867] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-47, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=47, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (10/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,903] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,907] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-15, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 39ms (11/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,919] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,924] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-10, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 16ms (12/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,952] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,955] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-2, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 26ms (13/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,963] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,966] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-18, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (14/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,977] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,996] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-46, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=46, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 29ms (15/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,000] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,002] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-12, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (16/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,020] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,026] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-23, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=23, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 16ms (17/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,041] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,048] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-14, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 19ms (18/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,073] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,087] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-5, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 38ms (19/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,096] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,099] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-41, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=41, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (20/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,109] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,114] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-1, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (21/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,127] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,133] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-19, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (22/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,154] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,163] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-3, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 21ms (23/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,188] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,207] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-16, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 43ms (24/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,221] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,223] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-25, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=25, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (25/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,232] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,233] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,233] INFO [ProducerStateManager partition=_schemas-0] Loading producer state from snapshot file 'SnapshotFile(offset=2, file=/tmp/kafka-logs/_schemas-0/00000000000000000002.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:12:58,234] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,236] INFO Completed load of Log(dir=/tmp/kafka-logs/_schemas-0, topicId=RrE8eovWRKu4kLR3MRJ0fA, topic=_schemas, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=2) with 1 segments, local-log-start-offset 0 and log-end-offset 2 in 12ms (26/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,259] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,277] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-28, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=28, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 38ms (27/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,290] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,295] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-13, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (28/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,304] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,313] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-24, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=24, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (29/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,332] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,347] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-32, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=32, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 21ms (30/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,372] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,377] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-31, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=31, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 19ms (31/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,380] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,383] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-21, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=21, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (32/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,387] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,392] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-6, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (33/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,399] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,404] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-11, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (34/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,418] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,421] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-30, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=30, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (35/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,425] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,430] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-43, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=43, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (36/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,457] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,462] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-7, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 30ms (37/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,471] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,475] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-33, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=33, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (38/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,481] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,485] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-36, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=36, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (39/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,491] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,495] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-45, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=45, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (40/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,500] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,504] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-0, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (41/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,512] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,519] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-20, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=20, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (42/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,524] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,526] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-4, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (43/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,530] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,533] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-8, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (44/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,553] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,558] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-49, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=49, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 23ms (45/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,565] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,570] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-34, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=34, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (46/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,618] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,631] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-48, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=48, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 51ms (47/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,654] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,672] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-44, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=44, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 40ms (48/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,689] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,696] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-37, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=37, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 22ms (49/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,724] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,739] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-35, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=35, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 38ms (50/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,747] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,757] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-17, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (51/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,761] INFO Loaded 51 logs in 1383ms (kafka.log.LogManager)
[2025-05-20 23:12:58,765] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-20 23:12:58,780] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-20 23:12:58,793] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-20 23:12:58,950] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 23:12:58,965] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 23:12:58,973] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:58,973] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 23:12:58,988] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,005] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 23:12:59,028] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 23:12:59,028] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 23:12:59,037] INFO [Broker id=6] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:12:59,039] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,059] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,062] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,063] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,067] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,067] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,069] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,073] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,074] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,074] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,075] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,076] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,079] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,082] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,091] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,100] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,102] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,103] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,103] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,106] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,109] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,116] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,119] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,124] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,130] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,136] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,140] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,142] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,147] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,149] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,150] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,152] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,152] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,156] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,169] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,174] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,175] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,177] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,181] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,182] INFO [Broker id=6] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-20 23:12:59,183] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 2 (kafka.cluster.Partition)
[2025-05-20 23:12:59,185] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 6 from offset 2 with partition epoch 11 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,186] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,189] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,191] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,192] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,193] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,193] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,193] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,195] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,195] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,195] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,196] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,196] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,198] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,205] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,206] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,208] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,209] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,214] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,214] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,217] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,221] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,223] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,227] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 2 (kafka.cluster.Partition)
[2025-05-20 23:12:59,230] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 6 from offset 2 with partition epoch 11 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,230] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,231] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,235] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,239] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,241] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,242] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,245] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,260] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,270] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,271] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,275] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,276] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,277] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,280] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,280] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,281] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,282] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,282] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,282] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,284] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,284] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,285] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,286] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,287] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,287] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,288] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,289] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,289] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,294] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,296] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,297] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,298] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,299] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,299] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,301] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,301] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,302] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,303] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,304] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,304] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,307] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,310] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,311] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,313] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,316] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,316] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,317] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,318] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,318] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,320] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,320] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,321] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,322] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,322] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,322] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,324] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,325] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,326] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,330] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,332] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,332] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,334] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,334] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,335] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,342] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,342] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,343] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,345] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,349] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,349] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,355] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,358] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,362] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,365] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,368] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,370] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,380] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,382] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,385] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,387] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,388] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,388] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,392] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,395] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,397] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,401] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,402] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,409] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:12:59,413] INFO [Broker id=6] Stopped fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 23:12:59,420] INFO [Broker id=6] Started fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 23:12:59,495] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,499] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,508] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,508] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,514] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,523] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,523] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,535] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,555] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,565] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,575] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,575] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,595] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,605] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,605] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,615] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,619] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,619] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,621] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,623] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,623] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,625] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,630] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,630] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,632] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,633] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,645] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,647] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,651] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,645] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,667] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,668] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,666] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,672] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,673] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,673] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,676] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,677] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,677] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,678] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,679] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,679] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,679] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,680] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,680] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,682] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,683] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,683] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,684] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,685] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,685] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,685] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,687] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,687] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,691] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,692] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,694] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,692] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,698] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,698] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,701] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,701] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,701] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,703] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,703] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,703] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,704] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,705] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,705] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,706] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,706] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,706] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,712] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,712] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,713] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,713] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,714] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,714] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,714] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,716] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,716] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,717] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,718] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,718] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,722] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,722] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,722] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,727] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,729] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,729] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,735] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,742] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,742] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,752] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,757] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,757] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,763] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,765] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,766] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,768] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,770] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,770] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,773] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,773] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,774] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,775] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,776] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,776] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,776] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,779] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,779] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,779] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,780] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,779] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,781] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,780] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,782] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,782] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,782] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,787] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,788] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,789] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,792] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,793] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,793] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,794] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,787] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,794] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,795] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,796] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,798] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,794] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,799] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,799] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,799] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,800] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,800] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,800] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,800] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,801] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,801] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,801] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,801] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,801] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,802] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,802] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,802] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,803] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,803] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,809] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 23:12:59,817] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 23:12:59,830] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 2743 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:59,848] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:12:59,849] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 23:12:59,849] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 23:12:59,850] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 23:12:59,892] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 23:12:59,904] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:59,904] INFO [BrokerLifecycleManager id=6] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:12:59,919] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 23:13:00,022] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:13:00,026] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 23:13:00,032] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 23:13:00,034] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 23:13:00,041] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2025-05-20 23:13:00,044] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 23:13:00,063] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 23:13:00,077] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 23:13:00,095] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 23:13:00,105] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 23:13:00,115] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 23:13:00,120] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-20 23:13:00,125] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 23:13:00,136] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 23:13:00,129] INFO [Broker id=6] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:13:00,145] INFO Kafka startTimeMs: 1747782780125 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 23:13:00,146] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,156] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,157] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,158] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,158] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,159] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,159] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,159] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,160] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,161] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,164] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,164] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,165] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,165] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 7 from offset 2 with partition epoch 12 and high watermark 2. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,166] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,166] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,167] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,168] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,168] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,169] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,169] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,177] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 7 from offset 2 with partition epoch 12 and high watermark 2. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,178] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,182] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,184] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,151] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-20 23:13:00,194] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,198] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,204] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,205] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,206] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,207] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,207] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,208] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,209] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,209] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,211] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,212] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,212] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,213] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,213] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,214] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,214] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,215] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,217] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,218] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,218] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,219] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,219] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,220] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 4 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,221] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 5 from offset 0 with partition epoch 11 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,221] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 7 from offset 0 with partition epoch 12 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:13:00,223] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:13:00,226] INFO [Broker id=6] Stopped fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 23:13:00,266] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,284] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-9 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-42 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-21 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-17 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-38 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-1 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,0), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), _schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,2), __consumer_offsets-45 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-12 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-41 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,0), __consumer_offsets-24 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-20 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,0), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-0 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,2), __consumer_offsets-25 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,0), __consumer_offsets-8 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-4 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,0), __consumer_offsets-33 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-15 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,0), __consumer_offsets-48 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,0), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-19 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-32 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-28 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-40 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-3 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-36 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-47 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-14 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-43 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-10 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-22 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-31 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-27 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0), __consumer_offsets-6 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,0), __consumer_offsets-35 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,0), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:13:00,284] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,286] INFO [Broker id=6] Started fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 23:13:00,297] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,306] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,307] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,309] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,310] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,310] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,311] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,311] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,311] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,312] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,312] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,312] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,313] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,313] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,314] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,316] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,318] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,319] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,319] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,320] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,320] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,320] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,320] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,321] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,321] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,321] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,322] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,322] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,323] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,323] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,323] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,324] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,324] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,324] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,324] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,325] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,344] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,344] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,345] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,346] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,347] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,347] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,347] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,348] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,349] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,351] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,352] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,353] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,354] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,355] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,357] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,357] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,360] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,369] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,372] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,375] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,376] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,380] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,391] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,394] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,406] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,413] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,414] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,458] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,463] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,465] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,466] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,467] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,468] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,469] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,470] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,471] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,472] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,472] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,473] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,474] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,475] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,475] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,476] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,476] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,477] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,477] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,478] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,480] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,481] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,482] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,483] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,484] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,485] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,485] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,486] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,486] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,487] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,487] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,488] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,489] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:13:00,490] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:13:00,516] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,516] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,517] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,517] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,518] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,519] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,519] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,520] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,519] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,522] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,523] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,524] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,520] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,526] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,526] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,527] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,526] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,527] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,528] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,528] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,529] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,529] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,529] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,530] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,531] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,529] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,531] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,532] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,532] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,533] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,532] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,536] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,536] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,537] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,537] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,537] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,538] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,534] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,538] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,539] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,539] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,540] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,541] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,542] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,542] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,540] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,543] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,546] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,546] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,553] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,554] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,554] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,554] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,555] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,560] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,561] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,561] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,561] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,562] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,561] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,565] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,566] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,568] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,568] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,569] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,569] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,570] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,570] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,571] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,571] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,571] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,572] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,572] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,572] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,570] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,580] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,581] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,582] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,582] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,583] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,583] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,583] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,584] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,593] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,593] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,603] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,604] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,606] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,606] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,606] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,607] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,607] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,607] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,607] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,608] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,608] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,608] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,581] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,608] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,609] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,610] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,618] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,608] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,619] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,620] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,620] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,621] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,621] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,621] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,625] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,621] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,627] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,628] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,631] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,632] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,634] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,627] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,638] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,639] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,639] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,636] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,640] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,639] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,641] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,642] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,643] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,641] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,648] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,651] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,652] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,653] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,653] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,654] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,657] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,657] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,658] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,658] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,659] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,660] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,661] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,655] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,663] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,664] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,666] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,668] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,668] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,670] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,670] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,671] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,672] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:13:00,673] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:00,674] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,674] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,675] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,162] INFO [Broker id=6] Transitioning 31 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:13:01,164] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,164] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,164] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,165] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,166] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,166] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:01,166] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:01,167] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,168] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,169] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,169] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,171] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,172] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,173] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:01,174] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,174] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,175] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,176] INFO [Broker id=6] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,177] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,178] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,178] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,178] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,179] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,179] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,183] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,184] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,184] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,184] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:01,185] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,185] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,185] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,187] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,187] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,188] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,188] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,188] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,189] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,191] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,191] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,192] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,192] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,195] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,195] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,196] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,196] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,197] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,196] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,198] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,199] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,199] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,205] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,206] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,199] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,210] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,211] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,211] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,212] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,212] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,213] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,213] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,214] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,211] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,216] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,216] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,216] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,217] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,217] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,218] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,218] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,218] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,218] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,219] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,219] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,220] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,220] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,220] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,221] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,221] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,220] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,222] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,223] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,223] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,222] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,224] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,225] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,227] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,228] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,228] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,229] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,234] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,225] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,238] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,238] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,239] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,245] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,245] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,246] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,244] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,247] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,248] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,248] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,248] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,248] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,249] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,249] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,250] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,250] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,251] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,251] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,250] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,251] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,252] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,252] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,252] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,253] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,253] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,252] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,254] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,254] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,255] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,256] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,261] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:13:01,261] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,262] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,263] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,263] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,654] INFO [Broker id=6] Transitioning 25 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:13:01,655] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,656] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:01,656] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:01,657] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,657] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,658] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,658] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,659] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,668] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:01,670] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,671] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,672] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,672] INFO [Broker id=6] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,672] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,673] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,673] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,673] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,674] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,674] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,674] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:01,675] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,675] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,685] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:01,689] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:01,691] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:01,692] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,693] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,701] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,704] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,705] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,713] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,713] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,714] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,714] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,715] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,717] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,717] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,717] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,719] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,720] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,720] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,721] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,725] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,726] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,727] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,736] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,737] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,737] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,743] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,745] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,745] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,746] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,748] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,746] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,752] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,753] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,756] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,756] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,759] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,761] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,763] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,768] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,757] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,771] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,772] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,770] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,773] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,773] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,774] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,779] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,779] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,780] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,781] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,781] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,782] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,783] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,783] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,783] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,787] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,788] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,790] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,790] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,790] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,793] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,800] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,801] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,801] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,801] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,802] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,802] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,803] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,803] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,804] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,806] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,807] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:01,810] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,815] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,154] INFO [Broker id=6] Transitioning 25 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:13:02,156] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:02,156] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:02,156] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,157] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,158] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,160] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,161] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,163] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:02,163] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,164] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,164] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,165] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,165] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:02,166] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:02,175] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,176] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,176] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:02,177] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,177] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:02,177] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,178] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,181] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,182] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:02,182] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,183] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,184] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,184] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,185] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,185] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,186] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,185] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,186] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,187] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,187] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,187] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,188] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,188] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,189] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,189] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,188] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,189] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,189] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,190] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,190] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,190] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,191] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,192] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,193] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,193] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,193] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,194] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,194] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,195] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,191] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,195] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,195] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,196] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,196] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,196] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,197] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,197] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,197] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,198] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,198] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,199] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,198] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,199] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,199] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,200] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,200] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,200] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,200] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,201] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,201] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,202] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,202] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,203] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,202] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,203] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,203] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,204] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,204] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,204] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,205] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,205] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,206] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,205] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,206] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,206] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,207] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,208] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,208] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,208] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,209] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,209] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,209] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,210] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,210] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,210] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,211] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,350] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:13:02,351] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:02,352] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,353] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,354] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,653] INFO [Broker id=6] Transitioning 18 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:13:02,655] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:02,655] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,656] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,657] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4, 6], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:02,657] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,659] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4, 6], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,662] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:02,663] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,664] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,665] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4, 6], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-20 23:13:02,666] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4, 6], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,666] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4, 6], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,667] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,667] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,669] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,670] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-20 23:13:02,671] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-20 23:13:02,671] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4, 6], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-20 23:13:02,672] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,672] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,672] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,673] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,673] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,674] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,674] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,674] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,675] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,674] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,677] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,677] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,678] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,679] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,679] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,680] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,681] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,677] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,682] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,683] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,681] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,683] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,683] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,684] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,684] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,685] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,686] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,686] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,687] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,687] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,686] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,687] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,688] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,688] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,688] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,689] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,689] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,691] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,692] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,692] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,692] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,693] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,693] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,693] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,694] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,694] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,694] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,695] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,697] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,697] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:02,697] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,696] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,698] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:02,699] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,239] INFO [Broker id=6] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2025-05-20 23:17:57,244] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-47, __consumer_offsets-48, __consumer_offsets-14, __consumer_offsets-9, __consumer_offsets-41, __consumer_offsets-42, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-31, __consumer_offsets-28, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-38, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-1) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:17:57,251] INFO [Broker id=6] Leader __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,257] INFO [Broker id=6] Leader __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 6 from offset 0 with partition epoch 14, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:17:57,262] INFO [Broker id=6] Leader __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,266] INFO [Broker id=6] Leader __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 6 from offset 0 with partition epoch 14, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:17:57,271] INFO [Broker id=6] Leader __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 6 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:17:57,276] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-13 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,277] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-13 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,277] INFO [Broker id=6] Leader __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,278] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-46 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,279] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-46 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,279] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-30 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,280] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-30 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,280] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-26 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,281] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-26 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,281] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-5 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,283] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-5 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,283] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-34 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,284] INFO [Broker id=6] Leader __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 6 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:17:57,284] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-34 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,284] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-16 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,285] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-16 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,285] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-49 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,286] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-49 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,286] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-37 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,287] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-37 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,288] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-11 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,288] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-11 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,288] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-44 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,289] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-44 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,289] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-23 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,290] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-23 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,291] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-18 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,291] INFO [Broker id=6] Leader __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 14, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,291] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-18 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,292] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-39 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,293] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-39 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,293] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-7 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,294] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-7 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,295] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-2 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,296] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-2 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,296] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-29 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,297] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-29 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,297] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition _schemas-0 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,298] INFO [Broker id=6] Leader __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,298] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition _schemas-0 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,305] INFO [Broker id=6] Leader __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 6 from offset 0 with partition epoch 14, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:17:57,311] INFO [Broker id=6] Leader __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 14, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,318] INFO [Broker id=6] Leader __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,323] INFO [Broker id=6] Leader __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 6 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:17:57,328] INFO [Broker id=6] Leader __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 6 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:17:57,333] INFO [Broker id=6] Leader __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 6 from offset 0 with partition epoch 14, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:17:57,337] INFO [Broker id=6] Leader __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,341] INFO [Broker id=6] Leader __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 14, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,346] INFO [Broker id=6] Transitioning 18 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:17:57,346] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,346] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 8 from offset 4 with partition epoch 15 and high watermark 4. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,347] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,347] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,347] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,347] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,348] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,348] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,348] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,348] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 8 from offset 3 with partition epoch 15 and high watermark 3. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,349] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,349] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,349] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,350] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,350] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,350] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,350] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,350] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,352] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-16, _schemas-0, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-29, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-39, __consumer_offsets-7, __consumer_offsets-37, __consumer_offsets-5, __consumer_offsets-34, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:17:57,352] INFO [Broker id=6] Stopped fetchers as part of become-follower for 18 partitions (state.change.logger)
[2025-05-20 23:17:57,355] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,356] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), _schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,4), __consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,3), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:17:57,356] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,356] INFO [Broker id=6] Started fetchers as part of become-follower for 18 partitions (state.change.logger)
[2025-05-20 23:17:57,357] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,357] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,357] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,358] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,358] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,358] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,359] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,359] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,359] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,359] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,360] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,360] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,360] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,361] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,361] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,361] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,362] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,362] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,362] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,363] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,363] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,363] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,363] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,364] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,364] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,364] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,364] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,365] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,365] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,365] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,365] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:17:57,367] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 15 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,368] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,369] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 47 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,369] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,369] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 48 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,369] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,370] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 14 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,370] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,371] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 9 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,371] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,371] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 41 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,372] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,372] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 42 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,372] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,372] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 22 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,373] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,373] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 20 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,373] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,373] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 31 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,374] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-15 in 6 milliseconds for epoch 7, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,374] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,375] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-47 in 6 milliseconds for epoch 6, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,375] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 28 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,375] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,375] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-48 in 5 milliseconds for epoch 7, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,375] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 25 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,376] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,376] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-14 in 5 milliseconds for epoch 6, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,376] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 8 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,377] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,377] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-9 in 6 milliseconds for epoch 6, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,377] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 38 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,378] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-41 in 5 milliseconds for epoch 7, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,378] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,378] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-42 in 6 milliseconds for epoch 6, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,378] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 35 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,379] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,379] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 4 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,379] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-22 in 6 milliseconds for epoch 7, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,379] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,380] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-20 in 7 milliseconds for epoch 7, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,380] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 1 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,380] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,380] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-31 in 5 milliseconds for epoch 6, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,381] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,381] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-28 in 6 milliseconds for epoch 7, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,381] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,381] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-25 in 5 milliseconds for epoch 7, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,382] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,382] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,382] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-8 in 5 milliseconds for epoch 6, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,382] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,383] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,383] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-38 in 5 milliseconds for epoch 6, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,383] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,384] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,384] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-35 in 5 milliseconds for epoch 6, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,384] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,385] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,384] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-4 in 4 milliseconds for epoch 7, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,385] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,385] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,385] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-1 in 4 milliseconds for epoch 7, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,386] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,386] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,386] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,386] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,387] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,387] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,387] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,387] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,388] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,388] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,388] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,389] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,388] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,389] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,389] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,389] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,389] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,390] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,390] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,390] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,391] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,390] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,391] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,391] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,391] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,392] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,392] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,392] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,392] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,392] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,393] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,393] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,393] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,394] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,394] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,393] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,394] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,394] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,395] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:22:56,322] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:22:56,822] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:31:57,931] INFO [NodeToControllerChannelManager id=6 name=forwarding] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:31:57,931] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:31:58,479] INFO [Broker id=6] Transitioning 7 partition(s) to local leaders. (state.change.logger)
[2025-05-20 23:31:58,480] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(financial_transactions-0, financial_transactions-17, financial_transactions-18, financial_transactions-3, financial_transactions-7, financial_transactions-10, financial_transactions-12) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:31:58,480] INFO [Broker id=6] Creating new partition financial_transactions-0 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,512] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,515] INFO Created log for partition financial_transactions-0 in /tmp/kafka-logs/financial_transactions-0 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,517] INFO [Partition financial_transactions-0 broker=6] No checkpointed highwatermark is found for partition financial_transactions-0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,517] INFO [Partition financial_transactions-0 broker=6] Log loaded for partition financial_transactions-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,517] INFO [Broker id=6] Leader financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,524] INFO [Broker id=6] Creating new partition financial_transactions-17 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,528] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,529] INFO Created log for partition financial_transactions-17 in /tmp/kafka-logs/financial_transactions-17 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,530] INFO [Partition financial_transactions-17 broker=6] No checkpointed highwatermark is found for partition financial_transactions-17 (kafka.cluster.Partition)
[2025-05-20 23:31:58,530] INFO [Partition financial_transactions-17 broker=6] Log loaded for partition financial_transactions-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,531] INFO [Broker id=6] Leader financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,536] INFO [Broker id=6] Creating new partition financial_transactions-18 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,540] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,541] INFO Created log for partition financial_transactions-18 in /tmp/kafka-logs/financial_transactions-18 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,542] INFO [Partition financial_transactions-18 broker=6] No checkpointed highwatermark is found for partition financial_transactions-18 (kafka.cluster.Partition)
[2025-05-20 23:31:58,542] INFO [Partition financial_transactions-18 broker=6] Log loaded for partition financial_transactions-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,543] INFO [Broker id=6] Leader financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,549] INFO [Broker id=6] Creating new partition financial_transactions-3 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,556] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,557] INFO Created log for partition financial_transactions-3 in /tmp/kafka-logs/financial_transactions-3 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,558] INFO [Partition financial_transactions-3 broker=6] No checkpointed highwatermark is found for partition financial_transactions-3 (kafka.cluster.Partition)
[2025-05-20 23:31:58,558] INFO [Partition financial_transactions-3 broker=6] Log loaded for partition financial_transactions-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,558] INFO [Broker id=6] Leader financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,565] INFO [Broker id=6] Creating new partition financial_transactions-7 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,570] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,572] INFO Created log for partition financial_transactions-7 in /tmp/kafka-logs/financial_transactions-7 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,572] INFO [Partition financial_transactions-7 broker=6] No checkpointed highwatermark is found for partition financial_transactions-7 (kafka.cluster.Partition)
[2025-05-20 23:31:58,572] INFO [Partition financial_transactions-7 broker=6] Log loaded for partition financial_transactions-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,573] INFO [Broker id=6] Leader financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,579] INFO [Broker id=6] Creating new partition financial_transactions-10 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,585] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,586] INFO Created log for partition financial_transactions-10 in /tmp/kafka-logs/financial_transactions-10 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,586] INFO [Partition financial_transactions-10 broker=6] No checkpointed highwatermark is found for partition financial_transactions-10 (kafka.cluster.Partition)
[2025-05-20 23:31:58,587] INFO [Partition financial_transactions-10 broker=6] Log loaded for partition financial_transactions-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,587] INFO [Broker id=6] Leader financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,596] INFO [Broker id=6] Creating new partition financial_transactions-12 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,600] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,602] INFO Created log for partition financial_transactions-12 in /tmp/kafka-logs/financial_transactions-12 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,603] INFO [Partition financial_transactions-12 broker=6] No checkpointed highwatermark is found for partition financial_transactions-12 (kafka.cluster.Partition)
[2025-05-20 23:31:58,604] INFO [Partition financial_transactions-12 broker=6] Log loaded for partition financial_transactions-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,605] INFO [Broker id=6] Leader financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,614] INFO [Broker id=6] Transitioning 13 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:31:58,614] INFO [Broker id=6] Creating new partition financial_transactions-13 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,623] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,627] INFO Created log for partition financial_transactions-13 in /tmp/kafka-logs/financial_transactions-13 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,628] INFO [Partition financial_transactions-13 broker=6] No checkpointed highwatermark is found for partition financial_transactions-13 (kafka.cluster.Partition)
[2025-05-20 23:31:58,629] INFO [Partition financial_transactions-13 broker=6] Log loaded for partition financial_transactions-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,630] INFO [Broker id=6] Follower financial_transactions-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,631] INFO [Broker id=6] Creating new partition financial_transactions-14 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,635] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,636] INFO Created log for partition financial_transactions-14 in /tmp/kafka-logs/financial_transactions-14 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,637] INFO [Partition financial_transactions-14 broker=6] No checkpointed highwatermark is found for partition financial_transactions-14 (kafka.cluster.Partition)
[2025-05-20 23:31:58,637] INFO [Partition financial_transactions-14 broker=6] Log loaded for partition financial_transactions-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,638] INFO [Broker id=6] Follower financial_transactions-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,639] INFO [Broker id=6] Creating new partition financial_transactions-15 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,646] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,647] INFO Created log for partition financial_transactions-15 in /tmp/kafka-logs/financial_transactions-15 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,648] INFO [Partition financial_transactions-15 broker=6] No checkpointed highwatermark is found for partition financial_transactions-15 (kafka.cluster.Partition)
[2025-05-20 23:31:58,649] INFO [Partition financial_transactions-15 broker=6] Log loaded for partition financial_transactions-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,649] INFO [Broker id=6] Follower financial_transactions-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,650] INFO [Broker id=6] Creating new partition financial_transactions-16 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,659] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,660] INFO Created log for partition financial_transactions-16 in /tmp/kafka-logs/financial_transactions-16 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,661] INFO [Partition financial_transactions-16 broker=6] No checkpointed highwatermark is found for partition financial_transactions-16 (kafka.cluster.Partition)
[2025-05-20 23:31:58,662] INFO [Partition financial_transactions-16 broker=6] Log loaded for partition financial_transactions-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,662] INFO [Broker id=6] Follower financial_transactions-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,663] INFO [Broker id=6] Creating new partition financial_transactions-19 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,667] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,668] INFO Created log for partition financial_transactions-19 in /tmp/kafka-logs/financial_transactions-19 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,678] INFO [Partition financial_transactions-19 broker=6] No checkpointed highwatermark is found for partition financial_transactions-19 (kafka.cluster.Partition)
[2025-05-20 23:31:58,679] INFO [Partition financial_transactions-19 broker=6] Log loaded for partition financial_transactions-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,680] INFO [Broker id=6] Follower financial_transactions-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,682] INFO [Broker id=6] Creating new partition financial_transactions-1 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,692] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,694] INFO Created log for partition financial_transactions-1 in /tmp/kafka-logs/financial_transactions-1 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,694] INFO [Partition financial_transactions-1 broker=6] No checkpointed highwatermark is found for partition financial_transactions-1 (kafka.cluster.Partition)
[2025-05-20 23:31:58,695] INFO [Partition financial_transactions-1 broker=6] Log loaded for partition financial_transactions-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,695] INFO [Broker id=6] Follower financial_transactions-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,695] INFO [Broker id=6] Creating new partition financial_transactions-2 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,700] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,702] INFO Created log for partition financial_transactions-2 in /tmp/kafka-logs/financial_transactions-2 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,703] INFO [Partition financial_transactions-2 broker=6] No checkpointed highwatermark is found for partition financial_transactions-2 (kafka.cluster.Partition)
[2025-05-20 23:31:58,704] INFO [Partition financial_transactions-2 broker=6] Log loaded for partition financial_transactions-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,705] INFO [Broker id=6] Follower financial_transactions-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,706] INFO [Broker id=6] Creating new partition financial_transactions-4 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,714] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,715] INFO Created log for partition financial_transactions-4 in /tmp/kafka-logs/financial_transactions-4 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,715] INFO [Partition financial_transactions-4 broker=6] No checkpointed highwatermark is found for partition financial_transactions-4 (kafka.cluster.Partition)
[2025-05-20 23:31:58,716] INFO [Partition financial_transactions-4 broker=6] Log loaded for partition financial_transactions-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,716] INFO [Broker id=6] Follower financial_transactions-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,716] INFO [Broker id=6] Creating new partition financial_transactions-5 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,720] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,722] INFO Created log for partition financial_transactions-5 in /tmp/kafka-logs/financial_transactions-5 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,724] INFO [Partition financial_transactions-5 broker=6] No checkpointed highwatermark is found for partition financial_transactions-5 (kafka.cluster.Partition)
[2025-05-20 23:31:58,725] INFO [Partition financial_transactions-5 broker=6] Log loaded for partition financial_transactions-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,726] INFO [Broker id=6] Follower financial_transactions-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,727] INFO [Broker id=6] Creating new partition financial_transactions-6 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,731] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,732] INFO Created log for partition financial_transactions-6 in /tmp/kafka-logs/financial_transactions-6 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,732] INFO [Partition financial_transactions-6 broker=6] No checkpointed highwatermark is found for partition financial_transactions-6 (kafka.cluster.Partition)
[2025-05-20 23:31:58,733] INFO [Partition financial_transactions-6 broker=6] Log loaded for partition financial_transactions-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,735] INFO [Broker id=6] Follower financial_transactions-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,736] INFO [Broker id=6] Creating new partition financial_transactions-8 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,741] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,743] INFO Created log for partition financial_transactions-8 in /tmp/kafka-logs/financial_transactions-8 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,745] INFO [Partition financial_transactions-8 broker=6] No checkpointed highwatermark is found for partition financial_transactions-8 (kafka.cluster.Partition)
[2025-05-20 23:31:58,745] INFO [Partition financial_transactions-8 broker=6] Log loaded for partition financial_transactions-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,746] INFO [Broker id=6] Follower financial_transactions-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,746] INFO [Broker id=6] Creating new partition financial_transactions-9 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,751] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,752] INFO Created log for partition financial_transactions-9 in /tmp/kafka-logs/financial_transactions-9 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,752] INFO [Partition financial_transactions-9 broker=6] No checkpointed highwatermark is found for partition financial_transactions-9 (kafka.cluster.Partition)
[2025-05-20 23:31:58,753] INFO [Partition financial_transactions-9 broker=6] Log loaded for partition financial_transactions-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,753] INFO [Broker id=6] Follower financial_transactions-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,754] INFO [Broker id=6] Creating new partition financial_transactions-11 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,758] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,759] INFO Created log for partition financial_transactions-11 in /tmp/kafka-logs/financial_transactions-11 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,759] INFO [Partition financial_transactions-11 broker=6] No checkpointed highwatermark is found for partition financial_transactions-11 (kafka.cluster.Partition)
[2025-05-20 23:31:58,760] INFO [Partition financial_transactions-11 broker=6] Log loaded for partition financial_transactions-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,760] INFO [Broker id=6] Follower financial_transactions-11 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,761] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(financial_transactions-13, financial_transactions-14, financial_transactions-15, financial_transactions-16, financial_transactions-19, financial_transactions-1, financial_transactions-2, financial_transactions-4, financial_transactions-5, financial_transactions-6, financial_transactions-8, financial_transactions-9, financial_transactions-11) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:31:58,762] INFO [Broker id=6] Stopped fetchers as part of become-follower for 13 partitions (state.change.logger)
[2025-05-20 23:31:58,763] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(financial_transactions-13 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), financial_transactions-16 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), financial_transactions-1 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), financial_transactions-5 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), financial_transactions-8 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), financial_transactions-9 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:31:58,764] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(financial_transactions-14 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), financial_transactions-15 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), financial_transactions-2 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), financial_transactions-19 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), financial_transactions-4 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), financial_transactions-6 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0), financial_transactions-11 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:31:58,764] INFO [Broker id=6] Started fetchers as part of become-follower for 13 partitions (state.change.logger)
[2025-05-20 23:31:59,082] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,083] INFO [UnifiedLog partition=financial_transactions-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,083] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,084] INFO [UnifiedLog partition=financial_transactions-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,084] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,085] INFO [UnifiedLog partition=financial_transactions-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,085] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,085] INFO [UnifiedLog partition=financial_transactions-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,086] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,086] INFO [UnifiedLog partition=financial_transactions-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,086] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,086] INFO [UnifiedLog partition=financial_transactions-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,087] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,087] INFO [UnifiedLog partition=financial_transactions-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,210] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition financial_transactions-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,211] INFO [UnifiedLog partition=financial_transactions-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,211] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition financial_transactions-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,212] INFO [UnifiedLog partition=financial_transactions-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,212] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition financial_transactions-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,212] INFO [UnifiedLog partition=financial_transactions-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,212] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition financial_transactions-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,213] INFO [UnifiedLog partition=financial_transactions-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,213] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition financial_transactions-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,213] INFO [UnifiedLog partition=financial_transactions-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,213] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition financial_transactions-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,214] INFO [UnifiedLog partition=financial_transactions-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:41:58,110] INFO [NodeToControllerChannelManager id=6 name=forwarding] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,106] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 23:53:45,113] INFO [BrokerServer id=6] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
[2025-05-20 23:53:45,114] INFO [BrokerServer id=6] shutting down (kafka.server.BrokerServer)
[2025-05-20 23:53:45,115] INFO [BrokerLifecycleManager id=6] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:53:45,152] INFO [BrokerLifecycleManager id=6] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:53:45,234] INFO [Broker id=6] Transitioning 71 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:53:45,235] INFO [Broker id=6] Follower financial_transactions-13 starts at leader epoch 3 from offset 53728 with partition epoch 3 and high watermark 53728. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:53:45,236] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,236] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,236] INFO [Broker id=6] Follower financial_transactions-17 starts at leader epoch 1 from offset 53580 with partition epoch 3 and high watermark 53580. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,236] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,237] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,237] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,237] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,237] INFO [Broker id=6] Follower financial_transactions-0 starts at leader epoch 1 from offset 53808 with partition epoch 3 and high watermark 53808. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,238] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,238] INFO [Broker id=6] Follower financial_transactions-4 starts at leader epoch 2 from offset 53258 with partition epoch 3 and high watermark 53258. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,238] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,239] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,239] INFO [Broker id=6] Follower financial_transactions-8 starts at leader epoch 3 from offset 53130 with partition epoch 3 and high watermark 53130. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:53:45,239] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,240] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,240] INFO [Broker id=6] Follower financial_transactions-12 starts at leader epoch 1 from offset 53441 with partition epoch 3 and high watermark 53441. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,241] INFO [BrokerLifecycleManager id=6] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:53:45,241] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,241] INFO [BrokerLifecycleManager id=6] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:45,242] INFO [Broker id=6] Follower financial_transactions-14 starts at leader epoch 2 from offset 53468 with partition epoch 3 and high watermark 53468. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,242] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,243] INFO [BrokerLifecycleManager id=6] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:53:45,243] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 11 from offset 4 with partition epoch 18 and high watermark 4. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,243] INFO [broker-6-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,244] INFO [broker-6-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,244] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,244] INFO [broker-6-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,246] INFO [Broker id=6] Follower financial_transactions-18 starts at leader epoch 1 from offset 53605 with partition epoch 3 and high watermark 53605. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,246] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,246] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,247] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,247] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,247] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,248] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,248] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 11 from offset 4 with partition epoch 18 and high watermark 4. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,248] INFO [Broker id=6] Follower financial_transactions-1 starts at leader epoch 3 from offset 53591 with partition epoch 3 and high watermark 53591. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:53:45,251] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,251] INFO [Broker id=6] Follower financial_transactions-5 starts at leader epoch 2 from offset 53395 with partition epoch 3 and high watermark 53395. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,252] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,252] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,253] INFO [Broker id=6] Follower financial_transactions-9 starts at leader epoch 2 from offset 53169 with partition epoch 3 and high watermark 53169. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,253] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,253] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 23:53:45,254] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,254] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,254] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,255] INFO [Broker id=6] Follower financial_transactions-15 starts at leader epoch 2 from offset 53502 with partition epoch 3 and high watermark 53502. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,255] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,255] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,256] INFO [Broker id=6] Follower financial_transactions-19 starts at leader epoch 2 from offset 53309 with partition epoch 3 and high watermark 53309. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,256] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,256] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,257] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,257] INFO [Broker id=6] Follower financial_transactions-2 starts at leader epoch 2 from offset 53173 with partition epoch 3 and high watermark 53173. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,257] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Node 5 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,257] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,258] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,258] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Cancelled in-flight FETCH request with correlation id 28271 due to node 5 being disconnected (elapsed time since creation: 213ms, elapsed time since send: 213ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,258] INFO [Broker id=6] Follower financial_transactions-6 starts at leader epoch 2 from offset 53206 with partition epoch 3 and high watermark 53206. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,259] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,262] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=519227613, epoch=28271) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 23:53:45,259] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,279] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Node 4 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,265] INFO [SocketServer listenerType=BROKER, nodeId=6] Stopping socket server request processors (kafka.network.SocketServer)
[2025-05-20 23:53:45,282] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Cancelled in-flight FETCH request with correlation id 24153 due to node 4 being disconnected (elapsed time since creation: 265ms, elapsed time since send: 265ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,282] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,283] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Client requested connection close from node 4 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,283] INFO [Broker id=6] Follower financial_transactions-10 starts at leader epoch 1 from offset 53252 with partition epoch 3 and high watermark 53252. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,284] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,284] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,283] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=6, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=519227613, epoch=28271), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 23:53:45,285] INFO [Broker id=6] Follower financial_transactions-16 starts at leader epoch 2 from offset 53208 with partition epoch 3 and high watermark 53208. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,283] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Error sending fetch request (sessionId=766046029, epoch=24153) to node 4: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 4 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 23:53:45,285] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,286] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,287] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=6, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=766046029, epoch=24153), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 4 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 23:53:45,287] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,290] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,291] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,291] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,291] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,292] INFO [Broker id=6] Follower financial_transactions-3 starts at leader epoch 1 from offset 53622 with partition epoch 3 and high watermark 53622. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,292] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,292] INFO [Broker id=6] Follower financial_transactions-7 starts at leader epoch 1 from offset 53438 with partition epoch 3 and high watermark 53438. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,293] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,293] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,294] INFO [Broker id=6] Follower financial_transactions-11 starts at leader epoch 2 from offset 53486 with partition epoch 3 and high watermark 53486. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,294] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,295] INFO [SocketServer listenerType=BROKER, nodeId=6] Stopped socket server request processors (kafka.network.SocketServer)
[2025-05-20 23:53:45,312] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, financial_transactions-18, financial_transactions-12, financial_transactions-11, __consumer_offsets-8, __consumer_offsets-21, financial_transactions-15, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, financial_transactions-4, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, financial_transactions-0, financial_transactions-6, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, financial_transactions-16, __consumer_offsets-15, financial_transactions-10, __consumer_offsets-24, financial_transactions-19, financial_transactions-7, __consumer_offsets-17, financial_transactions-17, financial_transactions-1, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, financial_transactions-14, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, financial_transactions-8, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, financial_transactions-2, financial_transactions-13, __consumer_offsets-32, financial_transactions-5, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:53:45,313] INFO [ReplicaAlterLogDirsManager on broker 6] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, financial_transactions-18, financial_transactions-12, financial_transactions-11, __consumer_offsets-8, __consumer_offsets-21, financial_transactions-15, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, financial_transactions-4, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, financial_transactions-0, financial_transactions-6, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, financial_transactions-16, __consumer_offsets-15, financial_transactions-10, __consumer_offsets-24, financial_transactions-19, financial_transactions-7, __consumer_offsets-17, financial_transactions-17, financial_transactions-1, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, financial_transactions-14, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, financial_transactions-8, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, financial_transactions-2, financial_transactions-13, __consumer_offsets-32, financial_transactions-5, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 23:53:45,323] INFO [Broker id=6] Stopped fetchers as part of controlled shutdown for 71 partitions (state.change.logger)
[2025-05-20 23:53:45,324] INFO [ReplicaFetcherThread-0-4]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:53:45,325] INFO [ReplicaFetcherThread-0-4]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:53:45,325] INFO [ReplicaFetcherThread-0-4]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:53:45,327] INFO [ReplicaFetcherThread-0-5]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:53:45,327] INFO [ReplicaFetcherThread-0-5]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:53:45,327] INFO [ReplicaFetcherThread-0-5]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:53:45,329] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,329] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,329] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,330] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,330] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,330] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,331] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,330] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,331] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,331] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,331] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,332] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,332] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,332] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,333] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,333] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,333] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,334] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,333] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,334] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,334] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,335] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,335] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,336] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,336] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,336] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,337] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,337] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,338] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,338] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,339] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,339] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,340] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,340] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,340] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,340] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,341] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,341] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,341] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,342] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,341] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,342] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,343] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,343] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,343] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,343] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,344] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,344] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,344] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,345] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,345] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,345] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,346] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,346] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,346] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,346] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,347] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,347] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,347] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,348] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,347] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,348] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,349] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,348] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,349] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,350] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,350] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,350] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,351] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,351] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,351] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,352] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,352] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,353] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,353] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,353] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,353] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,354] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,354] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,354] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,354] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,354] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,355] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,355] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,355] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,356] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,357] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,356] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,357] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,357] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,357] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,358] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,358] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,358] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,358] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,359] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,359] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,359] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,359] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,360] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,360] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,361] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,360] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,361] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,361] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,361] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,362] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,362] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,362] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,363] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,362] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,363] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,363] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,364] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,364] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,364] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,364] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,365] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,365] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,365] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,366] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,366] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,367] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,366] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,367] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,368] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,367] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,368] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,368] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,369] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,369] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,369] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,369] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,370] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,370] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,370] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,371] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,371] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,371] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,372] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,372] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,372] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,372] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,373] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,373] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,373] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,374] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,374] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,375] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,376] INFO [data-plane Kafka Request Handler on Broker 6], shutting down (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 23:53:45,378] INFO [data-plane Kafka Request Handler on Broker 6], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 23:53:45,379] INFO [ExpirationReaper-6-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,380] INFO [ExpirationReaper-6-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,380] INFO [ExpirationReaper-6-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,381] INFO [KafkaApi-6] Shutdown complete. (kafka.server.KafkaApis)
[2025-05-20 23:53:45,386] INFO [TransactionCoordinator id=6] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 23:53:45,387] INFO [Transaction State Manager 6]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2025-05-20 23:53:45,387] INFO [TxnMarkerSenderThread-6]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 23:53:45,388] INFO [TxnMarkerSenderThread-6]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 23:53:45,388] INFO [TxnMarkerSenderThread-6]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 23:53:45,390] INFO [TransactionCoordinator id=6] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 23:53:45,391] INFO [GroupCoordinator 6]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,392] INFO [ExpirationReaper-6-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,393] INFO [ExpirationReaper-6-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,393] INFO [ExpirationReaper-6-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,394] INFO [ExpirationReaper-6-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,394] INFO [ExpirationReaper-6-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,394] INFO [ExpirationReaper-6-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,396] INFO [GroupCoordinator 6]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,396] INFO [AssignmentsManager id=6]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:45,397] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,398] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,398] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,399] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 23:53:45,399] INFO [AssignmentsManager id=6]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:45,401] INFO [ReplicaManager broker=6] Shutting down (kafka.server.ReplicaManager)
[2025-05-20 23:53:45,401] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 23:53:45,402] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 23:53:45,402] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 23:53:45,403] INFO [ReplicaFetcherManager on broker 6] shutting down (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:53:45,404] INFO [ReplicaFetcherManager on broker 6] shutdown completed (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:53:45,405] INFO [ReplicaAlterLogDirsManager on broker 6] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 23:53:45,406] INFO [ReplicaAlterLogDirsManager on broker 6] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 23:53:45,406] INFO [ExpirationReaper-6-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,407] INFO [ExpirationReaper-6-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,407] INFO [ExpirationReaper-6-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,408] INFO [ExpirationReaper-6-RemoteFetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,409] INFO [ExpirationReaper-6-RemoteFetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,409] INFO [ExpirationReaper-6-RemoteFetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,410] INFO [ExpirationReaper-6-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,411] INFO [ExpirationReaper-6-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,411] INFO [ExpirationReaper-6-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,411] INFO [ExpirationReaper-6-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,412] INFO [ExpirationReaper-6-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,412] INFO [ExpirationReaper-6-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,413] INFO [ExpirationReaper-6-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,414] INFO [ExpirationReaper-6-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,414] INFO [ExpirationReaper-6-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,424] INFO [AddPartitionsToTxnSenderThread-6]: Shutting down (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 23:53:45,424] INFO [AddPartitionsToTxnSenderThread-6]: Stopped (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 23:53:45,424] INFO [AddPartitionsToTxnSenderThread-6]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 23:53:45,425] INFO [ReplicaManager broker=6] Shut down completely (kafka.server.ReplicaManager)
[2025-05-20 23:53:45,426] INFO [broker-6-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,426] INFO [broker-6-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,426] INFO [broker-6-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,427] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 23:53:45,427] INFO [broker-6-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,428] INFO [broker-6-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,428] INFO [broker-6-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,429] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 23:53:45,429] INFO Shutting down. (kafka.log.LogManager)
[2025-05-20 23:53:45,431] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
[2025-05-20 23:53:45,432] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 23:53:45,432] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 23:53:45,432] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 23:53:45,462] INFO [ProducerStateManager partition=financial_transactions-15] Wrote producer snapshot at offset 53502 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,511] INFO [ProducerStateManager partition=financial_transactions-0] Wrote producer snapshot at offset 53808 with 0 producer ids in 6 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,577] INFO [ProducerStateManager partition=financial_transactions-3] Wrote producer snapshot at offset 53622 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,600] INFO [ProducerStateManager partition=financial_transactions-10] Wrote producer snapshot at offset 53252 with 0 producer ids in 6 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,613] INFO [ProducerStateManager partition=financial_transactions-12] Wrote producer snapshot at offset 53441 with 0 producer ids in 6 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,627] INFO [ProducerStateManager partition=financial_transactions-13] Wrote producer snapshot at offset 53728 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,659] INFO [ProducerStateManager partition=financial_transactions-19] Wrote producer snapshot at offset 53309 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,670] INFO [ProducerStateManager partition=financial_transactions-5] Wrote producer snapshot at offset 53395 with 0 producer ids in 6 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,681] INFO [ProducerStateManager partition=financial_transactions-6] Wrote producer snapshot at offset 53206 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,721] INFO [ProducerStateManager partition=financial_transactions-17] Wrote producer snapshot at offset 53580 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,735] INFO [ProducerStateManager partition=financial_transactions-7] Wrote producer snapshot at offset 53438 with 0 producer ids in 6 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,766] INFO [ProducerStateManager partition=__consumer_offsets-29] Wrote producer snapshot at offset 4 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,824] INFO [ProducerStateManager partition=financial_transactions-4] Wrote producer snapshot at offset 53258 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,867] INFO [ProducerStateManager partition=financial_transactions-18] Wrote producer snapshot at offset 53605 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,885] INFO [ProducerStateManager partition=financial_transactions-9] Wrote producer snapshot at offset 53169 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,895] INFO [ProducerStateManager partition=financial_transactions-1] Wrote producer snapshot at offset 53591 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,902] INFO [ProducerStateManager partition=financial_transactions-14] Wrote producer snapshot at offset 53468 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,932] INFO [ProducerStateManager partition=financial_transactions-11] Wrote producer snapshot at offset 53486 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,961] INFO [ProducerStateManager partition=_schemas-0] Wrote producer snapshot at offset 4 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,983] INFO [ProducerStateManager partition=financial_transactions-8] Wrote producer snapshot at offset 53130 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:46,007] INFO [ProducerStateManager partition=financial_transactions-2] Wrote producer snapshot at offset 53173 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:46,027] INFO [ProducerStateManager partition=financial_transactions-16] Wrote producer snapshot at offset 53208 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:46,110] INFO Shutdown complete. (kafka.log.LogManager)
[2025-05-20 23:53:46,110] INFO [broker-6-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,111] INFO [broker-6-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,111] INFO [broker-6-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,112] INFO [broker-6-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,112] INFO [broker-6-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,112] INFO [broker-6-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,113] INFO [broker-6-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,113] INFO [broker-6-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,113] INFO [broker-6-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,113] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,114] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,114] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,115] INFO [SocketServer listenerType=BROKER, nodeId=6] Shutting down socket server (kafka.network.SocketServer)
[2025-05-20 23:53:46,124] INFO [SocketServer listenerType=BROKER, nodeId=6] Shutdown completed (kafka.network.SocketServer)
[2025-05-20 23:53:46,125] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
[2025-05-20 23:53:46,125] INFO [BrokerLifecycleManager id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:46,126] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 23:53:46,128] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 23:53:46,128] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 23:53:46,129] INFO [SharedServer id=6] Stopping SharedServer (kafka.server.SharedServer)
[2025-05-20 23:53:46,131] INFO [MetadataLoader id=6] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:46,131] INFO [SnapshotGenerator id=6] close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:46,132] INFO [SnapshotGenerator id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:46,133] INFO [MetadataLoader id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:46,134] INFO [SnapshotGenerator id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:46,136] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 23:53:46,261] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 23:53:46,261] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 23:53:46,263] INFO [kafka-6-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 23:53:46,263] INFO [RaftManager id=6] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 23:53:46,263] INFO [RaftManager id=6] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 23:53:46,264] INFO [RaftManager id=6] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 23:53:46,264] INFO [kafka-6-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 23:53:46,264] INFO [kafka-6-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 23:53:46,269] INFO [kafka-6-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 23:53:46,270] INFO [kafka-6-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 23:53:46,270] INFO [kafka-6-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 23:53:46,274] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 8082 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:46,277] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 23:53:46,277] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 23:53:46,277] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 23:53:46,278] INFO App info kafka.server for 6 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 23:53:46,278] INFO [BrokerServer id=6] shut down completed (kafka.server.BrokerServer)
[2025-05-20 23:53:46,279] INFO [BrokerServer id=6] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
[2025-05-20 23:53:46,279] INFO App info kafka.server for 6 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 12:15:19,097] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-21 12:15:19,429] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-21 12:15:19,446] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-21 12:15:19,460] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:24,284] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-21 12:15:24,468] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-21 12:15:24,475] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:24,699] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:24,713] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:24,756] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-21 12:15:24,764] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-21 12:15:24,769] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2025-05-21 12:15:24,777] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:24,915] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
[2025-05-21 12:15:24,917] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:24,922] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:24,925] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000002740.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 12:15:24,926] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000008082.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 12:15:24,927] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:25,344] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 8082 with 0 producer ids in 23 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:25,385] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 8082 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:25,386] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 8082 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:25,387] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=8082, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000008082.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:25,395] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 8ms for snapshot load and 0ms for segment recovery from offset 8082 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:25,433] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-21 12:15:25,615] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 12:15:25,622] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 12:15:25,928] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 12:15:26,043] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=16, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from null (org.apache.kafka.raft.QuorumState)
[2025-05-21 12:15:26,052] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 12:15:26,056] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 12:15:26,141] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:26,153] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2025-05-21 12:15:26,239] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:26,253] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:26,309] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:15:26,321] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:15:26,335] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:15:26,370] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:26,371] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@643709918 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 12:15:26,372] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:15:26,377] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,383] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,476] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:26,485] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-21 12:15:26,490] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-21 12:15:26,553] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:26,571] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,582] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,587] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:26,589] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,590] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,592] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,594] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,601] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:26,605] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 12:15:26,684] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,685] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,687] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,688] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,688] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:26,705] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,980] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,998] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,002] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,008] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,038] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,039] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,045] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,054] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,121] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,241] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,260] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,260] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,279] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,283] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,352] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,455] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,540] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,543] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,571] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,678] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,735] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-21 12:15:27,777] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-21 12:15:27,801] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-21 12:15:27,781] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,816] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-21 12:15:27,872] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:27,898] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:27,884] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:27,912] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,943] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:27,971] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,024] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,031] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,046] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,048] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,057] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,061] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,061] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,064] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,067] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,084] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,101] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,123] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,126] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,154] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,256] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,352] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,358] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,357] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,353] INFO [BrokerLifecycleManager id=6] Incarnation juRUyaWUSJOV5iq_RGwdWw of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:15:28,371] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,382] WARN [NodeToControllerChannelManager id=6 name=heartbeat] Connection to node 2 (kafka-controller-2/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,384] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,424] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,431] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,461] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,552] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,571] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,571] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-21 12:15:28,583] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-21 12:15:28,581] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,590] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-21 12:15:28,598] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,604] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,651] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,661] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,678] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,678] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,678] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,680] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,741] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,771] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,784] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,787] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,838] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,872] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,894] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,895] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,946] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,957] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,958] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,978] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,012] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,027] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,028] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,079] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,080] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,095] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,096] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,147] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,177] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,178] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,180] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,227] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,232] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,233] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,253] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,254] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,284] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,312] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,325] INFO [RaftManager id=6] Completed transition to Unattached(epoch=17, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=16, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-21 12:15:29,351] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,386] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,490] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,599] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,702] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,784] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=17, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=17, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-21 12:15:29,803] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,853] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=8087, metadata=Optional.empty)] for the first time for epoch 17 (org.apache.kafka.raft.FollowerState)
[2025-05-21 12:15:29,862] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 8087 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,863] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:30,175] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 8087 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,211] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 8086 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,221] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 8086 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,237] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 8086 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,248] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=8086, epoch=17) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-21 12:15:30,257] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:30,226] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:30,281] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:30,332] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:30,336] INFO Skipping recovery of 71 logs from /tmp/kafka-logs since clean shutdown file was found (kafka.log.LogManager)
[2025-05-21 12:15:30,427] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 8091 (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:15:30,428] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Loading producer state till offset 53728 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,429] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53728 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,430] INFO [ProducerStateManager partition=financial_transactions-13] Loading producer state from snapshot file 'SnapshotFile(offset=53728, file=/tmp/kafka-logs/financial_transactions-13/00000000000000053728.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:30,436] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Producer state recovery took 6ms for snapshot load and 0ms for segment recovery from offset 53728 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,664] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-13, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53728) with 1 segments, local-log-start-offset 0 and log-end-offset 53728 in 261ms (1/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:30,727] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Loading producer state till offset 53605 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,728] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53605 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,728] INFO [ProducerStateManager partition=financial_transactions-18] Loading producer state from snapshot file 'SnapshotFile(offset=53605, file=/tmp/kafka-logs/financial_transactions-18/00000000000000053605.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:30,731] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 0ms for segment recovery from offset 53605 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,747] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-18, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53605) with 1 segments, local-log-start-offset 0 and log-end-offset 53605 in 56ms (2/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:30,782] INFO Deleted producer state snapshot /tmp/kafka-logs/__consumer_offsets-29/00000000000000000002.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 12:15:30,782] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 4 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,801] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 4 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,805] INFO [ProducerStateManager partition=__consumer_offsets-29] Loading producer state from snapshot file 'SnapshotFile(offset=4, file=/tmp/kafka-logs/__consumer_offsets-29/00000000000000000004.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:30,814] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Producer state recovery took 9ms for snapshot load and 0ms for segment recovery from offset 4 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,831] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-29, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=29, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=4) with 1 segments, local-log-start-offset 0 and log-end-offset 4 in 82ms (3/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:30,837] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Loading producer state till offset 53441 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,838] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53441 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,838] INFO [ProducerStateManager partition=financial_transactions-12] Loading producer state from snapshot file 'SnapshotFile(offset=53441, file=/tmp/kafka-logs/financial_transactions-12/00000000000000053441.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:30,840] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 53441 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,851] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-12, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53441) with 1 segments, local-log-start-offset 0 and log-end-offset 53441 in 18ms (4/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:30,869] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,884] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-38, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=38, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 33ms (5/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:30,903] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Loading producer state till offset 53208 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,903] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53208 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,906] INFO [ProducerStateManager partition=financial_transactions-16] Loading producer state from snapshot file 'SnapshotFile(offset=53208, file=/tmp/kafka-logs/financial_transactions-16/00000000000000053208.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:30,911] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Producer state recovery took 5ms for snapshot load and 0ms for segment recovery from offset 53208 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,938] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-16, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53208) with 1 segments, local-log-start-offset 0 and log-end-offset 53208 in 53ms (6/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:30,944] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,952] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-27, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=27, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (7/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:30,959] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,965] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-39, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=39, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (8/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:30,978] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,985] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-42, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=42, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (9/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,010] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,018] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-9, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 33ms (10/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,034] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,047] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-22, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=22, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 28ms (11/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,097] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,102] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-26, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=26, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 54ms (12/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,130] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,148] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-40, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=40, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 46ms (13/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,174] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,184] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-47, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=47, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 31ms (14/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,199] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,217] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-15, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 26ms (15/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,234] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,236] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-10, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (16/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,274] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Loading producer state till offset 53486 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,283] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53486 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,285] INFO [ProducerStateManager partition=financial_transactions-11] Loading producer state from snapshot file 'SnapshotFile(offset=53486, file=/tmp/kafka-logs/financial_transactions-11/00000000000000053486.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,287] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 53486 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,294] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-11, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53486) with 1 segments, local-log-start-offset 0 and log-end-offset 53486 in 44ms (17/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,308] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,318] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-2, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (18/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,327] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,335] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-18, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (19/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,356] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,371] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-46, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=46, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 30ms (20/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,402] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Loading producer state till offset 53591 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,403] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53591 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,403] INFO [ProducerStateManager partition=financial_transactions-1] Loading producer state from snapshot file 'SnapshotFile(offset=53591, file=/tmp/kafka-logs/financial_transactions-1/00000000000000053591.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,404] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53591 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,419] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-1, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53591) with 1 segments, local-log-start-offset 0 and log-end-offset 53591 in 47ms (21/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,427] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,441] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-12, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 20ms (22/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,446] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,451] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-23, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=23, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (23/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,471] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,496] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-14, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 44ms (24/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,500] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,503] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-5, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (25/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,516] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,520] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-41, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=41, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 17ms (26/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,541] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Loading producer state till offset 53580 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,541] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53580 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,545] INFO [ProducerStateManager partition=financial_transactions-17] Loading producer state from snapshot file 'SnapshotFile(offset=53580, file=/tmp/kafka-logs/financial_transactions-17/00000000000000053580.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,549] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Producer state recovery took 4ms for snapshot load and 0ms for segment recovery from offset 53580 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,553] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-17, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53580) with 1 segments, local-log-start-offset 0 and log-end-offset 53580 in 29ms (27/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,558] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,561] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-1, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (28/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,674] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,683] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-19, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (29/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,691] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,695] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-3, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (30/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,701] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,708] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-16, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (31/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,713] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,716] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-25, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=25, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (32/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,726] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Loading producer state till offset 53502 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,727] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53502 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,732] INFO [ProducerStateManager partition=financial_transactions-15] Loading producer state from snapshot file 'SnapshotFile(offset=53502, file=/tmp/kafka-logs/financial_transactions-15/00000000000000053502.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,737] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Producer state recovery took 5ms for snapshot load and 0ms for segment recovery from offset 53502 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,742] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-15, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53502) with 1 segments, local-log-start-offset 0 and log-end-offset 53502 in 25ms (33/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,767] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Loading producer state till offset 53622 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,767] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53622 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,768] INFO [ProducerStateManager partition=financial_transactions-3] Loading producer state from snapshot file 'SnapshotFile(offset=53622, file=/tmp/kafka-logs/financial_transactions-3/00000000000000053622.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,776] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Producer state recovery took 8ms for snapshot load and 0ms for segment recovery from offset 53622 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,779] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-3, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53622) with 1 segments, local-log-start-offset 0 and log-end-offset 53622 in 21ms (34/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,797] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Loading producer state till offset 53252 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,798] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53252 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,798] INFO [ProducerStateManager partition=financial_transactions-10] Loading producer state from snapshot file 'SnapshotFile(offset=53252, file=/tmp/kafka-logs/financial_transactions-10/00000000000000053252.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,800] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 53252 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,806] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-10, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53252) with 1 segments, local-log-start-offset 0 and log-end-offset 53252 in 19ms (35/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,815] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Loading producer state till offset 53309 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,816] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53309 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,817] INFO [ProducerStateManager partition=financial_transactions-19] Loading producer state from snapshot file 'SnapshotFile(offset=53309, file=/tmp/kafka-logs/financial_transactions-19/00000000000000053309.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,818] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53309 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,830] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-19, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53309) with 1 segments, local-log-start-offset 0 and log-end-offset 53309 in 22ms (36/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,840] INFO Deleted producer state snapshot /tmp/kafka-logs/_schemas-0/00000000000000000002.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 12:15:31,840] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 4 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,840] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 4 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,845] INFO [ProducerStateManager partition=_schemas-0] Loading producer state from snapshot file 'SnapshotFile(offset=4, file=/tmp/kafka-logs/_schemas-0/00000000000000000004.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,850] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Producer state recovery took 4ms for snapshot load and 1ms for segment recovery from offset 4 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,853] INFO Completed load of Log(dir=/tmp/kafka-logs/_schemas-0, topicId=RrE8eovWRKu4kLR3MRJ0fA, topic=_schemas, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=4) with 1 segments, local-log-start-offset 0 and log-end-offset 4 in 23ms (37/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,877] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,884] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-28, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=28, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 31ms (38/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,897] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,898] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-13, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (39/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,901] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,905] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-24, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=24, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (40/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,911] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,916] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-32, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=32, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (41/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,928] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Loading producer state till offset 53468 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,930] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53468 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,930] INFO [ProducerStateManager partition=financial_transactions-14] Loading producer state from snapshot file 'SnapshotFile(offset=53468, file=/tmp/kafka-logs/financial_transactions-14/00000000000000053468.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,931] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53468 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,933] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-14, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53468) with 1 segments, local-log-start-offset 0 and log-end-offset 53468 in 17ms (42/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,940] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Loading producer state till offset 53173 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,941] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53173 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,944] INFO [ProducerStateManager partition=financial_transactions-2] Loading producer state from snapshot file 'SnapshotFile(offset=53173, file=/tmp/kafka-logs/financial_transactions-2/00000000000000053173.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,948] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Producer state recovery took 4ms for snapshot load and 0ms for segment recovery from offset 53173 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,954] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-2, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53173) with 1 segments, local-log-start-offset 0 and log-end-offset 53173 in 19ms (43/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,960] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,967] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-31, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=31, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (44/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,970] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,975] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-21, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=21, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (45/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,989] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 53808 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,990] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53808 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,990] INFO [ProducerStateManager partition=financial_transactions-0] Loading producer state from snapshot file 'SnapshotFile(offset=53808, file=/tmp/kafka-logs/financial_transactions-0/00000000000000053808.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,991] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53808 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,994] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-0, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53808) with 1 segments, local-log-start-offset 0 and log-end-offset 53808 in 17ms (46/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,004] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Loading producer state till offset 53169 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,004] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53169 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,005] INFO [ProducerStateManager partition=financial_transactions-9] Loading producer state from snapshot file 'SnapshotFile(offset=53169, file=/tmp/kafka-logs/financial_transactions-9/00000000000000053169.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:32,007] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 53169 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,009] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-9, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53169) with 1 segments, local-log-start-offset 0 and log-end-offset 53169 in 14ms (47/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,018] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,023] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-6, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (48/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,030] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,032] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-11, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (49/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,037] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Loading producer state till offset 53395 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,037] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53395 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,038] INFO [ProducerStateManager partition=financial_transactions-5] Loading producer state from snapshot file 'SnapshotFile(offset=53395, file=/tmp/kafka-logs/financial_transactions-5/00000000000000053395.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:32,039] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53395 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,041] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-5, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53395) with 1 segments, local-log-start-offset 0 and log-end-offset 53395 in 8ms (50/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,044] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,046] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-30, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=30, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (51/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,052] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Loading producer state till offset 53206 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,053] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53206 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,054] INFO [ProducerStateManager partition=financial_transactions-6] Loading producer state from snapshot file 'SnapshotFile(offset=53206, file=/tmp/kafka-logs/financial_transactions-6/00000000000000053206.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:32,055] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53206 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,057] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-6, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53206) with 1 segments, local-log-start-offset 0 and log-end-offset 53206 in 11ms (52/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,063] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,066] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-43, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=43, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (53/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,069] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,074] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-7, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (54/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,082] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,084] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-33, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=33, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (55/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,087] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,089] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-36, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=36, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (56/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,092] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,093] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-45, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=45, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (57/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,099] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,102] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-0, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (58/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,109] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,111] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-20, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=20, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (59/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,122] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,125] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-4, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (60/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,130] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,132] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-8, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (61/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,140] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,142] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-49, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=49, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (62/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,146] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,149] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-34, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=34, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (63/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,164] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,165] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-48, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=48, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (64/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,172] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Loading producer state till offset 53258 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,172] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53258 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,173] INFO [ProducerStateManager partition=financial_transactions-4] Loading producer state from snapshot file 'SnapshotFile(offset=53258, file=/tmp/kafka-logs/financial_transactions-4/00000000000000053258.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:32,173] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 53258 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,180] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-4, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53258) with 1 segments, local-log-start-offset 0 and log-end-offset 53258 in 14ms (65/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,192] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,194] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-44, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=44, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (66/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,203] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,206] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-37, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=37, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (67/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,212] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Loading producer state till offset 53438 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,212] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53438 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,212] INFO [ProducerStateManager partition=financial_transactions-7] Loading producer state from snapshot file 'SnapshotFile(offset=53438, file=/tmp/kafka-logs/financial_transactions-7/00000000000000053438.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:32,213] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53438 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,215] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-7, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53438) with 1 segments, local-log-start-offset 0 and log-end-offset 53438 in 9ms (68/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,220] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,223] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-35, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=35, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (69/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,228] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,230] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-17, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (70/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,234] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Loading producer state till offset 53130 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,234] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53130 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,235] INFO [ProducerStateManager partition=financial_transactions-8] Loading producer state from snapshot file 'SnapshotFile(offset=53130, file=/tmp/kafka-logs/financial_transactions-8/00000000000000053130.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:32,235] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 53130 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,241] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-8, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53130) with 1 segments, local-log-start-offset 0 and log-end-offset 53130 in 11ms (71/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,249] INFO Loaded 71 logs in 1972ms (kafka.log.LogManager)
[2025-05-21 12:15:32,250] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-21 12:15:32,252] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-21 12:15:32,261] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-21 12:15:32,406] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 12:15:32,413] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 12:15:32,418] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 12:15:32,420] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,431] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,433] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 12:15:32,458] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 12:15:32,462] INFO [Broker id=6] Transitioning 71 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:32,464] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 12:15:32,465] INFO [Broker id=6] Creating new partition financial_transactions-13 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,496] INFO [Partition financial_transactions-13 broker=6] Log loaded for partition financial_transactions-13 with initial high watermark 53728 (kafka.cluster.Partition)
[2025-05-21 12:15:32,500] INFO [Broker id=6] Follower financial_transactions-13 starts at leader epoch 3 from offset 53728 with partition epoch 3 and high watermark 53728. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:32,501] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,505] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,506] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,507] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,511] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,511] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,512] INFO [Broker id=6] Creating new partition financial_transactions-17 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,513] INFO [Partition financial_transactions-17 broker=6] Log loaded for partition financial_transactions-17 with initial high watermark 53580 (kafka.cluster.Partition)
[2025-05-21 12:15:32,515] INFO [Broker id=6] Follower financial_transactions-17 starts at leader epoch 1 from offset 53580 with partition epoch 3 and high watermark 53580. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,516] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,518] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,519] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,520] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,521] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,523] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,524] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,526] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,526] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,527] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,527] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,528] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,531] INFO [Broker id=6] Creating new partition financial_transactions-0 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,532] INFO [Partition financial_transactions-0 broker=6] Log loaded for partition financial_transactions-0 with initial high watermark 53808 (kafka.cluster.Partition)
[2025-05-21 12:15:32,533] INFO [Broker id=6] Follower financial_transactions-0 starts at leader epoch 1 from offset 53808 with partition epoch 3 and high watermark 53808. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,533] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,534] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,534] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,534] INFO [Broker id=6] Creating new partition financial_transactions-4 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,536] INFO [Partition financial_transactions-4 broker=6] Log loaded for partition financial_transactions-4 with initial high watermark 53258 (kafka.cluster.Partition)
[2025-05-21 12:15:32,536] INFO [Broker id=6] Follower financial_transactions-4 starts at leader epoch 2 from offset 53258 with partition epoch 3 and high watermark 53258. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,538] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,542] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,543] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,544] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,545] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,546] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,546] INFO [Broker id=6] Creating new partition financial_transactions-8 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,547] INFO [Partition financial_transactions-8 broker=6] Log loaded for partition financial_transactions-8 with initial high watermark 53130 (kafka.cluster.Partition)
[2025-05-21 12:15:32,548] INFO [Broker id=6] Follower financial_transactions-8 starts at leader epoch 3 from offset 53130 with partition epoch 3 and high watermark 53130. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:32,548] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,549] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,551] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,551] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,553] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,554] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,555] INFO [Broker id=6] Creating new partition financial_transactions-12 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,558] INFO [Partition financial_transactions-12 broker=6] Log loaded for partition financial_transactions-12 with initial high watermark 53441 (kafka.cluster.Partition)
[2025-05-21 12:15:32,559] INFO [Broker id=6] Follower financial_transactions-12 starts at leader epoch 1 from offset 53441 with partition epoch 3 and high watermark 53441. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,559] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,561] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,561] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,561] INFO [Broker id=6] Creating new partition financial_transactions-14 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,562] INFO [Partition financial_transactions-14 broker=6] Log loaded for partition financial_transactions-14 with initial high watermark 53468 (kafka.cluster.Partition)
[2025-05-21 12:15:32,564] INFO [Broker id=6] Follower financial_transactions-14 starts at leader epoch 2 from offset 53468 with partition epoch 3 and high watermark 53468. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,568] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,570] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,571] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,571] INFO [Broker id=6] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-21 12:15:32,573] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 4 (kafka.cluster.Partition)
[2025-05-21 12:15:32,574] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 11 from offset 4 with partition epoch 18 and high watermark 4. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,576] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,577] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,578] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,578] INFO [Broker id=6] Creating new partition financial_transactions-18 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,580] INFO [Partition financial_transactions-18 broker=6] Log loaded for partition financial_transactions-18 with initial high watermark 53605 (kafka.cluster.Partition)
[2025-05-21 12:15:32,580] INFO [Broker id=6] Follower financial_transactions-18 starts at leader epoch 1 from offset 53605 with partition epoch 3 and high watermark 53605. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,581] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,583] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,584] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,584] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,585] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,586] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,586] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,587] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,588] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,588] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,594] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,594] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,595] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,596] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,596] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,597] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,599] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,599] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,600] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,601] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 4 (kafka.cluster.Partition)
[2025-05-21 12:15:32,601] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 11 from offset 4 with partition epoch 18 and high watermark 4. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,601] INFO [Broker id=6] Creating new partition financial_transactions-1 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,602] INFO [Partition financial_transactions-1 broker=6] Log loaded for partition financial_transactions-1 with initial high watermark 53591 (kafka.cluster.Partition)
[2025-05-21 12:15:32,603] INFO [Broker id=6] Follower financial_transactions-1 starts at leader epoch 3 from offset 53591 with partition epoch 3 and high watermark 53591. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:32,604] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,607] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,607] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,607] INFO [Broker id=6] Creating new partition financial_transactions-5 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,609] INFO [Partition financial_transactions-5 broker=6] Log loaded for partition financial_transactions-5 with initial high watermark 53395 (kafka.cluster.Partition)
[2025-05-21 12:15:32,609] INFO [Broker id=6] Follower financial_transactions-5 starts at leader epoch 2 from offset 53395 with partition epoch 3 and high watermark 53395. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,610] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,611] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,611] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,612] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,613] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,614] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,614] INFO [Broker id=6] Creating new partition financial_transactions-9 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,616] INFO [Partition financial_transactions-9 broker=6] Log loaded for partition financial_transactions-9 with initial high watermark 53169 (kafka.cluster.Partition)
[2025-05-21 12:15:32,617] INFO [Broker id=6] Follower financial_transactions-9 starts at leader epoch 2 from offset 53169 with partition epoch 3 and high watermark 53169. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,618] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,622] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,623] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,623] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,624] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,624] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,626] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,629] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,630] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,632] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,633] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,634] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,634] INFO [Broker id=6] Creating new partition financial_transactions-15 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,635] INFO [Partition financial_transactions-15 broker=6] Log loaded for partition financial_transactions-15 with initial high watermark 53502 (kafka.cluster.Partition)
[2025-05-21 12:15:32,636] INFO [Broker id=6] Follower financial_transactions-15 starts at leader epoch 2 from offset 53502 with partition epoch 3 and high watermark 53502. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,636] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,637] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,637] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,637] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,640] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,642] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,642] INFO [Broker id=6] Creating new partition financial_transactions-19 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,643] INFO [Partition financial_transactions-19 broker=6] Log loaded for partition financial_transactions-19 with initial high watermark 53309 (kafka.cluster.Partition)
[2025-05-21 12:15:32,644] INFO [Broker id=6] Follower financial_transactions-19 starts at leader epoch 2 from offset 53309 with partition epoch 3 and high watermark 53309. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,644] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,645] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,645] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,646] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,647] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,648] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,648] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,649] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,650] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,650] INFO [Broker id=6] Creating new partition financial_transactions-2 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,651] INFO [Partition financial_transactions-2 broker=6] Log loaded for partition financial_transactions-2 with initial high watermark 53173 (kafka.cluster.Partition)
[2025-05-21 12:15:32,651] INFO [Broker id=6] Follower financial_transactions-2 starts at leader epoch 2 from offset 53173 with partition epoch 3 and high watermark 53173. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,651] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,652] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,653] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,655] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,656] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,657] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,657] INFO [Broker id=6] Creating new partition financial_transactions-6 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,658] INFO [Partition financial_transactions-6 broker=6] Log loaded for partition financial_transactions-6 with initial high watermark 53206 (kafka.cluster.Partition)
[2025-05-21 12:15:32,659] INFO [Broker id=6] Follower financial_transactions-6 starts at leader epoch 2 from offset 53206 with partition epoch 3 and high watermark 53206. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,659] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,662] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,662] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,663] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,664] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,664] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,665] INFO [Broker id=6] Creating new partition financial_transactions-10 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,666] INFO [Partition financial_transactions-10 broker=6] Log loaded for partition financial_transactions-10 with initial high watermark 53252 (kafka.cluster.Partition)
[2025-05-21 12:15:32,667] INFO [Broker id=6] Follower financial_transactions-10 starts at leader epoch 1 from offset 53252 with partition epoch 3 and high watermark 53252. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,668] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,669] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,670] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,670] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,673] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,674] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,675] INFO [Broker id=6] Creating new partition financial_transactions-16 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,676] INFO [Partition financial_transactions-16 broker=6] Log loaded for partition financial_transactions-16 with initial high watermark 53208 (kafka.cluster.Partition)
[2025-05-21 12:15:32,677] INFO [Broker id=6] Follower financial_transactions-16 starts at leader epoch 2 from offset 53208 with partition epoch 3 and high watermark 53208. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,677] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,679] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,679] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,680] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,681] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,681] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,681] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,683] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,683] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,683] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,685] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,687] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,687] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,688] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,689] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,690] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,692] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,692] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,693] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,696] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,701] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,701] INFO [Broker id=6] Creating new partition financial_transactions-3 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,702] INFO [Partition financial_transactions-3 broker=6] Log loaded for partition financial_transactions-3 with initial high watermark 53622 (kafka.cluster.Partition)
[2025-05-21 12:15:32,703] INFO [Broker id=6] Follower financial_transactions-3 starts at leader epoch 1 from offset 53622 with partition epoch 3 and high watermark 53622. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,703] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,704] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,705] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,705] INFO [Broker id=6] Creating new partition financial_transactions-7 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,706] INFO [Partition financial_transactions-7 broker=6] Log loaded for partition financial_transactions-7 with initial high watermark 53438 (kafka.cluster.Partition)
[2025-05-21 12:15:32,706] INFO [Broker id=6] Follower financial_transactions-7 starts at leader epoch 1 from offset 53438 with partition epoch 3 and high watermark 53438. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,707] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,708] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,708] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,708] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,709] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,710] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,710] INFO [Broker id=6] Creating new partition financial_transactions-11 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,712] INFO [Partition financial_transactions-11 broker=6] Log loaded for partition financial_transactions-11 with initial high watermark 53486 (kafka.cluster.Partition)
[2025-05-21 12:15:32,712] INFO [Broker id=6] Follower financial_transactions-11 starts at leader epoch 2 from offset 53486 with partition epoch 3 and high watermark 53486. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,713] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,714] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,714] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,716] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-6, __consumer_offsets-35, financial_transactions-11, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:15:32,717] INFO [Broker id=6] Stopped fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-21 12:15:32,725] INFO [Broker id=6] Started fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-21 12:15:32,738] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,748] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,751] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,753] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,754] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,754] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,755] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,755] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,755] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,755] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,756] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,759] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,759] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,760] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,760] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,760] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,760] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,761] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,761] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,761] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,761] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,762] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,762] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,762] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,762] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,760] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,763] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,764] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,764] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,764] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,764] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,764] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,765] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,765] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,765] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,766] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,766] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,766] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,767] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,767] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,767] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,768] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,768] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,768] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,769] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,770] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,770] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,771] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,770] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,773] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,774] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,774] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,773] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,784] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,785] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,785] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,786] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,784] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,789] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,789] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,789] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,790] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,790] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,790] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,791] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,791] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,791] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,792] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,792] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,793] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,794] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,795] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,795] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,796] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,797] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,797] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,798] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,798] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,799] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,799] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,799] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,798] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,800] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,801] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,800] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,802] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,803] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,803] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,803] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,804] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,804] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,804] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,805] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,805] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,805] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,806] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,806] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,806] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,806] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,807] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,807] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,809] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,809] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,806] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,815] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,816] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,817] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,817] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,816] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,818] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,818] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,819] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,819] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,819] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,820] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,819] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,821] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,821] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,821] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,822] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,822] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,823] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,822] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,823] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,823] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,823] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,824] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,824] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,824] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,825] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,825] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,825] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,825] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,824] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,826] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,826] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,827] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,827] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,827] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,826] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,828] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,828] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,828] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,829] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,830] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,830] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,832] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,832] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,833] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,834] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,839] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-21 12:15:32,846] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-21 12:15:32,856] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 8086 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:32,867] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:15:32,871] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-21 12:15:32,874] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-21 12:15:32,875] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-21 12:15:32,878] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-21 12:15:32,879] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:32,883] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-21 12:15:32,922] INFO [BrokerLifecycleManager id=6] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:15:32,984] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:15:32,984] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-21 12:15:32,986] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-21 12:15:32,987] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-21 12:15:32,987] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2025-05-21 12:15:32,990] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-21 12:15:32,993] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-21 12:15:32,997] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-21 12:15:32,997] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-21 12:15:32,998] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-21 12:15:32,998] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-21 12:15:32,998] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-21 12:15:32,998] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 12:15:32,999] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 12:15:32,999] INFO Kafka startTimeMs: 1747829732998 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 12:15:33,000] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-21 12:15:33,334] INFO [Broker id=6] Transitioning 71 partition(s) to local leaders. (state.change.logger)
[2025-05-21 12:15:33,335] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-6, __consumer_offsets-35, financial_transactions-11, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:15:33,352] INFO [Broker id=6] Leader financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 4 from offset 53728 with partition epoch 4, high watermark 53728, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,365] INFO [Broker id=6] Leader __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 11 from offset 0 with partition epoch 19, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:33,372] INFO [Broker id=6] Leader __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 11 from offset 0 with partition epoch 19, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:33,384] INFO [Broker id=6] Leader financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 2 from offset 53580 with partition epoch 4, high watermark 53580, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:33,391] INFO [Broker id=6] Leader __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,400] INFO [Broker id=6] Leader __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,411] INFO [Broker id=6] Leader __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:33,423] INFO [Broker id=6] Leader __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:33,439] INFO [Broker id=6] Leader financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 2 from offset 53808 with partition epoch 4, high watermark 53808, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:33,444] INFO [Broker id=6] Leader __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 11 from offset 0 with partition epoch 19, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:33,449] INFO [Broker id=6] Leader financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 3 from offset 53258 with partition epoch 4, high watermark 53258, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,453] INFO [Broker id=6] Leader __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 19, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,457] INFO [Broker id=6] Leader __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 19, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,461] INFO [Broker id=6] Leader financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 4 from offset 53130 with partition epoch 4, high watermark 53130, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,466] INFO [Broker id=6] Leader __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,470] INFO [Broker id=6] Leader __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 9 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,475] INFO [Broker id=6] Leader financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 2 from offset 53441 with partition epoch 4, high watermark 53441, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:33,480] INFO [Broker id=6] Leader __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 11 from offset 0 with partition epoch 19, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:33,485] INFO [Broker id=6] Leader financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 3 from offset 53468 with partition epoch 4, high watermark 53468, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,489] INFO [Broker id=6] Leader __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 19, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,493] INFO [Broker id=6] Leader _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) starts at leader epoch 12 from offset 4 with partition epoch 19, high watermark 4, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,499] INFO [Broker id=6] Leader __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:33,504] INFO [Broker id=6] Leader financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 2 from offset 53605 with partition epoch 4, high watermark 53605, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:33,509] INFO [Broker id=6] Leader __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:33,513] INFO [Broker id=6] Leader __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 9 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,519] INFO [Broker id=6] Leader __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:33,524] INFO [Broker id=6] Leader __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 9 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,530] INFO [Broker id=6] Leader __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 19, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,538] INFO [Broker id=6] Leader __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:33,545] INFO [Broker id=6] Leader __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 4 with partition epoch 19, high watermark 4, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,546] INFO [NodeToControllerChannelManager id=6 name=alter-partition] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:33,555] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:33,558] INFO [Broker id=6] Leader financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 4 from offset 53591 with partition epoch 4, high watermark 53591, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,567] INFO [Broker id=6] Leader __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 9 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,571] INFO [Broker id=6] Leader financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 3 from offset 53395 with partition epoch 4, high watermark 53395, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,577] INFO [Broker id=6] Leader __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,583] INFO [Broker id=6] Leader __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 11 from offset 0 with partition epoch 19, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:33,588] INFO [Broker id=6] Leader financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 3 from offset 53169 with partition epoch 4, high watermark 53169, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,597] INFO [Broker id=6] Leader __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 9 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,604] INFO [Broker id=6] Leader __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:33,608] INFO [Broker id=6] Leader __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 9 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,608] INFO [Partition financial_transactions-13 broker=6] ISR updated to 6,5  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:33,609] INFO [Partition financial_transactions-17 broker=6] ISR updated to 6,4  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:33,613] INFO [Broker id=6] Leader __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 9 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,618] INFO [Broker id=6] Leader financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 3 from offset 53502 with partition epoch 4, high watermark 53502, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,622] INFO [Broker id=6] Leader __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 11 from offset 0 with partition epoch 19, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:33,626] INFO [Broker id=6] Leader __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 11 from offset 0 with partition epoch 19, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:33,630] INFO [Broker id=6] Leader financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 3 from offset 53309 with partition epoch 4, high watermark 53309, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,635] INFO [Broker id=6] Leader __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 19, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,639] INFO [Broker id=6] Leader __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:33,643] INFO [Broker id=6] Leader __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:33,651] INFO [Broker id=6] Leader financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 3 from offset 53173 with partition epoch 4, high watermark 53173, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,654] INFO [Partition __consumer_offsets-13 broker=6] ISR updated to 6,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:33,655] INFO [Partition __consumer_offsets-46 broker=6] ISR updated to 6,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:33,655] INFO [Broker id=6] Leader __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 9 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,655] INFO [Partition __consumer_offsets-9 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:33,656] INFO [Partition __consumer_offsets-42 broker=6] ISR updated to 6,5  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:33,656] INFO [Partition __consumer_offsets-21 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:33,656] INFO [Partition __consumer_offsets-17 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:33,656] INFO [Partition financial_transactions-0 broker=6] ISR updated to 6,4  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:33,657] INFO [Partition __consumer_offsets-30 broker=6] ISR updated to 6,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:33,657] INFO [Partition financial_transactions-4 broker=6] ISR updated to 6,4  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:33,658] INFO [Partition __consumer_offsets-26 broker=6] ISR updated to 6,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:33,658] INFO [Partition __consumer_offsets-5 broker=6] ISR updated to 6,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:33,658] INFO [Partition financial_transactions-8 broker=6] ISR updated to 6,4  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:33,658] INFO [Partition __consumer_offsets-38 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:33,659] INFO [Partition __consumer_offsets-1 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:33,659] INFO [Partition financial_transactions-12 broker=6] ISR updated to 6,4  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:33,659] INFO [Broker id=6] Leader __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 11 from offset 0 with partition epoch 19, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:33,659] INFO [Partition __consumer_offsets-34 broker=6] ISR updated to 6,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:33,660] INFO [Partition financial_transactions-14 broker=6] ISR updated to 6,4  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:33,660] INFO [Partition __consumer_offsets-16 broker=6] ISR updated to 6,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:33,661] INFO [Partition _schemas-0 broker=6] ISR updated to 6,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:33,661] INFO [Partition __consumer_offsets-45 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:33,661] INFO [Partition financial_transactions-18 broker=6] ISR updated to 6,4  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:33,662] INFO [Partition __consumer_offsets-12 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:33,662] INFO [Partition __consumer_offsets-41 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:33,662] INFO [Partition __consumer_offsets-24 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:33,663] INFO [Partition __consumer_offsets-20 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:33,663] INFO [Partition __consumer_offsets-49 broker=6] ISR updated to 6,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:33,663] INFO [Partition __consumer_offsets-0 broker=6] ISR updated to 6,5  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:33,664] INFO [Broker id=6] Leader financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 3 from offset 53206 with partition epoch 4, high watermark 53206, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,668] INFO [Broker id=6] Leader __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:33,672] INFO [Broker id=6] Leader __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:33,677] INFO [Broker id=6] Leader financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 2 from offset 53252 with partition epoch 4, high watermark 53252, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:33,681] INFO [Broker id=6] Leader __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:33,685] INFO [Broker id=6] Leader __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,689] INFO [Broker id=6] Leader financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 3 from offset 53208 with partition epoch 4, high watermark 53208, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,693] INFO [Broker id=6] Leader __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,700] INFO [Broker id=6] Leader __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:33,704] INFO [Broker id=6] Leader __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:33,709] INFO [Broker id=6] Leader __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 9 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,716] INFO [Broker id=6] Leader __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 19, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,723] INFO [Broker id=6] Leader __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,727] INFO [Broker id=6] Leader __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:33,733] INFO [Broker id=6] Leader financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 2 from offset 53622 with partition epoch 4, high watermark 53622, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:33,739] INFO [Broker id=6] Leader __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 19, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,745] INFO [Broker id=6] Leader financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 2 from offset 53438 with partition epoch 4, high watermark 53438, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:33,749] INFO [Broker id=6] Leader __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:33,754] INFO [Broker id=6] Leader __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 18, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,760] INFO [Broker id=6] Leader financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 3 from offset 53486 with partition epoch 4, high watermark 53486, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,766] INFO [Broker id=6] Leader __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 19, high watermark 0, ISR [6], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,771] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 13 in epoch 11 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,772] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,773] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 46 in epoch 11 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,774] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,774] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 9 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,774] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,774] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 42 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,775] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,775] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 21 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,775] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,776] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 17 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,776] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,776] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 30 in epoch 11 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,776] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,777] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 26 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,777] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,777] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 5 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,777] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,778] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-13 in 4 milliseconds for epoch 11, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,778] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 38 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,779] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-46 in 5 milliseconds for epoch 11, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,779] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,780] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-9 in 6 milliseconds for epoch 8, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,781] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-42 in 6 milliseconds for epoch 8, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,782] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-21 in 6 milliseconds for epoch 7, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,780] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 1 in epoch 9 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,783] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-17 in 7 milliseconds for epoch 7, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,784] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,785] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 34 in epoch 11 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,785] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,786] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 16 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,786] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,786] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 45 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,787] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,787] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 12 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,787] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,787] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 41 in epoch 9 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,788] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,786] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-30 in 9 milliseconds for epoch 11, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,788] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 24 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,789] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,789] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 20 in epoch 9 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,790] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,790] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 49 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,790] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,789] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-26 in 12 milliseconds for epoch 12, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,791] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 0 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,791] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-5 in 13 milliseconds for epoch 12, of which 13 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,791] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,792] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-38 in 12 milliseconds for epoch 8, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,792] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 29 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,793] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,793] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 25 in epoch 9 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,793] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,793] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-1 in 7 milliseconds for epoch 9, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,794] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 8 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,794] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,795] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 37 in epoch 11 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,795] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,796] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 4 in epoch 9 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,796] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,797] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 33 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,797] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,795] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-34 in 9 milliseconds for epoch 11, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,798] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 15 in epoch 9 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,798] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,798] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-16 in 12 milliseconds for epoch 12, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,799] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 48 in epoch 9 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,799] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-45 in 12 milliseconds for epoch 7, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,802] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-12 in 15 milliseconds for epoch 7, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,803] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-41 in 15 milliseconds for epoch 9, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,801] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,804] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 11 in epoch 11 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,804] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,805] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 44 in epoch 11 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,805] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,805] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 23 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,806] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,803] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-24 in 14 milliseconds for epoch 7, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,807] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 19 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,807] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,808] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-20 in 18 milliseconds for epoch 9, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,808] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 32 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,808] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-49 in 17 milliseconds for epoch 12, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,808] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,809] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-0 in 16 milliseconds for epoch 7, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,809] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 28 in epoch 9 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,809] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,809] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 7 in epoch 11 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,810] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,811] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 40 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,811] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,812] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 3 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,813] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,813] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 36 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,813] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,814] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 47 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,814] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,814] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 14 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,814] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,815] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 43 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,815] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,815] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 10 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,816] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,816] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 22 in epoch 9 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,816] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,817] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 18 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,817] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,817] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 31 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,818] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,819] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 27 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,819] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,820] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 39 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,820] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,821] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 6 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,821] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,821] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 35 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,822] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,822] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 2 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,823] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,826] INFO [Broker id=6] Transitioning 2 partition(s) to local leaders. (state.change.logger)
[2025-05-21 12:15:33,826] INFO Loaded member MemberMetadata(memberId=sr-1-99122467-ec1a-4fc4-88f0-59fda8f60096, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 1. (kafka.coordinator.group.GroupMetadata$)
[2025-05-21 12:15:33,826] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(financial_transactions-13, financial_transactions-17) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:15:33,827] INFO Loaded member MemberMetadata(memberId=sr-1-3027da35-fa63-4fb3-9cba-2acfe293f9a1, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 3. (kafka.coordinator.group.GroupMetadata$)
[2025-05-21 12:15:33,828] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[6, 5], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 53728, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,829] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=17, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 53580, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,830] INFO [GroupCoordinator 6]: Loading group metadata for schema-registry with generation 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,831] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-29 in 38 milliseconds for epoch 12, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,832] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-25 in 38 milliseconds for epoch 9, of which 37 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,832] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-8 in 37 milliseconds for epoch 8, of which 37 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,833] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-37 in 37 milliseconds for epoch 11, of which 37 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,833] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-4 in 36 milliseconds for epoch 9, of which 36 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,834] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-33 in 37 milliseconds for epoch 7, of which 37 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,834] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-15 in 36 milliseconds for epoch 9, of which 36 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,834] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-48 in 30 milliseconds for epoch 9, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,834] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-11 in 29 milliseconds for epoch 11, of which 29 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,835] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-44 in 30 milliseconds for epoch 11, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,836] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-23 in 29 milliseconds for epoch 12, of which 29 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,838] INFO [Broker id=6] Transitioning 27 partition(s) to local leaders. (state.change.logger)
[2025-05-21 12:15:33,838] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-19 in 30 milliseconds for epoch 7, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,839] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:15:33,840] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-32 in 31 milliseconds for epoch 7, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,841] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 5], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,842] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-28 in 33 milliseconds for epoch 9, of which 32 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,843] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-7 in 32 milliseconds for epoch 11, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,843] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-40 in 31 milliseconds for epoch 7, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,844] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-3 in 31 milliseconds for epoch 7, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,844] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-36 in 30 milliseconds for epoch 7, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,847] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-47 in 33 milliseconds for epoch 8, of which 32 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,848] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-14 in 32 milliseconds for epoch 8, of which 32 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,848] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-43 in 33 milliseconds for epoch 7, of which 33 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,849] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-10 in 33 milliseconds for epoch 7, of which 33 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,851] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-22 in 34 milliseconds for epoch 9, of which 34 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,843] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 5], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,852] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-18 in 35 milliseconds for epoch 12, of which 34 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,855] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-31 in 36 milliseconds for epoch 8, of which 35 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,855] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 4], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,856] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-27 in 36 milliseconds for epoch 7, of which 36 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,857] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,859] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,859] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-39 in 38 milliseconds for epoch 12, of which 36 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,861] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-6 in 40 milliseconds for epoch 7, of which 40 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,861] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-35 in 39 milliseconds for epoch 8, of which 39 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,862] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-2 in 38 milliseconds for epoch 12, of which 38 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,862] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,864] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=5, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 53808, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,864] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,865] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53258, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,866] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,867] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,868] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[6, 4], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 53130, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,869] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 4], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,870] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,870] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=12, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=5, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 53441, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,872] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,873] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53468, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,874] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,875] INFO [Broker id=6] Skipped the become-leader state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 4, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,876] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,877] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=18, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 53605, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,877] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,878] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,879] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,880] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,881] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:33,882] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,152] INFO [Partition financial_transactions-17 broker=6] ISR updated to 6,4,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:34,188] INFO [Broker id=6] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-21 12:15:34,189] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(financial_transactions-17) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:15:34,190] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=17, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 53580, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,197] INFO [Partition financial_transactions-13 broker=6] ISR updated to 6,5,4  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:34,197] INFO [Partition __consumer_offsets-13 broker=6] ISR updated to 6,5,4  and version updated to 21 (kafka.cluster.Partition)
[2025-05-21 12:15:34,198] INFO [Partition __consumer_offsets-46 broker=6] ISR updated to 6,5,4  and version updated to 21 (kafka.cluster.Partition)
[2025-05-21 12:15:34,198] INFO [Partition __consumer_offsets-9 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,198] INFO [Partition __consumer_offsets-42 broker=6] ISR updated to 6,5,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,198] INFO [Partition __consumer_offsets-21 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,199] INFO [Partition __consumer_offsets-17 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,199] INFO [Partition financial_transactions-0 broker=6] ISR updated to 6,4,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:34,199] INFO [Partition __consumer_offsets-30 broker=6] ISR updated to 6,4,5  and version updated to 21 (kafka.cluster.Partition)
[2025-05-21 12:15:34,200] INFO [Partition financial_transactions-4 broker=6] ISR updated to 6,4,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:34,200] INFO [Partition __consumer_offsets-26 broker=6] ISR updated to 6,4,5  and version updated to 21 (kafka.cluster.Partition)
[2025-05-21 12:15:34,200] INFO [Partition __consumer_offsets-5 broker=6] ISR updated to 6,4,5  and version updated to 21 (kafka.cluster.Partition)
[2025-05-21 12:15:34,201] INFO [Partition financial_transactions-8 broker=6] ISR updated to 6,4,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:34,202] INFO [Partition __consumer_offsets-38 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,203] INFO [Partition __consumer_offsets-1 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,204] INFO [Partition financial_transactions-12 broker=6] ISR updated to 6,4,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:34,204] INFO [Partition __consumer_offsets-34 broker=6] ISR updated to 6,4,5  and version updated to 21 (kafka.cluster.Partition)
[2025-05-21 12:15:34,205] INFO [Partition financial_transactions-14 broker=6] ISR updated to 6,4,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:34,205] INFO [Partition __consumer_offsets-16 broker=6] ISR updated to 6,4,5  and version updated to 21 (kafka.cluster.Partition)
[2025-05-21 12:15:34,206] INFO [Partition _schemas-0 broker=6] ISR updated to 6,4,5  and version updated to 21 (kafka.cluster.Partition)
[2025-05-21 12:15:34,206] INFO [Partition __consumer_offsets-45 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,207] INFO [Partition financial_transactions-18 broker=6] ISR updated to 6,4,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:34,207] INFO [Partition __consumer_offsets-12 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,208] INFO [Partition __consumer_offsets-41 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,208] INFO [Partition __consumer_offsets-24 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,209] INFO [Partition __consumer_offsets-20 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,209] INFO [Partition __consumer_offsets-49 broker=6] ISR updated to 6,4,5  and version updated to 21 (kafka.cluster.Partition)
[2025-05-21 12:15:34,333] INFO [Broker id=6] Transitioning 27 partition(s) to local leaders. (state.change.logger)
[2025-05-21 12:15:34,336] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:15:34,337] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[6, 5, 4], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 53728, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,338] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,340] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,342] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,344] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,348] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,358] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,359] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 53808, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,359] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,360] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53258, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,361] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,362] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,363] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[6, 4, 5], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 53130, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,368] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,370] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,374] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=12, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 53441, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,380] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,384] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53468, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,384] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,386] INFO [Broker id=6] Skipped the become-leader state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 4, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,387] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,389] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=18, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 53605, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,392] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,393] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,393] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,394] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,395] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,657] INFO [Partition __consumer_offsets-15 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,692] INFO [Broker id=6] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-21 12:15:34,693] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:15:34,698] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,703] INFO [Partition __consumer_offsets-48 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,704] INFO [Partition financial_transactions-15 broker=6] ISR updated to 6,4  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:34,704] INFO [Partition __consumer_offsets-11 broker=6] ISR updated to 6,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,705] INFO [Partition __consumer_offsets-44 broker=6] ISR updated to 6,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,705] INFO [Partition financial_transactions-19 broker=6] ISR updated to 6,4  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:34,705] INFO [Partition __consumer_offsets-23 broker=6] ISR updated to 6,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,706] INFO [Partition __consumer_offsets-19 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,706] INFO [Partition __consumer_offsets-32 broker=6] ISR updated to 6,5  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,706] INFO [Partition financial_transactions-2 broker=6] ISR updated to 6,5  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:34,706] INFO [Partition __consumer_offsets-28 broker=6] ISR updated to 6,5  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,707] INFO [Partition __consumer_offsets-7 broker=6] ISR updated to 6,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,707] INFO [Partition financial_transactions-6 broker=6] ISR updated to 6,4  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:34,707] INFO [Partition __consumer_offsets-40 broker=6] ISR updated to 6,5  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,707] INFO [Partition __consumer_offsets-3 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,708] INFO [Partition financial_transactions-10 broker=6] ISR updated to 6,5  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:34,708] INFO [Partition __consumer_offsets-36 broker=6] ISR updated to 6,5  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,708] INFO [Partition __consumer_offsets-47 broker=6] ISR updated to 6,5  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,709] INFO [Partition financial_transactions-16 broker=6] ISR updated to 6,5  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:34,709] INFO [Partition __consumer_offsets-14 broker=6] ISR updated to 6,5  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,709] INFO [Partition __consumer_offsets-43 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,710] INFO [Partition __consumer_offsets-10 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,710] INFO [Partition __consumer_offsets-22 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,711] INFO [Partition __consumer_offsets-18 broker=6] ISR updated to 6,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,711] INFO [Partition __consumer_offsets-31 broker=6] ISR updated to 6,5  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,712] INFO [Partition __consumer_offsets-0 broker=6] ISR updated to 6,5,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,712] INFO [Partition __consumer_offsets-29 broker=6] ISR updated to 6,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,712] INFO [Partition financial_transactions-1 broker=6] ISR updated to 6,5  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:34,713] INFO [Partition __consumer_offsets-27 broker=6] ISR updated to 6,5  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,713] INFO [Partition financial_transactions-3 broker=6] ISR updated to 6,5  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:34,713] INFO [Partition __consumer_offsets-25 broker=6] ISR updated to 6,5  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,713] INFO [Partition financial_transactions-5 broker=6] ISR updated to 6,5  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:34,714] INFO [Partition __consumer_offsets-39 broker=6] ISR updated to 6,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,714] INFO [Partition __consumer_offsets-8 broker=6] ISR updated to 6,5  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,715] INFO [Partition financial_transactions-7 broker=6] ISR updated to 6,4  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:34,715] INFO [Partition __consumer_offsets-37 broker=6] ISR updated to 6,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,715] INFO [Partition __consumer_offsets-6 broker=6] ISR updated to 6,5  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,716] INFO [Partition financial_transactions-9 broker=6] ISR updated to 6,4  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:34,716] INFO [Partition __consumer_offsets-35 broker=6] ISR updated to 6,5  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,717] INFO [Partition __consumer_offsets-4 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,717] INFO [Partition financial_transactions-11 broker=6] ISR updated to 6,4  and version updated to 5 (kafka.cluster.Partition)
[2025-05-21 12:15:34,717] INFO [Partition __consumer_offsets-33 broker=6] ISR updated to 6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-21 12:15:34,717] INFO [Partition __consumer_offsets-2 broker=6] ISR updated to 6,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:34,834] INFO [Broker id=6] Transitioning 42 partition(s) to local leaders. (state.change.logger)
[2025-05-21 12:15:34,836] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-39, __consumer_offsets-8, financial_transactions-7, __consumer_offsets-37, __consumer_offsets-6, financial_transactions-9, __consumer_offsets-35, __consumer_offsets-4, financial_transactions-11, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:15:34,837] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,838] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53502, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,839] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,840] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,841] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53309, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,841] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,842] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,843] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,845] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53173, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,846] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 5], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,847] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 5], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,847] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53206, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,848] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,848] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,849] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=10, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 53252, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,850] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,850] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,851] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53208, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,851] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,852] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,852] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,853] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,853] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,853] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,854] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5, 4], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,854] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 4, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,855] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[6, 5], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 53591, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,856] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,856] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=3, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 53622, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,857] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 5], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,857] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53395, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,858] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,858] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,859] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=5, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 53438, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,859] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,860] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,860] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53169, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,860] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,861] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,862] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53486, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,862] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:34,863] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [6,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,184] INFO [Partition __consumer_offsets-15 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,231] INFO [Broker id=6] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-21 12:15:35,231] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:15:35,232] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,238] INFO [Partition __consumer_offsets-48 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,238] INFO [Partition financial_transactions-15 broker=6] ISR updated to 6,4,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:35,239] INFO [Partition __consumer_offsets-11 broker=6] ISR updated to 6,4,5  and version updated to 21 (kafka.cluster.Partition)
[2025-05-21 12:15:35,239] INFO [Partition __consumer_offsets-44 broker=6] ISR updated to 6,4,5  and version updated to 21 (kafka.cluster.Partition)
[2025-05-21 12:15:35,239] INFO [Partition financial_transactions-19 broker=6] ISR updated to 6,4,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:35,239] INFO [Partition __consumer_offsets-23 broker=6] ISR updated to 6,5,4  and version updated to 21 (kafka.cluster.Partition)
[2025-05-21 12:15:35,240] INFO [Partition __consumer_offsets-19 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,240] INFO [Partition __consumer_offsets-32 broker=6] ISR updated to 6,5,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,240] INFO [Partition financial_transactions-2 broker=6] ISR updated to 6,5,4  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:35,240] INFO [Partition __consumer_offsets-28 broker=6] ISR updated to 6,5,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,241] INFO [Partition __consumer_offsets-7 broker=6] ISR updated to 6,5,4  and version updated to 21 (kafka.cluster.Partition)
[2025-05-21 12:15:35,241] INFO [Partition financial_transactions-6 broker=6] ISR updated to 6,4,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:35,241] INFO [Partition __consumer_offsets-40 broker=6] ISR updated to 6,5,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,241] INFO [Partition __consumer_offsets-3 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,242] INFO [Partition financial_transactions-10 broker=6] ISR updated to 6,5,4  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:35,242] INFO [Partition __consumer_offsets-36 broker=6] ISR updated to 6,5,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,242] INFO [Partition __consumer_offsets-47 broker=6] ISR updated to 6,5,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,242] INFO [Partition financial_transactions-16 broker=6] ISR updated to 6,5,4  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:35,243] INFO [Partition __consumer_offsets-14 broker=6] ISR updated to 6,5,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,243] INFO [Partition __consumer_offsets-43 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,243] INFO [Partition __consumer_offsets-10 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,243] INFO [Partition __consumer_offsets-22 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,244] INFO [Partition __consumer_offsets-18 broker=6] ISR updated to 6,5,4  and version updated to 21 (kafka.cluster.Partition)
[2025-05-21 12:15:35,244] INFO [Partition __consumer_offsets-31 broker=6] ISR updated to 6,5,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,244] INFO [Partition __consumer_offsets-29 broker=6] ISR updated to 6,5,4  and version updated to 21 (kafka.cluster.Partition)
[2025-05-21 12:15:35,244] INFO [Partition financial_transactions-1 broker=6] ISR updated to 6,5,4  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:35,245] INFO [Partition __consumer_offsets-27 broker=6] ISR updated to 6,5,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,245] INFO [Partition financial_transactions-3 broker=6] ISR updated to 6,5,4  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:35,245] INFO [Partition __consumer_offsets-25 broker=6] ISR updated to 6,5,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,245] INFO [Partition financial_transactions-5 broker=6] ISR updated to 6,5,4  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:35,246] INFO [Partition __consumer_offsets-39 broker=6] ISR updated to 6,5,4  and version updated to 21 (kafka.cluster.Partition)
[2025-05-21 12:15:35,246] INFO [Partition __consumer_offsets-8 broker=6] ISR updated to 6,5,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,246] INFO [Partition financial_transactions-7 broker=6] ISR updated to 6,4,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:35,246] INFO [Partition __consumer_offsets-37 broker=6] ISR updated to 6,4,5  and version updated to 21 (kafka.cluster.Partition)
[2025-05-21 12:15:35,247] INFO [Partition __consumer_offsets-6 broker=6] ISR updated to 6,5,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,247] INFO [Partition financial_transactions-9 broker=6] ISR updated to 6,4,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:35,247] INFO [Partition __consumer_offsets-35 broker=6] ISR updated to 6,5,4  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,247] INFO [Partition __consumer_offsets-4 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,248] INFO [Partition financial_transactions-11 broker=6] ISR updated to 6,4,5  and version updated to 6 (kafka.cluster.Partition)
[2025-05-21 12:15:35,248] INFO [Partition __consumer_offsets-33 broker=6] ISR updated to 6,4,5  and version updated to 20 (kafka.cluster.Partition)
[2025-05-21 12:15:35,248] INFO [Partition __consumer_offsets-2 broker=6] ISR updated to 6,5,4  and version updated to 21 (kafka.cluster.Partition)
[2025-05-21 12:15:35,334] INFO [Broker id=6] Transitioning 41 partition(s) to local leaders. (state.change.logger)
[2025-05-21 12:15:35,335] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-39, __consumer_offsets-8, financial_transactions-7, __consumer_offsets-37, __consumer_offsets-6, financial_transactions-9, __consumer_offsets-35, __consumer_offsets-4, financial_transactions-11, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:15:35,336] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,337] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53502, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,337] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,338] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,341] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53309, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,342] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,343] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,344] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5, 4], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,344] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53173, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,345] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,346] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,347] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53206, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,347] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5, 4], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,348] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,349] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=10, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 53252, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,351] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5, 4], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,353] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,354] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=6, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53208, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,355] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,356] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,356] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,357] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,357] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,358] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,358] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 4, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,358] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[6, 5, 4], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 53591, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,359] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5, 4], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,359] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=3, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 53622, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,360] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,360] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=6, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53395, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,360] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,361] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,361] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 53438, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,362] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,362] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5, 4], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,362] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53169, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,363] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,363] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,364] INFO [Broker id=6] Skipped the become-leader state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 3. Current high watermark 53486, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,364] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:35,364] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:15:37,716] INFO [GroupCoordinator 6]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-960e2a90-cf63-43a4-b2ba-89aa2499d218 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:37,727] INFO [GroupCoordinator 6]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 4 (__consumer_offsets-29) (reason: Adding new member sr-1-960e2a90-cf63-43a4-b2ba-89aa2499d218 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:37,732] INFO [GroupCoordinator 6]: Stabilized group schema-registry generation 5 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:37,760] INFO [GroupCoordinator 6]: Assignment received from leader sr-1-960e2a90-cf63-43a4-b2ba-89aa2499d218 for group schema-registry for generation 5. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,420] INFO [Broker id=6] Transitioning 47 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:20:30,420] INFO [Broker id=6] Follower financial_transactions-13 starts at leader epoch 5 from offset 53728 with partition epoch 7 and high watermark 53728. Current leader is 4. Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 12:20:30,420] INFO [Broker id=6] Follower financial_transactions-15 starts at leader epoch 4 from offset 53502 with partition epoch 7 and high watermark 53502. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,421] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 12 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,421] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 12 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,421] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 12 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,422] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 12 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,422] INFO [Broker id=6] Follower financial_transactions-19 starts at leader epoch 4 from offset 53309 with partition epoch 7 and high watermark 53309. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,422] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 13 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,422] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,423] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,423] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,423] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,424] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 12 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,424] INFO [Broker id=6] Follower financial_transactions-2 starts at leader epoch 4 from offset 53173 with partition epoch 7 and high watermark 53173. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,425] INFO [Broker id=6] Follower financial_transactions-4 starts at leader epoch 4 from offset 53258 with partition epoch 7 and high watermark 53258. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,425] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 13 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,425] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 12 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,426] INFO [Broker id=6] Follower financial_transactions-6 starts at leader epoch 4 from offset 53206 with partition epoch 7 and high watermark 53206. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,426] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,426] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 13 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,426] INFO [Broker id=6] Follower financial_transactions-8 starts at leader epoch 5 from offset 53130 with partition epoch 7 and high watermark 53130. Current leader is 4. Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 12:20:30,427] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,427] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,427] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 12 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,428] INFO [Broker id=6] Follower financial_transactions-14 starts at leader epoch 4 from offset 53468 with partition epoch 7 and high watermark 53468. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,428] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 13 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,428] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 13 from offset 6 with partition epoch 22 and high watermark 6. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,428] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,429] INFO [Broker id=6] Follower financial_transactions-16 starts at leader epoch 4 from offset 53208 with partition epoch 7 and high watermark 53208. Current leader is 4. Previous leader Some(4) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,429] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,429] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,429] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,430] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,430] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 13 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,431] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 13 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,431] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,431] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 13 from offset 5 with partition epoch 22 and high watermark 5. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,431] INFO [Broker id=6] Follower financial_transactions-1 starts at leader epoch 5 from offset 53591 with partition epoch 7 and high watermark 53591. Current leader is 4. Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 12:20:30,432] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,432] INFO [Broker id=6] Follower financial_transactions-5 starts at leader epoch 4 from offset 53395 with partition epoch 7 and high watermark 53395. Current leader is 4. Previous leader Some(4) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,432] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 13 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,432] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 12 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,433] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,433] INFO [Broker id=6] Follower financial_transactions-9 starts at leader epoch 4 from offset 53169 with partition epoch 7 and high watermark 53169. Current leader is 4. Previous leader Some(4) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,433] INFO [Broker id=6] Follower financial_transactions-11 starts at leader epoch 4 from offset 53486 with partition epoch 7 and high watermark 53486. Current leader is 5. Previous leader Some(5) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,434] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 8 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:20:30,434] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 13 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,434] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(financial_transactions-13, financial_transactions-15, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-30, financial_transactions-2, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-16, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-27, financial_transactions-5, __consumer_offsets-39, __consumer_offsets-37, __consumer_offsets-6, financial_transactions-9, financial_transactions-11, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:20:30,434] INFO [Broker id=6] Stopped fetchers as part of become-follower for 47 partitions (state.change.logger)
[2025-05-21 12:20:30,447] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,449] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(financial_transactions-13 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),5,53728), __consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,0), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,0), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,0), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,0), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,0), financial_transactions-8 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),5,53130), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,0), __consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,0), _schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,6), financial_transactions-16 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),4,53208), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,5), financial_transactions-1 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),5,53591), financial_transactions-5 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),4,53395), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,0), financial_transactions-9 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),4,53169), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:20:30,452] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,452] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(financial_transactions-14 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,53468), financial_transactions-15 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,53502), __consumer_offsets-45 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-43 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-12 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), financial_transactions-19 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,53309), __consumer_offsets-10 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-24 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-21 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-19 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-17 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-32 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-0 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-27 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), financial_transactions-2 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,53173), financial_transactions-4 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,53258), financial_transactions-6 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,53206), __consumer_offsets-40 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-6 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-3 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), __consumer_offsets-36 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0), financial_transactions-11 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),4,53486), __consumer_offsets-33 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:20:30,452] INFO [Broker id=6] Started fetchers as part of become-follower for 47 partitions (state.change.logger)
[2025-05-21 12:20:30,532] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,533] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,533] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,533] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,534] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,533] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,534] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,534] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,535] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,536] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,536] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,537] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,537] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,537] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,537] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,538] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,538] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,538] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,539] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,538] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,539] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,539] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,540] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,540] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,540] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,540] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,541] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,541] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,542] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,542] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,541] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,542] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,543] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,543] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,543] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,544] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,544] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,544] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,545] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,544] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,545] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,545] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,546] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,546] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,547] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,547] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,547] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,548] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,548] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,548] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,548] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,549] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,549] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,550] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,550] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,550] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,551] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,551] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,551] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,552] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,553] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,552] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,553] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,553] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,554] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,554] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,554] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,554] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,555] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,555] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,556] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,556] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,556] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,556] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,557] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,557] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,558] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,558] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,558] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,558] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,559] INFO [GroupCoordinator 6]: Unloading group metadata for schema-registry with generation 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,558] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,560] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,560] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,560] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,561] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,562] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,562] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,562] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,562] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,562] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,563] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,563] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,563] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,564] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,563] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,564] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,564] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,565] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,565] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,571] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition financial_transactions-13 with TruncationState(offset=53728, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=13, leaderEpoch=0, endOffset=53728) (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,573] INFO [UnifiedLog partition=financial_transactions-13, dir=/tmp/kafka-logs] Truncating to 53728 has no effect as the largest offset in the log is 53727 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,575] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-14 with TruncationState(offset=53468, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=14, leaderEpoch=0, endOffset=53468) (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,576] INFO [UnifiedLog partition=financial_transactions-14, dir=/tmp/kafka-logs] Truncating to 53468 has no effect as the largest offset in the log is 53467 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,577] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition financial_transactions-16 with TruncationState(offset=53208, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=16, leaderEpoch=0, endOffset=53208) (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,577] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-15 with TruncationState(offset=53502, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=15, leaderEpoch=0, endOffset=53502) (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,577] INFO [UnifiedLog partition=financial_transactions-16, dir=/tmp/kafka-logs] Truncating to 53208 has no effect as the largest offset in the log is 53207 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,577] INFO [UnifiedLog partition=financial_transactions-15, dir=/tmp/kafka-logs] Truncating to 53502 has no effect as the largest offset in the log is 53501 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,578] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition financial_transactions-1 with TruncationState(offset=53591, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=1, leaderEpoch=0, endOffset=53591) (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,578] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-2 with TruncationState(offset=53173, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=2, leaderEpoch=0, endOffset=53173) (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,578] INFO [UnifiedLog partition=financial_transactions-1, dir=/tmp/kafka-logs] Truncating to 53591 has no effect as the largest offset in the log is 53590 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,579] INFO [UnifiedLog partition=financial_transactions-2, dir=/tmp/kafka-logs] Truncating to 53173 has no effect as the largest offset in the log is 53172 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,579] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition financial_transactions-5 with TruncationState(offset=53395, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=5, leaderEpoch=0, endOffset=53395) (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,579] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-19 with TruncationState(offset=53309, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=19, leaderEpoch=0, endOffset=53309) (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,580] INFO [UnifiedLog partition=financial_transactions-19, dir=/tmp/kafka-logs] Truncating to 53309 has no effect as the largest offset in the log is 53308 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,579] INFO [UnifiedLog partition=financial_transactions-5, dir=/tmp/kafka-logs] Truncating to 53395 has no effect as the largest offset in the log is 53394 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,581] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-4 with TruncationState(offset=53258, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=4, leaderEpoch=0, endOffset=53258) (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,582] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition financial_transactions-8 with TruncationState(offset=53130, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=8, leaderEpoch=0, endOffset=53130) (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,582] INFO [UnifiedLog partition=financial_transactions-4, dir=/tmp/kafka-logs] Truncating to 53258 has no effect as the largest offset in the log is 53257 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,582] INFO [UnifiedLog partition=financial_transactions-8, dir=/tmp/kafka-logs] Truncating to 53130 has no effect as the largest offset in the log is 53129 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,582] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-6 with TruncationState(offset=53206, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=6, leaderEpoch=0, endOffset=53206) (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,583] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition financial_transactions-9 with TruncationState(offset=53169, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=9, leaderEpoch=0, endOffset=53169) (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,583] INFO [UnifiedLog partition=financial_transactions-6, dir=/tmp/kafka-logs] Truncating to 53206 has no effect as the largest offset in the log is 53205 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,583] INFO [UnifiedLog partition=financial_transactions-9, dir=/tmp/kafka-logs] Truncating to 53169 has no effect as the largest offset in the log is 53168 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,584] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-11 with TruncationState(offset=53486, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=11, leaderEpoch=0, endOffset=53486) (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,584] INFO [UnifiedLog partition=financial_transactions-11, dir=/tmp/kafka-logs] Truncating to 53486 has no effect as the largest offset in the log is 53485 (kafka.log.UnifiedLog)
[2025-05-21 12:25:29,810] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:25:35,501] INFO [NodeToControllerChannelManager id=6 name=alter-partition] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:57,800] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-21 12:47:57,810] INFO [BrokerServer id=6] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
[2025-05-21 12:47:57,811] INFO [BrokerServer id=6] shutting down (kafka.server.BrokerServer)
[2025-05-21 12:47:57,815] INFO [BrokerLifecycleManager id=6] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:47:57,874] INFO [BrokerLifecycleManager id=6] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:47:57,893] INFO [Broker id=6] Transitioning 71 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:47:57,899] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=4, leaderEpoch=5, isr=[5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 12:47:57,900] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[5, 4], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:47:57,901] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[5, 4], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:47:57,901] INFO [Broker id=6] Follower financial_transactions-17 starts at leader epoch 3 from offset 255866 with partition epoch 7 and high watermark 255866. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:47:57,906] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 9 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:57,907] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 9 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:57,908] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:57,909] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:57,910] INFO [Broker id=6] Follower financial_transactions-0 starts at leader epoch 3 from offset 257211 with partition epoch 7 and high watermark 257197. Current leader is 4. Previous leader Some(4) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:47:57,911] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[4, 5], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:47:57,914] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:57,915] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[4, 5], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:57,916] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[4, 5], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:57,917] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=4, leaderEpoch=5, isr=[4, 5], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 12:47:57,917] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 9 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:57,918] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 10 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:57,918] INFO [Broker id=6] Follower financial_transactions-12 starts at leader epoch 3 from offset 255590 with partition epoch 7 and high watermark 255590. Current leader is 4. Previous leader Some(4) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:47:57,918] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[4, 5], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:47:57,919] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:57,920] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[4, 5], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:57,920] INFO [Broker id=6] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[4, 5], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:57,927] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:57,928] INFO [Broker id=6] Follower financial_transactions-18 starts at leader epoch 3 from offset 255601 with partition epoch 7 and high watermark 255590. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:47:57,931] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:57,932] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 10 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:57,932] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:57,933] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 10 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:57,933] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[4, 5], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:57,934] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:57,935] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[5, 4], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:57,936] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=4, leaderEpoch=5, isr=[5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 12:47:57,937] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 10 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:57,938] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=4, leaderEpoch=4, isr=[5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:57,940] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 9 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:57,941] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[4, 5], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:47:57,943] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=4, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:57,944] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 10 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:57,945] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:57,946] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 10 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:57,947] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 10 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:57,947] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:57,948] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[4, 5], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:47:57,948] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[4, 5], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:47:57,949] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:57,949] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[5, 4], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:57,950] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:57,950] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:57,951] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:57,952] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 10 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:57,952] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[5, 4], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:47:57,953] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:57,954] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:57,956] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:57,957] INFO [Broker id=6] Follower financial_transactions-10 starts at leader epoch 3 from offset 256290 with partition epoch 7 and high watermark 256290. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:47:57,958] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:57,959] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 9 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:57,961] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=4, leaderEpoch=4, isr=[5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:57,963] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 9 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:57,968] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:57,969] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:57,970] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 10 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:57,972] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[5, 4], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:57,972] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 9 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:57,973] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:57,974] INFO [Broker id=6] Follower financial_transactions-3 starts at leader epoch 3 from offset 255715 with partition epoch 7 and high watermark 255715. Current leader is 5. Previous leader Some(5) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:47:57,977] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[5, 4], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:57,977] INFO [Broker id=6] Follower financial_transactions-7 starts at leader epoch 3 from offset 255844 with partition epoch 7 and high watermark 255844. Current leader is 4. Previous leader Some(4) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:47:57,984] INFO [BrokerLifecycleManager id=6] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:47:57,988] INFO [BrokerLifecycleManager id=6] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:57,989] INFO [BrokerLifecycleManager id=6] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:47:57,991] INFO [broker-6-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:57,987] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:47:58,002] INFO [broker-6-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,001] INFO [broker-6-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:57,993] INFO [SocketServer listenerType=BROKER, nodeId=6] Stopping socket server request processors (kafka.network.SocketServer)
[2025-05-21 12:47:58,011] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 9 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,012] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:58,012] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[5, 4], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:58,032] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 12:47:58,046] INFO [SocketServer listenerType=BROKER, nodeId=6] Stopped socket server request processors (kafka.network.SocketServer)
[2025-05-21 12:47:58,056] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, financial_transactions-18, financial_transactions-12, financial_transactions-11, __consumer_offsets-8, __consumer_offsets-21, financial_transactions-15, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, financial_transactions-4, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, financial_transactions-0, financial_transactions-6, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, financial_transactions-16, __consumer_offsets-15, financial_transactions-10, __consumer_offsets-24, financial_transactions-19, financial_transactions-7, __consumer_offsets-17, financial_transactions-17, financial_transactions-1, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, financial_transactions-14, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, financial_transactions-8, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, financial_transactions-2, financial_transactions-13, __consumer_offsets-32, financial_transactions-5, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:47:58,061] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Node 5 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,076] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Node 4 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,078] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Cancelled in-flight FETCH request with correlation id 58393 due to node 4 being disconnected (elapsed time since creation: 70ms, elapsed time since send: 70ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,068] INFO [ReplicaAlterLogDirsManager on broker 6] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, financial_transactions-18, financial_transactions-12, financial_transactions-11, __consumer_offsets-8, __consumer_offsets-21, financial_transactions-15, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, financial_transactions-4, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, financial_transactions-0, financial_transactions-6, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, financial_transactions-16, __consumer_offsets-15, financial_transactions-10, __consumer_offsets-24, financial_transactions-19, financial_transactions-7, __consumer_offsets-17, financial_transactions-17, financial_transactions-1, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, financial_transactions-14, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, financial_transactions-8, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, financial_transactions-2, financial_transactions-13, __consumer_offsets-32, financial_transactions-5, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-21 12:47:58,077] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Cancelled in-flight FETCH request with correlation id 67863 due to node 5 being disconnected (elapsed time since creation: 5ms, elapsed time since send: 5ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,084] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Client requested connection close from node 5 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,086] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Client requested connection close from node 4 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,090] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error sending fetch request (sessionId=252943050, epoch=67863) to node 5: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 12:47:58,091] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Error sending fetch request (sessionId=175184887, epoch=58393) to node 4: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 4 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 12:47:58,111] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=6, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=0e8v3fGFR_uwy9DAR-lNZA:financial_transactions-14, 0e8v3fGFR_uwy9DAR-lNZA:financial_transactions-15, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-45, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-43, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-12, 0e8v3fGFR_uwy9DAR-lNZA:financial_transactions-19, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-10, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-24, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-21, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-19, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-17, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-32, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-0, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-27, 0e8v3fGFR_uwy9DAR-lNZA:financial_transactions-2, 0e8v3fGFR_uwy9DAR-lNZA:financial_transactions-4, 0e8v3fGFR_uwy9DAR-lNZA:financial_transactions-6, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-40, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-6, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-3, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-36, 0e8v3fGFR_uwy9DAR-lNZA:financial_transactions-11, 94Q8ilNOTgqGgcE4hkgLtw:__consumer_offsets-33, replaced=, metadata=(sessionId=252943050, epoch=67863), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 5 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 12:47:58,099] INFO [Broker id=6] Stopped fetchers as part of controlled shutdown for 71 partitions (state.change.logger)
[2025-05-21 12:47:58,111] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=6, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={financial_transactions-13=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=255889, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[5])}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=175184887, epoch=58393), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 4 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 12:47:58,114] INFO [ReplicaFetcherThread-0-4]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,116] INFO [ReplicaFetcherThread-0-4]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,116] INFO [ReplicaFetcherThread-0-4]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,118] INFO [ReplicaFetcherThread-0-5]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,119] INFO [ReplicaFetcherThread-0-5]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,119] INFO [ReplicaFetcherThread-0-5]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,126] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,128] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,129] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,129] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,130] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,131] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,132] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,131] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,137] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,140] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,141] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,142] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,142] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,143] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,144] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,144] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,144] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,145] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,145] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,145] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,146] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,146] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,146] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,147] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,147] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,145] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,148] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,148] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,149] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,148] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,149] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,149] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,149] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,150] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,150] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,150] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,151] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,151] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,151] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,151] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,152] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,152] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,152] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,153] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,153] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,153] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,155] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,155] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,155] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,156] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,157] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,156] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,157] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,157] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,158] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,159] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,159] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,158] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,161] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,161] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,161] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,161] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,162] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,162] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,163] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,162] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,163] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,163] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,164] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,164] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,164] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,164] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,165] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,165] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,165] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,166] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,166] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,166] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,167] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,166] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,167] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,167] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,168] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,168] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,168] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,168] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,169] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,169] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,170] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,171] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,171] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,171] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,171] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,172] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,173] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,174] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,174] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,174] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,175] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,175] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,175] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,176] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,176] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,177] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,177] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,178] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,177] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,178] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,178] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,179] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,179] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,180] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,180] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,181] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,182] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,181] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,182] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,182] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,183] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,184] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,184] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,184] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,185] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,185] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,185] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,186] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,186] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,186] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,186] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,187] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,187] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,187] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,188] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,188] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,189] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,191] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,191] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,191] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,191] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,192] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,192] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,193] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,193] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,193] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,194] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,194] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,195] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,195] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,195] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,196] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,197] INFO [data-plane Kafka Request Handler on Broker 6], shutting down (kafka.server.KafkaRequestHandlerPool)
[2025-05-21 12:47:58,199] INFO [data-plane Kafka Request Handler on Broker 6], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2025-05-21 12:47:58,201] INFO [ExpirationReaper-6-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,205] INFO [ExpirationReaper-6-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,205] INFO [ExpirationReaper-6-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,208] INFO [KafkaApi-6] Shutdown complete. (kafka.server.KafkaApis)
[2025-05-21 12:47:58,212] INFO [TransactionCoordinator id=6] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 12:47:58,213] INFO [Transaction State Manager 6]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2025-05-21 12:47:58,214] INFO [TxnMarkerSenderThread-6]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 12:47:58,214] INFO [TxnMarkerSenderThread-6]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 12:47:58,214] INFO [TxnMarkerSenderThread-6]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 12:47:58,218] INFO [TransactionCoordinator id=6] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 12:47:58,219] INFO [GroupCoordinator 6]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,220] INFO [ExpirationReaper-6-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,221] INFO [ExpirationReaper-6-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,221] INFO [ExpirationReaper-6-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,222] INFO [ExpirationReaper-6-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,223] INFO [ExpirationReaper-6-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,223] INFO [ExpirationReaper-6-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,224] INFO [GroupCoordinator 6]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,226] INFO [AssignmentsManager id=6]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,226] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,227] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,227] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,228] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 12:47:58,228] INFO [AssignmentsManager id=6]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,229] INFO [ReplicaManager broker=6] Shutting down (kafka.server.ReplicaManager)
[2025-05-21 12:47:58,230] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 12:47:58,232] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 12:47:58,232] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 12:47:58,233] INFO [ReplicaFetcherManager on broker 6] shutting down (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:47:58,236] INFO [ReplicaFetcherManager on broker 6] shutdown completed (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:47:58,237] INFO [ReplicaAlterLogDirsManager on broker 6] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-21 12:47:58,240] INFO [ReplicaAlterLogDirsManager on broker 6] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-21 12:47:58,241] INFO [ExpirationReaper-6-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,441] INFO [ExpirationReaper-6-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,441] INFO [ExpirationReaper-6-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,442] INFO [ExpirationReaper-6-RemoteFetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,443] INFO [ExpirationReaper-6-RemoteFetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,443] INFO [ExpirationReaper-6-RemoteFetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,443] INFO [ExpirationReaper-6-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,444] INFO [ExpirationReaper-6-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,444] INFO [ExpirationReaper-6-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,445] INFO [ExpirationReaper-6-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,446] INFO [ExpirationReaper-6-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,446] INFO [ExpirationReaper-6-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,447] INFO [ExpirationReaper-6-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,448] INFO [ExpirationReaper-6-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,448] INFO [ExpirationReaper-6-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,455] INFO [AddPartitionsToTxnSenderThread-6]: Shutting down (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 12:47:58,456] INFO [AddPartitionsToTxnSenderThread-6]: Stopped (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 12:47:58,456] INFO [AddPartitionsToTxnSenderThread-6]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 12:47:58,457] INFO [ReplicaManager broker=6] Shut down completely (kafka.server.ReplicaManager)
[2025-05-21 12:47:58,458] INFO [broker-6-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,458] INFO [broker-6-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,458] INFO [broker-6-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,459] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 12:47:58,460] INFO [broker-6-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,460] INFO [broker-6-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,460] INFO [broker-6-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,461] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 12:47:58,462] INFO Shutting down. (kafka.log.LogManager)
[2025-05-21 12:47:58,465] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
[2025-05-21 12:47:58,467] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 12:47:58,468] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 12:47:58,468] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 12:47:58,490] INFO [ProducerStateManager partition=financial_transactions-15] Wrote producer snapshot at offset 256237 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,518] INFO [ProducerStateManager partition=financial_transactions-0] Wrote producer snapshot at offset 257211 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,548] INFO [ProducerStateManager partition=financial_transactions-3] Wrote producer snapshot at offset 255715 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,558] INFO [ProducerStateManager partition=financial_transactions-10] Wrote producer snapshot at offset 256290 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,565] INFO [ProducerStateManager partition=financial_transactions-12] Wrote producer snapshot at offset 255590 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,570] INFO [ProducerStateManager partition=financial_transactions-13] Wrote producer snapshot at offset 255889 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,588] INFO [ProducerStateManager partition=financial_transactions-19] Wrote producer snapshot at offset 255167 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,593] INFO [ProducerStateManager partition=financial_transactions-5] Wrote producer snapshot at offset 254990 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,598] INFO [ProducerStateManager partition=financial_transactions-6] Wrote producer snapshot at offset 255175 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,618] INFO [ProducerStateManager partition=financial_transactions-17] Wrote producer snapshot at offset 255866 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,624] INFO [ProducerStateManager partition=financial_transactions-7] Wrote producer snapshot at offset 255844 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,638] INFO [ProducerStateManager partition=__consumer_offsets-29] Wrote producer snapshot at offset 6 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,669] INFO [ProducerStateManager partition=financial_transactions-4] Wrote producer snapshot at offset 255335 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,687] INFO [ProducerStateManager partition=financial_transactions-18] Wrote producer snapshot at offset 255601 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,695] INFO [ProducerStateManager partition=financial_transactions-9] Wrote producer snapshot at offset 255435 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,701] INFO [ProducerStateManager partition=financial_transactions-1] Wrote producer snapshot at offset 255371 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,707] INFO [ProducerStateManager partition=financial_transactions-14] Wrote producer snapshot at offset 255812 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,719] INFO [ProducerStateManager partition=financial_transactions-11] Wrote producer snapshot at offset 256033 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,734] INFO [ProducerStateManager partition=_schemas-0] Wrote producer snapshot at offset 6 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,744] INFO [ProducerStateManager partition=financial_transactions-8] Wrote producer snapshot at offset 255525 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,752] INFO [ProducerStateManager partition=financial_transactions-2] Wrote producer snapshot at offset 255541 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,761] INFO [ProducerStateManager partition=financial_transactions-16] Wrote producer snapshot at offset 255412 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,797] INFO Shutdown complete. (kafka.log.LogManager)
[2025-05-21 12:47:58,798] INFO [broker-6-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,799] INFO [broker-6-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,799] INFO [broker-6-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,799] INFO [broker-6-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,800] INFO [broker-6-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,800] INFO [broker-6-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,800] INFO [broker-6-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,801] INFO [broker-6-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,801] INFO [broker-6-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,801] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,802] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,802] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,805] INFO [SocketServer listenerType=BROKER, nodeId=6] Shutting down socket server (kafka.network.SocketServer)
[2025-05-21 12:47:58,816] INFO [SocketServer listenerType=BROKER, nodeId=6] Shutdown completed (kafka.network.SocketServer)
[2025-05-21 12:47:58,817] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
[2025-05-21 12:47:58,818] INFO [BrokerLifecycleManager id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,818] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 12:47:58,819] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 12:47:58,819] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 12:47:58,821] INFO [SharedServer id=6] Stopping SharedServer (kafka.server.SharedServer)
[2025-05-21 12:47:58,821] INFO [MetadataLoader id=6] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,822] INFO [SnapshotGenerator id=6] close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,823] INFO [SnapshotGenerator id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,823] INFO [MetadataLoader id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,824] INFO [SnapshotGenerator id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,825] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 12:47:58,886] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 12:47:58,886] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 12:47:58,888] INFO [kafka-6-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 12:47:58,888] INFO [RaftManager id=6] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 12:47:58,889] INFO [RaftManager id=6] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 12:47:58,889] INFO [RaftManager id=6] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 12:47:58,889] INFO [kafka-6-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 12:47:58,889] INFO [kafka-6-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 12:47:58,894] INFO [kafka-6-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 12:47:58,894] INFO [kafka-6-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 12:47:58,894] INFO [kafka-6-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 12:47:58,898] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 12473 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,900] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-21 12:47:58,901] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2025-05-21 12:47:58,901] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-21 12:47:58,901] INFO App info kafka.server for 6 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 12:47:58,902] INFO [BrokerServer id=6] shut down completed (kafka.server.BrokerServer)
[2025-05-21 12:47:58,902] INFO [BrokerServer id=6] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
[2025-05-21 12:47:58,903] INFO App info kafka.server for 6 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 14:13:10,289] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-21 14:13:10,669] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-21 14:13:10,693] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-21 14:13:10,697] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:16,469] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-21 14:13:16,742] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-21 14:13:16,769] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:17,180] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:17,218] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:17,270] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-21 14:13:17,280] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-21 14:13:17,282] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2025-05-21 14:13:17,294] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:17,391] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
[2025-05-21 14:13:17,394] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:17,397] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:17,401] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000008082.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:17,403] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000012473.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:17,405] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 5ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:17,792] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 12473 with 0 producer ids in 18 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:17,802] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 12473 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:17,802] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 12473 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:17,803] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=12473, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000012473.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:17,813] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 10ms for snapshot load and 0ms for segment recovery from offset 12473 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:17,851] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-21 14:13:17,903] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 14:13:17,903] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 14:13:18,191] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 14:13:18,442] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=17, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from null (org.apache.kafka.raft.QuorumState)
[2025-05-21 14:13:18,463] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 14:13:18,491] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 14:13:18,566] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:18,594] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2025-05-21 14:13:18,685] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:18,718] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:18,734] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:13:18,742] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:13:18,737] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:13:18,744] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:13:18,773] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@440707552 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 14:13:18,825] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-21 14:13:18,825] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-21 14:13:18,831] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:19,027] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:19,037] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:19,019] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,062] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 14:13:19,072] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,127] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:19,168] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,170] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,173] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,173] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,178] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,180] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,237] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:19,267] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,293] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,302] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,304] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,307] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,314] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,346] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:19,476] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:19,479] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,495] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,518] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,525] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,528] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,529] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,586] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:19,712] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:19,826] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:19,934] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-21 14:13:19,937] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:19,975] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,977] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,989] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:19,996] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,001] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,020] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,024] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-21 14:13:20,035] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-21 14:13:20,050] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-21 14:13:20,061] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:20,091] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:20,106] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:20,164] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:20,190] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:20,194] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:20,198] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:20,237] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:20,239] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:20,243] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:20,265] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:20,266] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:20,282] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:20,286] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:20,366] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:20,470] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:20,521] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,535] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,549] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,552] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,555] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,556] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,572] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:20,645] INFO [BrokerLifecycleManager id=6] Incarnation xyfjyUmsSo-9XVrEe_yZkA of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:13:20,656] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:20,668] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:20,690] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,679] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:20,695] WARN [NodeToControllerChannelManager id=6 name=heartbeat] Connection to node 1 (kafka-controller-1/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,715] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:20,716] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:20,752] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:20,799] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:20,806] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,822] WARN [NodeToControllerChannelManager id=6 name=heartbeat] Connection to node 1 (kafka-controller-1/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,837] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:20,859] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:20,864] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,878] WARN [NodeToControllerChannelManager id=6 name=heartbeat] Connection to node 1 (kafka-controller-1/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:20,886] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:20,831] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-21 14:13:20,831] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:20,891] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-21 14:13:20,897] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-21 14:13:20,915] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:20,994] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,211] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,240] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:21,277] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:21,284] INFO [RaftManager id=6] Completed transition to Unattached(epoch=18, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=17, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-21 14:13:21,291] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:21,294] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:21,321] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,423] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,448] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=18, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=18, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-21 14:13:21,524] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,544] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:21,585] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:21,593] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:21,626] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,645] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:21,739] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,844] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,952] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,061] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,145] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=12477, metadata=Optional.empty)] for the first time for epoch 18 (org.apache.kafka.raft.FollowerState)
[2025-05-21 14:13:22,179] INFO [MetadataLoader id=6] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 12477 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,197] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 12477 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,845] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 12477 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,862] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 12476 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,867] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 12476 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,884] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 12476 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,901] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=12476, epoch=18) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-21 14:13:23,012] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:23,023] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:23,066] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:23,089] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:23,139] INFO Skipping recovery of 71 logs from /tmp/kafka-logs since clean shutdown file was found (kafka.log.LogManager)
[2025-05-21 14:13:23,312] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-13/00000000000000053728.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:23,326] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Loading producer state till offset 255889 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,329] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255889 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,336] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 12477 (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:13:23,342] INFO [ProducerStateManager partition=financial_transactions-13] Loading producer state from snapshot file 'SnapshotFile(offset=255889, file=/tmp/kafka-logs/financial_transactions-13/00000000000000255889.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:23,399] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Producer state recovery took 57ms for snapshot load and 0ms for segment recovery from offset 255889 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,436] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-13, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255889) with 1 segments, local-log-start-offset 0 and log-end-offset 255889 in 198ms (1/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:23,559] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-18/00000000000000053605.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:23,572] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Loading producer state till offset 255601 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,590] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255601 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,602] INFO [ProducerStateManager partition=financial_transactions-18] Loading producer state from snapshot file 'SnapshotFile(offset=255601, file=/tmp/kafka-logs/financial_transactions-18/00000000000000255601.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:23,622] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Producer state recovery took 20ms for snapshot load and 0ms for segment recovery from offset 255601 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,648] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-18, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255601) with 1 segments, local-log-start-offset 0 and log-end-offset 255601 in 191ms (2/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:23,675] INFO Deleted producer state snapshot /tmp/kafka-logs/__consumer_offsets-29/00000000000000000004.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:23,675] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 6 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,675] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 6 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,676] INFO [ProducerStateManager partition=__consumer_offsets-29] Loading producer state from snapshot file 'SnapshotFile(offset=6, file=/tmp/kafka-logs/__consumer_offsets-29/00000000000000000006.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:23,684] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Producer state recovery took 8ms for snapshot load and 0ms for segment recovery from offset 6 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,703] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-29, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=29, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=6) with 1 segments, local-log-start-offset 0 and log-end-offset 6 in 38ms (3/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:23,755] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-12/00000000000000053441.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:23,772] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Loading producer state till offset 255590 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,773] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255590 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,773] INFO [ProducerStateManager partition=financial_transactions-12] Loading producer state from snapshot file 'SnapshotFile(offset=255590, file=/tmp/kafka-logs/financial_transactions-12/00000000000000255590.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:23,779] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Producer state recovery took 6ms for snapshot load and 0ms for segment recovery from offset 255590 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,803] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-12, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255590) with 1 segments, local-log-start-offset 0 and log-end-offset 255590 in 85ms (4/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:23,889] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,915] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-38, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=38, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 107ms (5/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:23,958] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-16/00000000000000053208.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:23,963] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Loading producer state till offset 255412 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,968] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255412 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,985] INFO [ProducerStateManager partition=financial_transactions-16] Loading producer state from snapshot file 'SnapshotFile(offset=255412, file=/tmp/kafka-logs/financial_transactions-16/00000000000000255412.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:23,995] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Producer state recovery took 10ms for snapshot load and 0ms for segment recovery from offset 255412 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,042] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-16, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255412) with 1 segments, local-log-start-offset 0 and log-end-offset 255412 in 126ms (6/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,137] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,140] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-27, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=27, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 93ms (7/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,204] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,208] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-39, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=39, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 68ms (8/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,268] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,270] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-42, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=42, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 58ms (9/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,305] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,308] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-9, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 37ms (10/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,324] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,332] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-22, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=22, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 24ms (11/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,351] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,368] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-26, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=26, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 33ms (12/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,398] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,425] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-40, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=40, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 45ms (13/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,434] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,438] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-47, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=47, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (14/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,441] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,453] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-15, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (15/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,463] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,479] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-10, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 26ms (16/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,498] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-11/00000000000000053486.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:24,503] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Loading producer state till offset 256033 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,503] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 256033 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,503] INFO [ProducerStateManager partition=financial_transactions-11] Loading producer state from snapshot file 'SnapshotFile(offset=256033, file=/tmp/kafka-logs/financial_transactions-11/00000000000000256033.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:24,506] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 0ms for segment recovery from offset 256033 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,516] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-11, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=256033) with 1 segments, local-log-start-offset 0 and log-end-offset 256033 in 35ms (17/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,521] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,533] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-2, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (18/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,552] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,560] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-18, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 24ms (19/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,575] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,578] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-46, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=46, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (20/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,612] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-1/00000000000000053591.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:24,615] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Loading producer state till offset 255371 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,616] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255371 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,617] INFO [ProducerStateManager partition=financial_transactions-1] Loading producer state from snapshot file 'SnapshotFile(offset=255371, file=/tmp/kafka-logs/financial_transactions-1/00000000000000255371.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:24,618] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 255371 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,620] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-1, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255371) with 1 segments, local-log-start-offset 0 and log-end-offset 255371 in 25ms (21/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,624] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,636] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-12, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 16ms (22/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,656] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,658] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-23, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=23, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 21ms (23/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,662] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,671] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-14, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (24/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,676] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,684] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-5, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (25/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,701] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,732] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-41, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=41, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 48ms (26/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,746] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-17/00000000000000053580.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:24,752] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Loading producer state till offset 255866 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,764] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255866 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,768] INFO [ProducerStateManager partition=financial_transactions-17] Loading producer state from snapshot file 'SnapshotFile(offset=255866, file=/tmp/kafka-logs/financial_transactions-17/00000000000000255866.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:24,768] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 255866 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,794] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-17, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255866) with 1 segments, local-log-start-offset 0 and log-end-offset 255866 in 59ms (27/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,806] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,809] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-1, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (28/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,819] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,825] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-19, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (29/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,836] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,838] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-3, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (30/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,847] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,874] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-16, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 35ms (31/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,879] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,881] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-25, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=25, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (32/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,905] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-15/00000000000000053502.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:24,908] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Loading producer state till offset 256237 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,909] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 256237 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,913] INFO [ProducerStateManager partition=financial_transactions-15] Loading producer state from snapshot file 'SnapshotFile(offset=256237, file=/tmp/kafka-logs/financial_transactions-15/00000000000000256237.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:24,920] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Producer state recovery took 7ms for snapshot load and 0ms for segment recovery from offset 256237 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,924] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-15, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=256237) with 1 segments, local-log-start-offset 0 and log-end-offset 256237 in 43ms (33/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,954] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-3/00000000000000053622.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:24,966] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Loading producer state till offset 255715 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,968] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255715 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,970] INFO [ProducerStateManager partition=financial_transactions-3] Loading producer state from snapshot file 'SnapshotFile(offset=255715, file=/tmp/kafka-logs/financial_transactions-3/00000000000000255715.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:24,975] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Producer state recovery took 5ms for snapshot load and 0ms for segment recovery from offset 255715 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,980] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-3, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255715) with 1 segments, local-log-start-offset 0 and log-end-offset 255715 in 49ms (34/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,992] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-10/00000000000000053252.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:24,995] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Loading producer state till offset 256290 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,996] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 256290 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,996] INFO [ProducerStateManager partition=financial_transactions-10] Loading producer state from snapshot file 'SnapshotFile(offset=256290, file=/tmp/kafka-logs/financial_transactions-10/00000000000000256290.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:24,999] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 0ms for segment recovery from offset 256290 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,022] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-10, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=256290) with 1 segments, local-log-start-offset 0 and log-end-offset 256290 in 40ms (35/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,048] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-19/00000000000000053309.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,052] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Loading producer state till offset 255167 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,054] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255167 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,054] INFO [ProducerStateManager partition=financial_transactions-19] Loading producer state from snapshot file 'SnapshotFile(offset=255167, file=/tmp/kafka-logs/financial_transactions-19/00000000000000255167.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,055] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 255167 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,057] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-19, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255167) with 1 segments, local-log-start-offset 0 and log-end-offset 255167 in 23ms (36/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,079] INFO Deleted producer state snapshot /tmp/kafka-logs/_schemas-0/00000000000000000004.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,085] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 6 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,086] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 6 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,088] INFO [ProducerStateManager partition=_schemas-0] Loading producer state from snapshot file 'SnapshotFile(offset=6, file=/tmp/kafka-logs/_schemas-0/00000000000000000006.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,092] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Producer state recovery took 4ms for snapshot load and 0ms for segment recovery from offset 6 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,100] INFO Completed load of Log(dir=/tmp/kafka-logs/_schemas-0, topicId=RrE8eovWRKu4kLR3MRJ0fA, topic=_schemas, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=6) with 1 segments, local-log-start-offset 0 and log-end-offset 6 in 39ms (37/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,109] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,135] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-28, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=28, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 34ms (38/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,146] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,148] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-13, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (39/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,167] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,173] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-24, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=24, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 22ms (40/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,189] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,199] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-32, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=32, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 26ms (41/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,222] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-14/00000000000000053468.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,223] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Loading producer state till offset 255812 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,224] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255812 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,229] INFO [ProducerStateManager partition=financial_transactions-14] Loading producer state from snapshot file 'SnapshotFile(offset=255812, file=/tmp/kafka-logs/financial_transactions-14/00000000000000255812.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,231] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 255812 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,235] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-14, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255812) with 1 segments, local-log-start-offset 0 and log-end-offset 255812 in 34ms (42/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,251] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-2/00000000000000053173.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,252] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Loading producer state till offset 255541 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,252] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255541 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,252] INFO [ProducerStateManager partition=financial_transactions-2] Loading producer state from snapshot file 'SnapshotFile(offset=255541, file=/tmp/kafka-logs/financial_transactions-2/00000000000000255541.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,257] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Producer state recovery took 5ms for snapshot load and 0ms for segment recovery from offset 255541 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,266] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-2, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255541) with 1 segments, local-log-start-offset 0 and log-end-offset 255541 in 30ms (43/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,275] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,287] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-31, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=31, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 16ms (44/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,324] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,333] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-21, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=21, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 45ms (45/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,340] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-0/00000000000000053808.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,341] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 257211 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,341] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 257211 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,342] INFO [ProducerStateManager partition=financial_transactions-0] Loading producer state from snapshot file 'SnapshotFile(offset=257211, file=/tmp/kafka-logs/financial_transactions-0/00000000000000257211.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,343] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 257211 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,344] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-0, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=257211) with 1 segments, local-log-start-offset 0 and log-end-offset 257211 in 9ms (46/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,383] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-9/00000000000000053169.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,385] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Loading producer state till offset 255435 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,397] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255435 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,398] INFO [ProducerStateManager partition=financial_transactions-9] Loading producer state from snapshot file 'SnapshotFile(offset=255435, file=/tmp/kafka-logs/financial_transactions-9/00000000000000255435.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,399] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 255435 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,402] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-9, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255435) with 1 segments, local-log-start-offset 0 and log-end-offset 255435 in 56ms (47/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,422] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,425] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-6, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 20ms (48/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,428] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,431] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-11, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (49/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,440] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-5/00000000000000053395.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,442] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Loading producer state till offset 254990 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,442] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 254990 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,443] INFO [ProducerStateManager partition=financial_transactions-5] Loading producer state from snapshot file 'SnapshotFile(offset=254990, file=/tmp/kafka-logs/financial_transactions-5/00000000000000254990.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,444] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 254990 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,446] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-5, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=254990) with 1 segments, local-log-start-offset 0 and log-end-offset 254990 in 14ms (50/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,451] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,453] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-30, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=30, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (51/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,477] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-6/00000000000000053206.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,480] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Loading producer state till offset 255175 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,485] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255175 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,486] INFO [ProducerStateManager partition=financial_transactions-6] Loading producer state from snapshot file 'SnapshotFile(offset=255175, file=/tmp/kafka-logs/financial_transactions-6/00000000000000255175.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,487] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 255175 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,490] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-6, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255175) with 1 segments, local-log-start-offset 0 and log-end-offset 255175 in 35ms (52/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,497] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,500] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-43, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=43, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (53/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,519] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,527] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-7, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 22ms (54/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,537] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,541] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-33, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=33, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (55/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,554] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,560] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-36, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=36, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (56/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,583] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,586] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-45, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=45, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 23ms (57/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,605] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,607] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-0, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (58/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,614] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,617] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-20, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=20, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (59/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,623] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,626] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-4, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (60/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,631] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,635] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-8, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (61/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,645] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,647] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-49, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=49, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (62/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,657] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,659] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-34, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=34, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (63/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,672] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,675] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-48, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=48, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (64/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,688] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-4/00000000000000053258.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,688] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Loading producer state till offset 255335 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,688] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255335 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,689] INFO [ProducerStateManager partition=financial_transactions-4] Loading producer state from snapshot file 'SnapshotFile(offset=255335, file=/tmp/kafka-logs/financial_transactions-4/00000000000000255335.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,699] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Producer state recovery took 10ms for snapshot load and 0ms for segment recovery from offset 255335 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,709] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-4, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255335) with 1 segments, local-log-start-offset 0 and log-end-offset 255335 in 33ms (65/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,721] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,726] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-44, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=44, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (66/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,741] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,752] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-37, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=37, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 23ms (67/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,768] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-7/00000000000000053438.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,769] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Loading producer state till offset 255844 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,770] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255844 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,771] INFO [ProducerStateManager partition=financial_transactions-7] Loading producer state from snapshot file 'SnapshotFile(offset=255844, file=/tmp/kafka-logs/financial_transactions-7/00000000000000255844.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,772] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 255844 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,774] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-7, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255844) with 1 segments, local-log-start-offset 0 and log-end-offset 255844 in 19ms (68/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,778] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,781] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-35, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=35, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (69/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,794] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,796] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-17, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (70/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,815] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-8/00000000000000053130.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,822] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Loading producer state till offset 255525 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,826] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255525 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,827] INFO [ProducerStateManager partition=financial_transactions-8] Loading producer state from snapshot file 'SnapshotFile(offset=255525, file=/tmp/kafka-logs/financial_transactions-8/00000000000000255525.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,831] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Producer state recovery took 4ms for snapshot load and 0ms for segment recovery from offset 255525 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,834] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-8, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255525) with 1 segments, local-log-start-offset 0 and log-end-offset 255525 in 38ms (71/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,844] INFO Loaded 71 logs in 2815ms (kafka.log.LogManager)
[2025-05-21 14:13:25,849] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-21 14:13:25,851] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-21 14:13:25,874] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-21 14:13:26,107] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 14:13:26,119] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 14:13:26,133] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 14:13:26,141] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,145] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,153] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 14:13:26,160] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 14:13:26,160] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 14:13:26,184] INFO [Broker id=6] Transitioning 71 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:26,216] INFO [Broker id=6] Creating new partition financial_transactions-13 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,246] INFO [Partition financial_transactions-13 broker=6] Log loaded for partition financial_transactions-13 with initial high watermark 255874 (kafka.cluster.Partition)
[2025-05-21 14:13:26,249] INFO [Broker id=6] Follower financial_transactions-13 starts at leader epoch 7 from offset 255889 with partition epoch 10 and high watermark 255874. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:13:26,259] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,261] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,264] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,265] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,266] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,267] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,267] INFO [Broker id=6] Creating new partition financial_transactions-17 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,268] INFO [Partition financial_transactions-17 broker=6] Log loaded for partition financial_transactions-17 with initial high watermark 255866 (kafka.cluster.Partition)
[2025-05-21 14:13:26,269] INFO [Broker id=6] Follower financial_transactions-17 starts at leader epoch 4 from offset 255866 with partition epoch 9 and high watermark 255866. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 14:13:26,269] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,280] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,281] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,281] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,284] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,284] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,285] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,287] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,288] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,288] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,289] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,291] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,294] INFO [Broker id=6] Creating new partition financial_transactions-0 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,301] INFO [Partition financial_transactions-0 broker=6] Log loaded for partition financial_transactions-0 with initial high watermark 257197 (kafka.cluster.Partition)
[2025-05-21 14:13:26,301] INFO [Broker id=6] Follower financial_transactions-0 starts at leader epoch 5 from offset 257211 with partition epoch 9 and high watermark 257197. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,302] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,304] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,305] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,305] INFO [Broker id=6] Creating new partition financial_transactions-4 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,307] INFO [Partition financial_transactions-4 broker=6] Log loaded for partition financial_transactions-4 with initial high watermark 255335 (kafka.cluster.Partition)
[2025-05-21 14:13:26,308] INFO [Broker id=6] Follower financial_transactions-4 starts at leader epoch 5 from offset 255335 with partition epoch 10 and high watermark 255335. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,308] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,309] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,310] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,311] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,319] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,322] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,323] INFO [Broker id=6] Creating new partition financial_transactions-8 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,324] INFO [Partition financial_transactions-8 broker=6] Log loaded for partition financial_transactions-8 with initial high watermark 255525 (kafka.cluster.Partition)
[2025-05-21 14:13:26,330] INFO [Broker id=6] Follower financial_transactions-8 starts at leader epoch 7 from offset 255525 with partition epoch 10 and high watermark 255525. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:13:26,331] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,332] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,332] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,333] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,337] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,341] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,342] INFO [Broker id=6] Creating new partition financial_transactions-12 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,343] INFO [Partition financial_transactions-12 broker=6] Log loaded for partition financial_transactions-12 with initial high watermark 255590 (kafka.cluster.Partition)
[2025-05-21 14:13:26,344] INFO [Broker id=6] Follower financial_transactions-12 starts at leader epoch 5 from offset 255590 with partition epoch 9 and high watermark 255590. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,344] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,349] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,350] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,351] INFO [Broker id=6] Creating new partition financial_transactions-14 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,355] INFO [Partition financial_transactions-14 broker=6] Log loaded for partition financial_transactions-14 with initial high watermark 255812 (kafka.cluster.Partition)
[2025-05-21 14:13:26,358] INFO [Broker id=6] Follower financial_transactions-14 starts at leader epoch 5 from offset 255812 with partition epoch 10 and high watermark 255812. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,360] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,362] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,363] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,365] INFO [Broker id=6] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-21 14:13:26,366] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 6 (kafka.cluster.Partition)
[2025-05-21 14:13:26,367] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 15 from offset 6 with partition epoch 25 and high watermark 6. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,367] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,371] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,371] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,372] INFO [Broker id=6] Creating new partition financial_transactions-18 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,378] INFO [Partition financial_transactions-18 broker=6] Log loaded for partition financial_transactions-18 with initial high watermark 255590 (kafka.cluster.Partition)
[2025-05-21 14:13:26,379] INFO [Broker id=6] Follower financial_transactions-18 starts at leader epoch 4 from offset 255601 with partition epoch 9 and high watermark 255590. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 14:13:26,379] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,384] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,384] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,384] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,386] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,387] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,387] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,388] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,390] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,390] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,395] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,397] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,415] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,417] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,417] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,418] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,419] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,420] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,420] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,426] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 6 (kafka.cluster.Partition)
[2025-05-21 14:13:26,428] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 15 from offset 6 with partition epoch 25 and high watermark 6. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,430] INFO [Broker id=6] Creating new partition financial_transactions-1 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,432] INFO [Partition financial_transactions-1 broker=6] Log loaded for partition financial_transactions-1 with initial high watermark 255371 (kafka.cluster.Partition)
[2025-05-21 14:13:26,433] INFO [Broker id=6] Follower financial_transactions-1 starts at leader epoch 7 from offset 255371 with partition epoch 10 and high watermark 255371. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:13:26,433] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,445] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,445] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,446] INFO [Broker id=6] Creating new partition financial_transactions-5 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,447] INFO [Partition financial_transactions-5 broker=6] Log loaded for partition financial_transactions-5 with initial high watermark 254990 (kafka.cluster.Partition)
[2025-05-21 14:13:26,447] INFO [Broker id=6] Follower financial_transactions-5 starts at leader epoch 6 from offset 254990 with partition epoch 10 and high watermark 254990. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:26,447] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,448] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,449] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,449] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,450] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,451] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,451] INFO [Broker id=6] Creating new partition financial_transactions-9 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,454] INFO [Partition financial_transactions-9 broker=6] Log loaded for partition financial_transactions-9 with initial high watermark 255435 (kafka.cluster.Partition)
[2025-05-21 14:13:26,456] INFO [Broker id=6] Follower financial_transactions-9 starts at leader epoch 6 from offset 255435 with partition epoch 10 and high watermark 255435. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:26,457] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,462] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,465] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,466] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,467] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,467] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,467] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,470] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,471] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,473] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,478] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,478] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,479] INFO [Broker id=6] Creating new partition financial_transactions-15 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,481] INFO [Partition financial_transactions-15 broker=6] Log loaded for partition financial_transactions-15 with initial high watermark 256237 (kafka.cluster.Partition)
[2025-05-21 14:13:26,481] INFO [Broker id=6] Follower financial_transactions-15 starts at leader epoch 5 from offset 256237 with partition epoch 10 and high watermark 256237. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,481] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,483] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,483] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,484] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,486] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,488] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,490] INFO [Broker id=6] Creating new partition financial_transactions-19 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,494] INFO [Partition financial_transactions-19 broker=6] Log loaded for partition financial_transactions-19 with initial high watermark 255167 (kafka.cluster.Partition)
[2025-05-21 14:13:26,496] INFO [Broker id=6] Follower financial_transactions-19 starts at leader epoch 5 from offset 255167 with partition epoch 10 and high watermark 255167. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,499] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,501] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,501] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,501] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,503] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,504] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,504] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,514] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,518] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,518] INFO [Broker id=6] Creating new partition financial_transactions-2 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,519] INFO [Partition financial_transactions-2 broker=6] Log loaded for partition financial_transactions-2 with initial high watermark 255541 (kafka.cluster.Partition)
[2025-05-21 14:13:26,519] INFO [Broker id=6] Follower financial_transactions-2 starts at leader epoch 5 from offset 255541 with partition epoch 10 and high watermark 255541. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,520] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,521] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,522] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,522] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,523] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,524] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,524] INFO [Broker id=6] Creating new partition financial_transactions-6 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,526] INFO [Partition financial_transactions-6 broker=6] Log loaded for partition financial_transactions-6 with initial high watermark 255168 (kafka.cluster.Partition)
[2025-05-21 14:13:26,527] INFO [Broker id=6] Follower financial_transactions-6 starts at leader epoch 5 from offset 255175 with partition epoch 10 and high watermark 255168. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,532] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,540] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,542] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,542] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,548] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,550] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,551] INFO [Broker id=6] Creating new partition financial_transactions-10 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,556] INFO [Partition financial_transactions-10 broker=6] Log loaded for partition financial_transactions-10 with initial high watermark 256290 (kafka.cluster.Partition)
[2025-05-21 14:13:26,559] INFO [Broker id=6] Follower financial_transactions-10 starts at leader epoch 4 from offset 256290 with partition epoch 9 and high watermark 256290. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 14:13:26,560] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,563] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,564] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,565] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,571] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,572] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,573] INFO [Broker id=6] Creating new partition financial_transactions-16 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,575] INFO [Partition financial_transactions-16 broker=6] Log loaded for partition financial_transactions-16 with initial high watermark 255412 (kafka.cluster.Partition)
[2025-05-21 14:13:26,575] INFO [Broker id=6] Follower financial_transactions-16 starts at leader epoch 6 from offset 255412 with partition epoch 10 and high watermark 255412. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:26,575] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,577] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,578] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,578] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,580] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,581] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,581] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,583] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,583] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,584] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,586] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,587] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,588] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,589] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,590] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,591] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,601] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,603] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,604] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,605] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,606] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,606] INFO [Broker id=6] Creating new partition financial_transactions-3 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,611] INFO [Partition financial_transactions-3 broker=6] Log loaded for partition financial_transactions-3 with initial high watermark 255715 (kafka.cluster.Partition)
[2025-05-21 14:13:26,614] INFO [Broker id=6] Follower financial_transactions-3 starts at leader epoch 4 from offset 255715 with partition epoch 9 and high watermark 255715. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 14:13:26,616] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,619] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,620] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,621] INFO [Broker id=6] Creating new partition financial_transactions-7 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,622] INFO [Partition financial_transactions-7 broker=6] Log loaded for partition financial_transactions-7 with initial high watermark 255844 (kafka.cluster.Partition)
[2025-05-21 14:13:26,623] INFO [Broker id=6] Follower financial_transactions-7 starts at leader epoch 5 from offset 255844 with partition epoch 9 and high watermark 255844. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,623] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,624] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,625] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,625] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,626] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,627] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,627] INFO [Broker id=6] Creating new partition financial_transactions-11 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,628] INFO [Partition financial_transactions-11 broker=6] Log loaded for partition financial_transactions-11 with initial high watermark 256033 (kafka.cluster.Partition)
[2025-05-21 14:13:26,628] INFO [Broker id=6] Follower financial_transactions-11 starts at leader epoch 5 from offset 256033 with partition epoch 10 and high watermark 256033. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,629] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,630] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,630] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,633] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-6, __consumer_offsets-35, financial_transactions-11, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:13:26,634] INFO [Broker id=6] Stopped fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-21 14:13:26,647] INFO [Broker id=6] Started fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-21 14:13:26,720] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,721] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,723] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,723] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,724] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,726] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,729] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,726] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,730] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,732] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,732] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,734] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,740] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,741] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,742] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,743] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,743] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,734] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,744] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,747] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,745] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,749] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,750] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,750] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,748] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,751] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,751] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,752] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,752] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,754] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,764] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,765] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,769] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,770] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,770] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,770] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,771] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,771] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,771] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,772] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,772] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,772] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,772] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,773] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,773] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,774] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,775] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,775] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,774] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,790] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,793] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,794] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,795] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,795] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,790] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,807] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,807] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,808] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,808] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,808] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,808] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,809] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,813] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,807] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,815] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,816] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,817] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,817] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,818] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,817] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,825] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,822] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,826] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,827] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,828] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,827] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,828] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,828] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,829] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,831] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,831] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,832] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,829] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,833] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,833] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,833] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,834] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,834] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,835] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,837] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,835] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,839] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,840] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,840] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,838] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,841] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,841] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,841] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,842] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,842] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,843] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,843] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,845] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,843] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,846] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,847] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,847] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,848] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,847] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,850] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,852] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,852] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,853] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,854] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,855] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,855] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,856] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,857] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,857] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,854] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,859] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,860] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,860] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,858] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,865] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,866] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,868] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,875] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,880] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,888] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,895] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,896] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,896] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,897] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,897] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,898] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,898] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,899] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,880] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,899] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,901] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,901] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,901] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,900] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,903] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,904] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,904] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,905] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,905] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,906] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,911] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-21 14:13:26,928] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-21 14:13:26,950] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 12476 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:26,984] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:13:26,999] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-21 14:13:26,999] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-21 14:13:27,000] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-21 14:13:27,001] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-21 14:13:27,003] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:27,007] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-21 14:13:27,195] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:13:27,196] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-21 14:13:27,208] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-21 14:13:27,208] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-21 14:13:27,209] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2025-05-21 14:13:27,226] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-21 14:13:27,232] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-21 14:13:27,280] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-21 14:13:27,283] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-21 14:13:27,283] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-21 14:13:27,284] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-21 14:13:27,284] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-21 14:13:27,285] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 14:13:27,294] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 14:13:27,294] INFO Kafka startTimeMs: 1747836807285 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 14:13:27,299] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-21 14:13:27,485] INFO [Broker id=6] Transitioning 71 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:27,486] INFO [Broker id=6] Follower financial_transactions-13 starts at leader epoch 8 from offset 255889 with partition epoch 11 and high watermark 255874. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:13:27,496] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 15 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,505] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 15 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,512] INFO [Broker id=6] Follower financial_transactions-17 starts at leader epoch 5 from offset 255866 with partition epoch 10 and high watermark 255866. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:27,514] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 11 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:13:27,521] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 11 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:13:27,525] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,541] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,552] INFO [Broker id=6] Follower financial_transactions-0 starts at leader epoch 6 from offset 257211 with partition epoch 10 and high watermark 257197. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,561] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 15 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,572] INFO [Broker id=6] Follower financial_transactions-4 starts at leader epoch 6 from offset 255335 with partition epoch 11 and high watermark 255335. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,575] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 16 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,592] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 16 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,628] INFO [Broker id=6] Follower financial_transactions-8 starts at leader epoch 8 from offset 255525 with partition epoch 11 and high watermark 255525. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:13:27,631] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 11 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:13:27,640] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 13 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:13:27,642] INFO [Broker id=6] Follower financial_transactions-12 starts at leader epoch 6 from offset 255590 with partition epoch 10 and high watermark 255590. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,643] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 15 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,645] INFO [Broker id=6] Follower financial_transactions-14 starts at leader epoch 6 from offset 255812 with partition epoch 11 and high watermark 255812. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,655] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 16 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,656] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 16 from offset 6 with partition epoch 26 and high watermark 6. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,657] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,685] INFO [Broker id=6] Follower financial_transactions-18 starts at leader epoch 5 from offset 255601 with partition epoch 10 and high watermark 255590. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:27,691] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,696] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 13 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:13:27,705] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,706] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 13 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:13:27,716] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 16 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,720] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,722] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 16 from offset 6 with partition epoch 26 and high watermark 6. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,725] INFO [Broker id=6] Follower financial_transactions-1 starts at leader epoch 8 from offset 255371 with partition epoch 11 and high watermark 255371. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:13:27,730] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 13 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:13:27,733] INFO [Broker id=6] Follower financial_transactions-5 starts at leader epoch 7 from offset 254990 with partition epoch 11 and high watermark 254990. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:13:27,734] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 11 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:13:27,737] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 15 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,743] INFO [Broker id=6] Follower financial_transactions-9 starts at leader epoch 7 from offset 255435 with partition epoch 11 and high watermark 255435. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:13:27,759] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 13 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:13:27,760] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,760] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 13 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:13:27,761] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 13 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:13:27,764] INFO [Broker id=6] Follower financial_transactions-15 starts at leader epoch 6 from offset 256237 with partition epoch 11 and high watermark 256237. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,768] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 15 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,772] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 15 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,773] INFO [Broker id=6] Follower financial_transactions-19 starts at leader epoch 6 from offset 255167 with partition epoch 11 and high watermark 255167. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,774] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 16 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,774] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,775] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,776] INFO [Broker id=6] Follower financial_transactions-2 starts at leader epoch 6 from offset 255541 with partition epoch 11 and high watermark 255541. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,777] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 13 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:13:27,777] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 15 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,777] INFO [Broker id=6] Follower financial_transactions-6 starts at leader epoch 6 from offset 255175 with partition epoch 11 and high watermark 255168. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,778] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,778] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,782] INFO [Broker id=6] Follower financial_transactions-10 starts at leader epoch 5 from offset 256290 with partition epoch 10 and high watermark 256290. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:27,782] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,783] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 11 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:13:27,784] INFO [Broker id=6] Follower financial_transactions-16 starts at leader epoch 7 from offset 255412 with partition epoch 11 and high watermark 255412. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:13:27,785] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 11 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:13:27,790] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,796] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,801] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 13 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:13:27,802] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 16 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,803] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 11 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:13:27,804] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,816] INFO [Broker id=6] Follower financial_transactions-3 starts at leader epoch 5 from offset 255715 with partition epoch 10 and high watermark 255715. Current leader is 5. Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:27,819] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 16 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,820] INFO [Broker id=6] Follower financial_transactions-7 starts at leader epoch 6 from offset 255844 with partition epoch 10 and high watermark 255844. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,820] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 10 from offset 0 with partition epoch 25 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,821] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 11 from offset 0 with partition epoch 24 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:13:27,822] INFO [Broker id=6] Follower financial_transactions-11 starts at leader epoch 6 from offset 256033 with partition epoch 11 and high watermark 256033. Current leader is 5. Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:27,822] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 16 from offset 0 with partition epoch 26 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:13:27,824] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-6, __consumer_offsets-35, financial_transactions-11, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:13:27,825] INFO [Broker id=6] Stopped fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-21 14:13:27,886] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,901] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(financial_transactions-13 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,255889), __consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), financial_transactions-17 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,255866), __consumer_offsets-9 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,0), __consumer_offsets-42 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,0), __consumer_offsets-21 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-17 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), financial_transactions-0 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,257211), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), financial_transactions-4 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,255335), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,0), financial_transactions-8 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,255525), __consumer_offsets-38 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,0), __consumer_offsets-1 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), financial_transactions-12 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,255590), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), financial_transactions-14 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,255812), __consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,0), _schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,6), __consumer_offsets-45 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), financial_transactions-18 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,255601), __consumer_offsets-12 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-41 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), __consumer_offsets-24 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-20 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,0), __consumer_offsets-0 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,6), financial_transactions-1 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,255371), __consumer_offsets-25 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), financial_transactions-5 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,254990), __consumer_offsets-8 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), financial_transactions-9 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,255435), __consumer_offsets-4 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), __consumer_offsets-33 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-15 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), __consumer_offsets-48 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), financial_transactions-15 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,256237), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), financial_transactions-19 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,255167), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,0), __consumer_offsets-19 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-32 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), financial_transactions-2 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,255541), __consumer_offsets-28 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), financial_transactions-6 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,255175), __consumer_offsets-40 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-3 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), financial_transactions-10 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,256290), __consumer_offsets-36 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-47 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,0), financial_transactions-16 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),7,255412), __consumer_offsets-14 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,0), __consumer_offsets-43 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-10 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-22 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),13,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,0), __consumer_offsets-31 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,0), __consumer_offsets-27 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), financial_transactions-3 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),5,255715), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,0), financial_transactions-7 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,255844), __consumer_offsets-6 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),10,0), __consumer_offsets-35 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,0), financial_transactions-11 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),6,256033), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),16,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:13:27,902] INFO [Broker id=6] Started fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-21 14:13:27,904] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,906] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,909] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,910] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,911] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,911] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,911] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,912] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,916] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,917] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,921] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,922] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,922] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,923] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,923] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,934] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,938] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,941] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,942] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,953] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,961] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,967] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,969] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,970] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,972] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,973] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,974] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,974] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,975] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,982] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,986] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,987] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,988] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,988] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,989] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,989] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,990] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,990] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,991] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,991] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:27,992] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:27,992] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,002] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,004] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,004] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,006] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,007] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,007] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,008] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,009] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,009] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,010] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,011] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,012] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,012] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,013] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,013] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,014] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,014] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,015] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,015] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,015] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,017] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,020] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,022] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,023] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,024] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,024] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,027] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,028] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,028] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,029] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,029] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,030] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,030] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,036] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,038] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,038] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,038] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,039] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,040] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,044] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,045] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,045] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,045] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,046] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,046] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,046] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,047] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,054] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,058] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,059] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,059] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,061] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,065] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,068] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,069] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,070] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,093] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,097] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,097] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,098] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,098] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,099] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,099] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,099] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,100] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,102] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,104] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,101] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,106] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,105] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,109] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,109] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,109] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,113] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,113] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,119] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,120] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,121] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,121] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,122] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,122] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,122] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,123] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,123] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,124] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,124] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,124] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,125] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,125] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,125] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,126] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,126] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,126] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,127] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,127] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,128] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,128] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,128] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,129] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,131] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,132] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,129] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,135] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,135] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,133] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,137] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,137] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,138] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,138] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,139] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,140] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,140] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,142] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,143] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,144] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,144] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,143] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,144] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,145] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,145] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,145] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,146] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,147] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,147] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,149] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,149] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,148] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,150] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,150] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,151] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,151] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,151] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,152] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,152] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,153] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,152] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,154] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,154] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,154] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,155] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,155] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,155] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,173] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,173] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,180] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,185] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,195] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,200] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,204] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,204] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,204] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,205] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,205] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,185] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,216] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,216] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,216] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,216] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,218] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,219] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,219] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,219] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,219] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,220] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,220] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,220] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,221] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,221] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,221] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,222] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,221] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,222] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,223] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,227] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,225] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,232] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,235] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,236] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,236] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,236] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,237] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,237] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,237] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,232] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,238] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,240] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,239] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,249] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,245] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,255] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,259] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,265] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,266] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,266] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,266] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,266] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,267] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,267] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,251] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,269] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,272] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,279] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,282] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,284] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,284] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,285] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,359] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:28,371] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 6], partitionEpoch=12, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 14:13:28,421] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-18 with TruncationState(offset=255590, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=18, leaderEpoch=2, endOffset=255590) (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:13:28,429] INFO [UnifiedLog partition=financial_transactions-18, dir=/tmp/kafka-logs] Truncating to offset 255590 (kafka.log.UnifiedLog)
[2025-05-21 14:13:28,451] INFO [UnifiedLog partition=financial_transactions-18, dir=/tmp/kafka-logs] Loading producer state till offset 255590 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,452] INFO [UnifiedLog partition=financial_transactions-18, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255590 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,452] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-18/00000000000000255601.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:28,482] INFO [ProducerStateManager partition=financial_transactions-18] Wrote producer snapshot at offset 255590 with 0 producer ids in 19 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:28,485] INFO [UnifiedLog partition=financial_transactions-18, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 32ms for segment recovery from offset 255590 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:28,672] INFO [Broker id=6] Transitioning 40 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:28,693] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:28,697] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:28,712] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:28,713] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:28,713] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:28,714] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=11, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 14:13:28,715] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:28,716] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:28,717] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:28,717] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:28,718] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:28,719] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=11, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:28,720] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:28,723] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:28,724] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:28,735] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:28,746] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 6], partitionEpoch=12, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 14:13:28,752] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:28,759] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:28,763] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=11, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:28,765] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:28,770] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:28,772] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:28,774] INFO [Broker id=6] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:28,776] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:28,784] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:28,793] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:28,793] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:28,794] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:28,797] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:28,805] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:28,808] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:28,815] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 6], partitionEpoch=12, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 14:13:28,818] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:28,820] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=12, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 14:13:28,821] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:28,825] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:28,829] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=12, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 14:13:28,833] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:28,838] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:28,840] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,841] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,841] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,841] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,842] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,842] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,842] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,842] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,842] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,843] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,843] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,843] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,843] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,843] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,844] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,844] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,844] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,844] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,844] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,844] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,845] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,845] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,845] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,846] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,846] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,846] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,846] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,846] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,847] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,847] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,847] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,847] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,845] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,848] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,848] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,847] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,849] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,849] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,849] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,849] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,850] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,853] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,859] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,851] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,875] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,876] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,876] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,876] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,876] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,877] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,877] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,877] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,878] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,878] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,878] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,879] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,879] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,879] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,879] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,880] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,880] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,880] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,880] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,880] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,881] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,881] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,881] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,882] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,882] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,882] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,883] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,883] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,883] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,884] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,884] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,884] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,884] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,885] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,885] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,896] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,901] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,901] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,905] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,908] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:28,908] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,908] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:28,910] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,179] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:29,180] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=11, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 14:13:29,577] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:29,585] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 14:13:29,675] INFO [Broker id=6] Transitioning 39 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:29,676] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:29,686] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:29,695] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:29,700] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:29,701] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 14:13:29,701] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:29,701] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:29,702] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,702] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,703] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:29,703] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:29,703] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:29,704] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:29,704] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:29,712] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 14:13:29,716] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:29,719] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:29,722] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:29,724] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:29,756] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:29,759] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:29,776] INFO [Broker id=6] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:29,782] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,782] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 14:13:29,783] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,788] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:29,788] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,789] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:29,790] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:29,793] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,793] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:29,794] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 14:13:29,795] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:29,796] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 14:13:29,797] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:29,799] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:29,800] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 14:13:29,801] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:29,801] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,802] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,802] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,803] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,803] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,803] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,803] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,804] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,804] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,804] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,805] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,806] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,806] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,807] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,808] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,808] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,809] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,809] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,809] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,810] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,810] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,815] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,817] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,817] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,818] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,818] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,819] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,819] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,820] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,820] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,821] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,824] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,824] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,824] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,825] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,805] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,825] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,826] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,825] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,826] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,827] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,826] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,827] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,827] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,827] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,828] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,828] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,828] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,829] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,832] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,828] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,834] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,834] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,834] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,837] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,839] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,839] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,840] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,841] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,842] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,843] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,843] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,844] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,845] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,850] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,852] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,853] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,836] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,858] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,860] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,861] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,863] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,863] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,864] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,864] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,865] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,865] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,867] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,868] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,868] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,869] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,870] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,870] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,870] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,870] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,911] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:29,912] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:29,914] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,915] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,918] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,992] INFO [Broker id=6] Transitioning 26 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:29,993] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:29,995] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:29,996] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,997] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:29,998] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,009] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:30,022] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:30,024] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,025] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,026] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,027] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,029] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:30,029] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=12, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 14:13:30,031] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:30,032] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,036] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,039] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:30,056] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:30,058] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:30,059] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,060] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=11, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 14:13:30,060] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:30,061] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=11, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,061] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,062] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:30,062] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:30,063] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,063] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,064] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,064] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,065] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,066] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,067] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,069] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,071] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,072] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,071] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,075] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,073] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,077] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,077] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,078] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,077] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,078] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,078] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,080] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,081] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,081] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,081] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,082] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,083] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,084] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,085] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,087] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,090] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,086] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,095] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,096] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,097] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,099] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,098] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,103] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,102] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,113] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,115] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,115] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,115] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,115] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,115] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,116] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,116] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,116] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,117] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,117] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,118] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,118] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,119] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,119] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,123] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,127] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,131] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,127] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,135] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,135] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,136] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,136] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,138] INFO [Broker id=6] Transitioning 2 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:30,139] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=11, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 14:13:30,140] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,164] INFO [Broker id=6] Transitioning 28 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:30,165] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,166] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:30,169] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:30,170] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,174] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:30,175] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,176] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,176] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,177] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:30,177] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-21 14:13:30,178] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,179] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,179] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,180] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,180] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:30,180] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:30,181] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,181] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,181] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 14:13:30,182] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:30,182] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:30,182] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,184] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 14:13:30,185] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:30,193] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,201] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:13:30,202] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 14:13:30,204] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:13:30,215] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,216] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,217] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,218] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,219] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,219] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,219] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,220] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,221] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,221] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,226] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,232] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,232] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,232] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,233] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,233] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,238] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,239] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,239] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,241] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,241] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,241] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,242] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,243] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,243] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,244] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,244] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,244] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,245] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,246] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,246] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,246] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,247] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,247] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,248] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,248] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,248] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,257] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,258] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,258] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,260] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,261] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,264] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,264] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,265] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,265] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,266] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,270] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,270] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,271] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,271] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,271] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,272] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,272] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,272] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,273] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,273] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,273] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,274] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,275] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,275] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,289] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,292] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,292] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:30,294] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,295] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:30,710] INFO [Broker id=6] Transitioning 3 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:30,713] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 14:13:30,714] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:13:30,714] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 14:16:06,309] INFO [NodeToControllerChannelManager id=6 name=forwarding] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:16:06,314] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:18:23,601] INFO [Broker id=6] Transitioning 24 partition(s) to local leaders. (state.change.logger)
[2025-05-21 14:18:23,607] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-47, __consumer_offsets-48, __consumer_offsets-14, financial_transactions-17, financial_transactions-18, __consumer_offsets-9, __consumer_offsets-41, __consumer_offsets-42, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-31, financial_transactions-0, __consumer_offsets-28, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-8, financial_transactions-7, __consumer_offsets-38, __consumer_offsets-35, financial_transactions-10, __consumer_offsets-4, __consumer_offsets-1, financial_transactions-12) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:18:23,614] INFO [Broker id=6] Leader __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 14 from offset 0 with partition epoch 27, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:18:23,621] INFO [Broker id=6] Leader __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 27, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:18:23,626] INFO [Broker id=6] Leader __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 14 from offset 0 with partition epoch 27, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:18:23,631] INFO [Broker id=6] Leader __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 27, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:18:23,635] INFO [Broker id=6] Leader financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 6 from offset 268373 with partition epoch 13, high watermark 268373, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:18:23,641] INFO [Broker id=6] Leader financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 6 from offset 267868 with partition epoch 13, high watermark 267868, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:18:23,646] INFO [Broker id=6] Leader __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 27, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:18:23,651] INFO [Broker id=6] Leader __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 14 from offset 0 with partition epoch 27, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:18:23,654] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-13 has an older epoch (15) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,655] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-13 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,657] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-46 has an older epoch (15) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,657] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-46 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,658] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-30 has an older epoch (15) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,659] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-30 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,657] INFO [Broker id=6] Leader __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 27, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:18:23,659] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-26 has an older epoch (16) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,660] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-26 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,660] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-5 has an older epoch (16) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,661] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-5 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,662] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-34 has an older epoch (15) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,663] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-34 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,663] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-16 has an older epoch (16) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,663] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-16 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,664] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-49 has an older epoch (16) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,664] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-49 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,665] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-37 has an older epoch (15) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,665] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-37 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,665] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-11 has an older epoch (15) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,666] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-11 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,667] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-44 has an older epoch (15) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,668] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-44 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,669] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-23 has an older epoch (16) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,665] INFO [Broker id=6] Leader __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 14 from offset 0 with partition epoch 27, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:18:23,670] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-23 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,671] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-7 has an older epoch (15) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,672] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-7 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,673] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-18 has an older epoch (16) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,674] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-18 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,675] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-39 has an older epoch (16) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,675] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-39 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,676] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-2 has an older epoch (16) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,676] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-2 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,677] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-29 has an older epoch (16) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,678] INFO [Broker id=6] Leader __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 14 from offset 0 with partition epoch 27, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:18:23,678] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-29 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,680] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition _schemas-0 has an older epoch (16) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,681] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition _schemas-0 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,681] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-9 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,682] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-9 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,682] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-16 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,682] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-16 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,683] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-1 has an older epoch (8) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,683] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-1 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,684] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-5 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,684] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-5 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,684] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-13 has an older epoch (8) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,685] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-13 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,685] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-8 has an older epoch (8) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,685] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-8 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,685] INFO [Broker id=6] Leader __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 27, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:18:23,692] INFO [Broker id=6] Leader financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 7 from offset 269651 with partition epoch 13, high watermark 269651, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:18:23,699] INFO [Broker id=6] Leader __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 14 from offset 0 with partition epoch 27, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:18:23,704] INFO [Broker id=6] Leader financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 6 from offset 268318 with partition epoch 13, high watermark 268318, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:18:23,711] INFO [Broker id=6] Leader __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 14 from offset 0 with partition epoch 27, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:18:23,717] INFO [Broker id=6] Leader __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 27, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:18:23,721] INFO [Broker id=6] Leader financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 7 from offset 268422 with partition epoch 13, high watermark 268422, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:18:23,726] INFO [Broker id=6] Leader __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 27, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:18:23,730] INFO [Broker id=6] Leader __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 27, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:18:23,734] INFO [Broker id=6] Leader financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 6 from offset 268988 with partition epoch 13, high watermark 268988, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:18:23,739] INFO [Broker id=6] Leader __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 14 from offset 0 with partition epoch 27, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:18:23,743] INFO [Broker id=6] Leader __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 14 from offset 0 with partition epoch 27, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:18:23,748] INFO [Broker id=6] Leader financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 7 from offset 268072 with partition epoch 13, high watermark 268072, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:18:23,752] INFO [Broker id=6] Transitioning 24 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:18:23,752] INFO [Broker id=6] Follower financial_transactions-13 starts at leader epoch 9 from offset 268526 with partition epoch 14 and high watermark 268526. Current leader is 4. Previous leader Some(4) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:18:23,753] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 17 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,753] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 17 from offset 8 with partition epoch 29 and high watermark 8. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,754] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 16 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,754] INFO [Broker id=6] Follower financial_transactions-16 starts at leader epoch 8 from offset 267900 with partition epoch 14 and high watermark 267900. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:18:23,754] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 16 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,754] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 16 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,755] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 16 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,755] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 17 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,755] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 17 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,755] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 17 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,756] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 17 from offset 7 with partition epoch 29 and high watermark 7. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,756] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 16 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,756] INFO [Broker id=6] Follower financial_transactions-1 starts at leader epoch 9 from offset 267918 with partition epoch 14 and high watermark 267918. Current leader is 4. Previous leader Some(4) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:18:23,756] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 17 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,757] INFO [Broker id=6] Follower financial_transactions-5 starts at leader epoch 8 from offset 267422 with partition epoch 14 and high watermark 267422. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:18:23,757] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 16 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,757] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 17 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,757] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 17 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,758] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 16 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,758] INFO [Broker id=6] Follower financial_transactions-8 starts at leader epoch 9 from offset 267896 with partition epoch 14 and high watermark 267896. Current leader is 4. Previous leader Some(4) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:18:23,758] INFO [Broker id=6] Follower financial_transactions-9 starts at leader epoch 8 from offset 268043 with partition epoch 14 and high watermark 268043. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:18:23,759] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 17 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,759] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 16 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,760] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, financial_transactions-16, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-29, financial_transactions-1, financial_transactions-5, __consumer_offsets-39, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:18:23,760] INFO [Broker id=6] Stopped fetchers as part of become-follower for 24 partitions (state.change.logger)
[2025-05-21 14:18:23,763] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,763] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(financial_transactions-13 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),9,268526), __consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),16,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),16,0), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),16,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),16,0), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,0), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),16,0), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),16,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,0), financial_transactions-8 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),9,267896), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),16,0), __consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,0), _schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,8), financial_transactions-16 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,267900), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,7), financial_transactions-1 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),9,267918), financial_transactions-5 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,267422), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),16,0), financial_transactions-9 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,268043), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:18:23,764] INFO [Broker id=6] Started fetchers as part of become-follower for 24 partitions (state.change.logger)
[2025-05-21 14:18:23,764] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,764] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,764] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,765] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,765] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,765] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,765] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,766] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,766] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,766] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,766] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,767] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,767] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,767] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,767] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,768] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,768] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,768] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,769] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,769] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,769] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,769] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,770] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,770] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,770] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,770] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,771] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,771] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,771] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,771] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,772] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,772] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 14:18:23,774] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 15 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,774] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,775] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 47 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,775] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,776] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 48 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,777] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,777] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 14 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,778] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,778] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 9 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,778] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,778] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 41 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,779] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,779] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 42 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,779] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,779] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-15 in 4 milliseconds for epoch 14, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,780] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 22 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,780] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,780] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-47 in 4 milliseconds for epoch 12, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,781] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 20 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,781] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,781] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-48 in 4 milliseconds for epoch 14, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,781] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 31 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,782] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-14 in 4 milliseconds for epoch 12, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,782] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,782] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-9 in 4 milliseconds for epoch 12, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,783] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 28 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,783] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-41 in 4 milliseconds for epoch 14, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,783] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,783] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-42 in 3 milliseconds for epoch 12, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,784] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 25 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,784] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-22 in 3 milliseconds for epoch 14, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,784] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,784] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-20 in 3 milliseconds for epoch 14, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,785] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 8 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,785] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,785] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 38 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,786] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,785] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-31 in 2 milliseconds for epoch 12, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,786] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 35 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,786] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-28 in 2 milliseconds for epoch 14, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,786] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,787] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-25 in 2 milliseconds for epoch 14, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,787] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 4 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,787] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-8 in 2 milliseconds for epoch 12, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,788] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,788] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-38 in 2 milliseconds for epoch 12, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,788] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 1 in epoch 14 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,788] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-35 in 1 milliseconds for epoch 12, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,788] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,789] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds for epoch 14, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,789] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,790] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,789] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds for epoch 14, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,790] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,790] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,790] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,791] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,791] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,792] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,792] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,792] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,792] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,793] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,793] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,793] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,793] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,794] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,794] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,794] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,795] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,794] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,795] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,795] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,795] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,796] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,796] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,796] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,797] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,797] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,797] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,798] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,798] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,798] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,799] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,800] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,799] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,800] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,800] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,800] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,801] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,801] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,801] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,801] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,802] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,802] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,802] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,802] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,802] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,803] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,803] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,803] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,804] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:23:21,834] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:26:06,445] INFO [NodeToControllerChannelManager id=6 name=forwarding] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:45:31,884] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-21 14:45:31,890] INFO [BrokerServer id=6] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
[2025-05-21 14:45:31,891] INFO [BrokerServer id=6] shutting down (kafka.server.BrokerServer)
[2025-05-21 14:45:31,893] INFO [BrokerLifecycleManager id=6] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:45:31,926] INFO [BrokerLifecycleManager id=6] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:45:31,948] INFO [Broker id=6] Transitioning 71 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:45:31,950] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=4, leaderEpoch=9, isr=[5, 4], partitionEpoch=15, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 14:45:31,951] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:45:31,952] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:45:31,953] INFO [Broker id=6] Follower financial_transactions-17 starts at leader epoch 7 from offset 305279 with partition epoch 14 and high watermark 305279. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:31,954] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 13 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:45:31,956] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 13 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:45:31,959] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:31,960] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:31,961] INFO [Broker id=6] Follower financial_transactions-0 starts at leader epoch 8 from offset 306853 with partition epoch 14 and high watermark 306847. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:45:31,964] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:45:31,965] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:45:31,965] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:31,965] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:31,966] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=4, leaderEpoch=9, isr=[5, 4], partitionEpoch=15, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 14:45:31,966] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 13 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:45:31,966] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 15 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:45:31,967] INFO [Broker id=6] Follower financial_transactions-12 starts at leader epoch 8 from offset 305079 with partition epoch 14 and high watermark 305079. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:45:31,967] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:45:31,967] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:45:31,967] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:31,968] INFO [Broker id=6] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:31,969] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:31,969] INFO [Broker id=6] Follower financial_transactions-18 starts at leader epoch 7 from offset 304723 with partition epoch 14 and high watermark 304723. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:31,970] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:31,972] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 15 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:45:31,974] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:31,977] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 15 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:45:31,977] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:31,978] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:31,979] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:31,982] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=4, leaderEpoch=9, isr=[5, 4], partitionEpoch=15, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 14:45:31,982] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 15 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:45:31,983] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=4, leaderEpoch=8, isr=[5, 4], partitionEpoch=15, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 14:45:31,984] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 13 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:45:31,984] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:45:31,984] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=4, leaderEpoch=8, isr=[5, 4], partitionEpoch=15, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 14:45:31,985] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 15 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:45:31,985] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:31,985] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 15 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:45:31,986] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 15 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:45:31,986] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:45:31,986] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:45:31,987] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:45:31,987] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:45:31,988] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:31,988] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:31,988] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:31,989] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:45:31,989] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 15 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:45:31,990] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:45:31,991] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:45:31,992] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:31,995] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:31,998] INFO [Broker id=6] Follower financial_transactions-10 starts at leader epoch 7 from offset 305673 with partition epoch 14 and high watermark 305673. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:31,999] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,000] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 13 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:45:32,005] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=4, leaderEpoch=8, isr=[5, 4], partitionEpoch=15, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 14:45:32,006] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 13 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:45:32,007] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,009] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,010] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 15 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:45:32,011] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:32,011] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 13 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:45:32,011] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,012] INFO [Broker id=6] Follower financial_transactions-3 starts at leader epoch 7 from offset 305161 with partition epoch 14 and high watermark 305161. Current leader is 5. Previous leader Some(5) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,013] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:32,014] INFO [Broker id=6] Follower financial_transactions-7 starts at leader epoch 8 from offset 305027 with partition epoch 14 and high watermark 305027. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:45:32,015] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 10. (state.change.logger)
[2025-05-21 14:45:32,016] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 13 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 14:45:32,017] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 6. (state.change.logger)
[2025-05-21 14:45:32,019] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:32,038] INFO [BrokerLifecycleManager id=6] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:45:32,047] INFO [BrokerLifecycleManager id=6] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:32,052] INFO [BrokerLifecycleManager id=6] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:45:32,052] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, financial_transactions-18, financial_transactions-12, financial_transactions-11, __consumer_offsets-8, __consumer_offsets-21, financial_transactions-15, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, financial_transactions-4, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, financial_transactions-0, financial_transactions-6, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, financial_transactions-16, __consumer_offsets-15, financial_transactions-10, __consumer_offsets-24, financial_transactions-19, financial_transactions-7, __consumer_offsets-17, financial_transactions-17, financial_transactions-1, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, financial_transactions-14, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, financial_transactions-8, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, financial_transactions-2, financial_transactions-13, __consumer_offsets-32, financial_transactions-5, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:45:32,071] INFO [broker-6-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,074] INFO [ReplicaAlterLogDirsManager on broker 6] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, financial_transactions-18, financial_transactions-12, financial_transactions-11, __consumer_offsets-8, __consumer_offsets-21, financial_transactions-15, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, financial_transactions-4, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, financial_transactions-0, financial_transactions-6, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, financial_transactions-16, __consumer_offsets-15, financial_transactions-10, __consumer_offsets-24, financial_transactions-19, financial_transactions-7, __consumer_offsets-17, financial_transactions-17, financial_transactions-1, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, financial_transactions-14, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, financial_transactions-8, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, financial_transactions-2, financial_transactions-13, __consumer_offsets-32, financial_transactions-5, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-21 14:45:32,077] INFO [broker-6-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,077] INFO [broker-6-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,082] INFO [Broker id=6] Stopped fetchers as part of controlled shutdown for 71 partitions (state.change.logger)
[2025-05-21 14:45:32,098] INFO [ReplicaFetcherThread-0-4]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,103] INFO [ReplicaFetcherThread-0-4]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,107] INFO [SocketServer listenerType=BROKER, nodeId=6] Stopping socket server request processors (kafka.network.SocketServer)
[2025-05-21 14:45:32,103] INFO [ReplicaFetcherThread-0-4]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,114] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 14:45:32,176] INFO [ReplicaFetcherThread-0-5]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,178] INFO [ReplicaFetcherThread-0-5]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,178] INFO [ReplicaFetcherThread-0-5]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,180] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,181] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,186] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,186] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,187] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,188] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,188] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,189] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,189] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,189] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,190] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,191] INFO [SocketServer listenerType=BROKER, nodeId=6] Stopped socket server request processors (kafka.network.SocketServer)
[2025-05-21 14:45:32,191] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,191] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,194] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,194] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,195] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,195] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,199] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,200] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,200] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,201] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,203] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,202] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,204] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,205] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,205] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,206] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,206] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,207] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,208] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,208] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,208] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,209] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,209] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,210] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,210] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,211] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,210] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,213] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,214] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,215] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,216] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,217] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,220] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,223] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,223] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,224] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,225] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,226] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,226] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,227] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,227] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,227] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,228] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,228] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,229] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,230] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,231] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,230] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,231] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,232] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,232] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,237] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,237] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,239] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,241] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,241] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,243] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,245] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,245] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,245] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,249] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,249] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,250] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,251] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,251] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,251] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,252] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,253] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,255] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,252] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,255] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,256] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,261] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,261] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,262] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,264] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,264] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,266] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,267] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,268] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,268] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,269] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,277] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,278] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,279] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,280] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,280] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,281] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,279] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,282] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,284] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,286] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,287] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,282] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,288] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,289] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,289] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,288] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,289] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,298] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,300] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,290] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,300] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,301] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,301] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,301] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,302] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,302] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,301] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,303] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,305] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,307] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,307] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,306] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,313] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,315] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,316] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,316] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,317] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,317] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,319] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,321] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,321] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,322] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,323] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,322] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,323] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,324] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,324] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,325] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,325] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,324] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,327] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,327] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,328] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,328] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,328] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,329] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,331] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,332] INFO [Broker id=6] Transitioning 71 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:45:32,333] INFO [Broker id=6] Follower financial_transactions-13 starts at leader epoch 11 from offset 305468 with partition epoch 17 and high watermark 305468. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,334] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,334] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,335] INFO [Broker id=6] Follower financial_transactions-17 starts at leader epoch 8 from offset 305279 with partition epoch 16 and high watermark 305279. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:45:32,336] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,337] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,338] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,339] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,339] INFO [Broker id=6] Follower financial_transactions-0 starts at leader epoch 10 from offset 306853 with partition epoch 16 and high watermark 306847. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:45:32,340] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,341] INFO [Broker id=6] Follower financial_transactions-4 starts at leader epoch 7 from offset 304575 with partition epoch 16 and high watermark 304575. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,341] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,341] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,342] INFO [Broker id=6] Follower financial_transactions-8 starts at leader epoch 11 from offset 304749 with partition epoch 17 and high watermark 304736. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,342] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,343] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,343] INFO [Broker id=6] Follower financial_transactions-12 starts at leader epoch 10 from offset 305079 with partition epoch 16 and high watermark 305079. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:45:32,344] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,344] INFO [Broker id=6] Follower financial_transactions-14 starts at leader epoch 7 from offset 305191 with partition epoch 16 and high watermark 305191. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,345] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,348] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 19 from offset 8 with partition epoch 32 and high watermark 8. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,348] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,349] INFO [Broker id=6] Follower financial_transactions-18 starts at leader epoch 8 from offset 304723 with partition epoch 16 and high watermark 304723. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:45:32,349] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,350] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,350] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,351] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,351] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,352] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,354] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 19 from offset 8 with partition epoch 32 and high watermark 8. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,354] INFO [Broker id=6] Follower financial_transactions-1 starts at leader epoch 11 from offset 304782 with partition epoch 17 and high watermark 304774. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,356] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,357] INFO [Broker id=6] Follower financial_transactions-5 starts at leader epoch 10 from offset 304275 with partition epoch 17 and high watermark 304265. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:45:32,362] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,368] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,369] INFO [Broker id=6] Follower financial_transactions-9 starts at leader epoch 10 from offset 304857 with partition epoch 17 and high watermark 304839. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:45:32,370] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,373] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,374] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,374] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,376] INFO [Broker id=6] Follower financial_transactions-15 starts at leader epoch 7 from offset 305563 with partition epoch 16 and high watermark 305563. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,377] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,380] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,381] INFO [Broker id=6] Follower financial_transactions-19 starts at leader epoch 7 from offset 304471 with partition epoch 16 and high watermark 304471. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,383] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,383] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,384] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,384] INFO [Broker id=6] Follower financial_transactions-2 starts at leader epoch 7 from offset 305054 with partition epoch 16 and high watermark 305044. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,385] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,385] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,385] INFO [Broker id=6] Follower financial_transactions-6 starts at leader epoch 7 from offset 304608 with partition epoch 16 and high watermark 304608. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,389] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,391] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,394] INFO [Broker id=6] Follower financial_transactions-10 starts at leader epoch 8 from offset 305673 with partition epoch 16 and high watermark 305673. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:45:32,395] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,396] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,397] INFO [Broker id=6] Follower financial_transactions-16 starts at leader epoch 10 from offset 304604 with partition epoch 17 and high watermark 304604. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:45:32,398] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,399] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,400] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,400] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,403] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,404] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,405] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,405] INFO [Broker id=6] Follower financial_transactions-3 starts at leader epoch 8 from offset 305161 with partition epoch 16 and high watermark 305161. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:45:32,406] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,406] INFO [Broker id=6] Follower financial_transactions-7 starts at leader epoch 10 from offset 305027 with partition epoch 16 and high watermark 305027. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:45:32,407] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,408] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,411] INFO [Broker id=6] Follower financial_transactions-11 starts at leader epoch 7 from offset 305362 with partition epoch 16 and high watermark 305362. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,412] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,412] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, financial_transactions-18, financial_transactions-12, financial_transactions-11, __consumer_offsets-8, __consumer_offsets-21, financial_transactions-15, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, financial_transactions-4, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, financial_transactions-0, financial_transactions-6, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, financial_transactions-16, __consumer_offsets-15, financial_transactions-10, __consumer_offsets-24, financial_transactions-19, financial_transactions-7, __consumer_offsets-17, financial_transactions-17, financial_transactions-1, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, financial_transactions-14, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, financial_transactions-8, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, financial_transactions-2, financial_transactions-13, __consumer_offsets-32, financial_transactions-5, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:45:32,413] INFO [ReplicaAlterLogDirsManager on broker 6] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, financial_transactions-18, financial_transactions-12, financial_transactions-11, __consumer_offsets-8, __consumer_offsets-21, financial_transactions-15, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, financial_transactions-4, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, financial_transactions-0, financial_transactions-6, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, financial_transactions-16, __consumer_offsets-15, financial_transactions-10, __consumer_offsets-24, financial_transactions-19, financial_transactions-7, __consumer_offsets-17, financial_transactions-17, financial_transactions-1, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, financial_transactions-14, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, financial_transactions-8, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, financial_transactions-2, financial_transactions-13, __consumer_offsets-32, financial_transactions-5, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-21 14:45:32,414] INFO [Broker id=6] Stopped fetchers as part of controlled shutdown for 71 partitions (state.change.logger)
[2025-05-21 14:45:32,415] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,420] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,422] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,422] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,422] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,425] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,425] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,427] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,430] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,430] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,430] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,432] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,432] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,433] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,434] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,434] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,437] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,445] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,445] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,451] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,453] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,453] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,454] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,462] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,462] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,465] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,468] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,468] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,469] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,483] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,484] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,485] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,485] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,484] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,489] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,489] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,488] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,491] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,492] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,492] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,493] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,494] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,494] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,494] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,500] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,502] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,503] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,506] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,506] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,507] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,508] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,524] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,524] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,536] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,536] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,537] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,538] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,539] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,539] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,541] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,540] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,543] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,547] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,548] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,549] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,549] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,548] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,550] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,552] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,559] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,562] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,561] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,571] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,571] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,573] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,573] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,574] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,578] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,578] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,578] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,582] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,583] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,583] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,588] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,588] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,589] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,590] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,590] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,591] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,607] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,607] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,608] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,610] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,610] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,610] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,612] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,612] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,612] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,614] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,614] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,615] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,616] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,616] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,617] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,618] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,618] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,618] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,619] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,620] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,620] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,622] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,622] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,623] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,626] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,627] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,626] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,629] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,632] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,636] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,631] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,642] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,644] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,643] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,646] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,651] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,649] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,660] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,655] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,662] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,663] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,662] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,665] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,664] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,669] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,672] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,673] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,674] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,676] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,672] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,677] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,678] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,681] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,684] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,686] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,686] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,683] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,686] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,687] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,688] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,689] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,690] INFO [data-plane Kafka Request Handler on Broker 6], shutting down (kafka.server.KafkaRequestHandlerPool)
[2025-05-21 14:45:32,695] INFO [data-plane Kafka Request Handler on Broker 6], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2025-05-21 14:45:32,700] INFO [ExpirationReaper-6-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,706] INFO [ExpirationReaper-6-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,706] INFO [ExpirationReaper-6-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,710] INFO [KafkaApi-6] Shutdown complete. (kafka.server.KafkaApis)
[2025-05-21 14:45:32,723] INFO [TransactionCoordinator id=6] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 14:45:32,725] INFO [Transaction State Manager 6]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2025-05-21 14:45:32,725] INFO [TxnMarkerSenderThread-6]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 14:45:32,727] INFO [TxnMarkerSenderThread-6]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 14:45:32,727] INFO [TxnMarkerSenderThread-6]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 14:45:32,741] INFO [TransactionCoordinator id=6] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 14:45:32,749] INFO [GroupCoordinator 6]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,752] INFO [ExpirationReaper-6-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,760] INFO [ExpirationReaper-6-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,760] INFO [ExpirationReaper-6-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,763] INFO [ExpirationReaper-6-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,765] INFO [ExpirationReaper-6-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,765] INFO [ExpirationReaper-6-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,768] INFO [GroupCoordinator 6]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,771] INFO [AssignmentsManager id=6]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:32,782] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,785] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,785] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,790] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 14:45:32,791] INFO [AssignmentsManager id=6]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:32,792] INFO [ReplicaManager broker=6] Shutting down (kafka.server.ReplicaManager)
[2025-05-21 14:45:32,795] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 14:45:32,797] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 14:45:32,797] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 14:45:32,799] INFO [ReplicaFetcherManager on broker 6] shutting down (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:45:32,802] INFO [ReplicaFetcherManager on broker 6] shutdown completed (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:45:32,805] INFO [ReplicaAlterLogDirsManager on broker 6] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-21 14:45:32,806] INFO [ReplicaAlterLogDirsManager on broker 6] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-21 14:45:32,807] INFO [ExpirationReaper-6-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,808] INFO [ExpirationReaper-6-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,808] INFO [ExpirationReaper-6-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,811] INFO [ExpirationReaper-6-RemoteFetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,813] INFO [ExpirationReaper-6-RemoteFetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,813] INFO [ExpirationReaper-6-RemoteFetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,817] INFO [ExpirationReaper-6-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,819] INFO [ExpirationReaper-6-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,819] INFO [ExpirationReaper-6-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,830] INFO [ExpirationReaper-6-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,832] INFO [ExpirationReaper-6-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,833] INFO [ExpirationReaper-6-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,837] INFO [ExpirationReaper-6-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,844] INFO [ExpirationReaper-6-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,857] INFO [ExpirationReaper-6-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,886] INFO [AddPartitionsToTxnSenderThread-6]: Shutting down (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 14:45:32,887] INFO [AddPartitionsToTxnSenderThread-6]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 14:45:32,887] INFO [AddPartitionsToTxnSenderThread-6]: Stopped (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 14:45:32,890] INFO [ReplicaManager broker=6] Shut down completely (kafka.server.ReplicaManager)
[2025-05-21 14:45:32,892] INFO [broker-6-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,892] INFO [broker-6-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,892] INFO [broker-6-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,899] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 14:45:32,900] INFO [broker-6-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,901] INFO [broker-6-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,902] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 14:45:32,901] INFO [broker-6-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,907] INFO Shutting down. (kafka.log.LogManager)
[2025-05-21 14:45:32,909] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
[2025-05-21 14:45:32,909] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 14:45:32,910] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 14:45:32,911] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 14:45:32,949] INFO [ProducerStateManager partition=financial_transactions-15] Wrote producer snapshot at offset 305563 with 0 producer ids in 6 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,030] INFO [ProducerStateManager partition=financial_transactions-0] Wrote producer snapshot at offset 306853 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,094] INFO [ProducerStateManager partition=financial_transactions-3] Wrote producer snapshot at offset 305161 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,109] INFO [ProducerStateManager partition=financial_transactions-10] Wrote producer snapshot at offset 305673 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,119] INFO [ProducerStateManager partition=financial_transactions-12] Wrote producer snapshot at offset 305079 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,128] INFO [ProducerStateManager partition=financial_transactions-13] Wrote producer snapshot at offset 305468 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,157] INFO [ProducerStateManager partition=financial_transactions-19] Wrote producer snapshot at offset 304471 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,167] INFO [ProducerStateManager partition=financial_transactions-5] Wrote producer snapshot at offset 304275 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,179] INFO [ProducerStateManager partition=financial_transactions-6] Wrote producer snapshot at offset 304608 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,209] INFO [ProducerStateManager partition=financial_transactions-17] Wrote producer snapshot at offset 305279 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,221] INFO [ProducerStateManager partition=financial_transactions-7] Wrote producer snapshot at offset 305027 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,323] INFO [ProducerStateManager partition=__consumer_offsets-29] Wrote producer snapshot at offset 8 with 0 producer ids in 8 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,371] INFO [ProducerStateManager partition=financial_transactions-4] Wrote producer snapshot at offset 304575 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,399] INFO [ProducerStateManager partition=financial_transactions-18] Wrote producer snapshot at offset 304723 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,409] INFO [ProducerStateManager partition=financial_transactions-9] Wrote producer snapshot at offset 304857 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,416] INFO [ProducerStateManager partition=financial_transactions-1] Wrote producer snapshot at offset 304782 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,422] INFO [ProducerStateManager partition=financial_transactions-14] Wrote producer snapshot at offset 305191 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,436] INFO [ProducerStateManager partition=financial_transactions-11] Wrote producer snapshot at offset 305362 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,453] INFO [ProducerStateManager partition=_schemas-0] Wrote producer snapshot at offset 8 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,466] INFO [ProducerStateManager partition=financial_transactions-8] Wrote producer snapshot at offset 304749 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,476] INFO [ProducerStateManager partition=financial_transactions-2] Wrote producer snapshot at offset 305054 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,487] INFO [ProducerStateManager partition=financial_transactions-16] Wrote producer snapshot at offset 304604 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,540] INFO Shutdown complete. (kafka.log.LogManager)
[2025-05-21 14:45:33,542] INFO [broker-6-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,545] INFO [broker-6-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,545] INFO [broker-6-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,546] INFO [broker-6-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,546] INFO [broker-6-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,546] INFO [broker-6-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,547] INFO [broker-6-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,548] INFO [broker-6-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,548] INFO [broker-6-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,549] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,549] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,549] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,552] INFO [SocketServer listenerType=BROKER, nodeId=6] Shutting down socket server (kafka.network.SocketServer)
[2025-05-21 14:45:33,565] INFO [SocketServer listenerType=BROKER, nodeId=6] Shutdown completed (kafka.network.SocketServer)
[2025-05-21 14:45:33,567] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
[2025-05-21 14:45:33,567] INFO [BrokerLifecycleManager id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,569] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 14:45:33,570] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 14:45:33,570] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 14:45:33,571] INFO [SharedServer id=6] Stopping SharedServer (kafka.server.SharedServer)
[2025-05-21 14:45:33,572] INFO [MetadataLoader id=6] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,572] INFO [SnapshotGenerator id=6] close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,573] INFO [SnapshotGenerator id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,574] INFO [MetadataLoader id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,576] INFO [SnapshotGenerator id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,577] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 14:45:33,598] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 14:45:33,598] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 14:45:33,600] INFO [kafka-6-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 14:45:33,600] INFO [RaftManager id=6] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 14:45:33,601] INFO [RaftManager id=6] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 14:45:33,601] INFO [RaftManager id=6] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 14:45:33,602] INFO [kafka-6-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 14:45:33,602] INFO [kafka-6-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 14:45:33,608] INFO [kafka-6-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 14:45:33,609] INFO [kafka-6-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 14:45:33,609] INFO [kafka-6-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 14:45:33,614] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 16829 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,617] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-21 14:45:33,618] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2025-05-21 14:45:33,618] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-21 14:45:33,619] INFO App info kafka.server for 6 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 14:45:33,619] INFO [BrokerServer id=6] shut down completed (kafka.server.BrokerServer)
[2025-05-21 14:45:33,620] INFO [BrokerServer id=6] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
[2025-05-21 14:45:33,621] INFO App info kafka.server for 6 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-22 17:40:21,784] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-22 17:40:21,967] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-22 17:40:21,979] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-22 17:40:21,988] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-22 17:40:24,017] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-22 17:40:24,100] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-22 17:40:24,104] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-22 17:40:24,198] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-22 17:40:24,212] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-22 17:40:24,228] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-22 17:40:24,232] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-22 17:40:24,234] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2025-05-22 17:40:24,238] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-22 17:40:24,308] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
[2025-05-22 17:40:24,311] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:24,315] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:24,316] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000012473.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:24,316] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000016829.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:24,317] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:24,516] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 16829 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:24,535] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 16829 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:24,536] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 16829 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:24,542] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=16829, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000016829.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:24,552] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 15ms for snapshot load and 0ms for segment recovery from offset 16829 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:24,571] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-22 17:40:24,596] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-22 17:40:24,599] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-22 17:40:24,776] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-22 17:40:24,955] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=18, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from null (org.apache.kafka.raft.QuorumState)
[2025-05-22 17:40:24,963] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-22 17:40:24,982] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-22 17:40:25,053] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:25,104] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2025-05-22 17:40:25,110] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-22 17:40:25,157] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:25,176] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-22 17:40:25,192] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-22 17:40:25,203] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-22 17:40:25,212] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1707368806 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-22 17:40:25,216] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,230] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-22 17:40:25,238] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,270] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:25,289] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-22 17:40:25,299] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-22 17:40:25,375] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,378] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,382] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:25,386] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,399] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,423] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-22 17:40:25,401] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:25,424] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,458] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,495] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:25,497] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:25,497] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,506] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,517] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,522] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,589] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,593] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,602] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:25,707] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:25,790] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-22 17:40:25,819] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,828] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-22 17:40:25,820] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:25,835] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-22 17:40:25,845] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-22 17:40:25,832] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,882] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:25,884] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:25,915] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:25,918] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:25,941] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-22 17:40:25,941] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:25,943] INFO [RaftManager id=6] Completed transition to Unattached(epoch=20, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=18, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-22 17:40:25,953] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-22 17:40:25,954] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-22 17:40:25,953] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-22 17:40:25,955] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-22 17:40:25,968] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-22 17:40:25,970] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-22 17:40:26,052] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,078] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:26,078] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:26,121] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:26,123] INFO [BrokerLifecycleManager id=6] Incarnation mU_ayR23Qsit8bxiW3g1dQ of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-22 17:40:26,135] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-22 17:40:26,160] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,166] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-22 17:40:26,167] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,168] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-22 17:40:26,171] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-22 17:40:26,270] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,373] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,474] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,565] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=20, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=20, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-22 17:40:26,577] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,592] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=16834, metadata=Optional.empty)] for the first time for epoch 20 (org.apache.kafka.raft.FollowerState)
[2025-05-22 17:40:26,599] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 16834 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,629] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:26,799] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 16834 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,805] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 16833 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,805] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 16833 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,806] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 16833 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,807] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=16833, epoch=20) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-22 17:40:26,813] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:26,828] INFO Skipping recovery of 71 logs from /tmp/kafka-logs since clean shutdown file was found (kafka.log.LogManager)
[2025-05-22 17:40:26,847] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-13/00000000000000255889.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:26,847] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Loading producer state till offset 305468 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:26,848] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305468 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:26,849] INFO [ProducerStateManager partition=financial_transactions-13] Loading producer state from snapshot file 'SnapshotFile(offset=305468, file=/tmp/kafka-logs/financial_transactions-13/00000000000000305468.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:26,854] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Producer state recovery took 5ms for snapshot load and 0ms for segment recovery from offset 305468 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:26,870] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-13, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305468) with 1 segments, local-log-start-offset 0 and log-end-offset 305468 in 31ms (1/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:26,885] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:26,887] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:26,902] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-18/00000000000000255590.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:26,911] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Loading producer state till offset 304723 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:26,914] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 304723 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:26,922] INFO [ProducerStateManager partition=financial_transactions-18] Loading producer state from snapshot file 'SnapshotFile(offset=304723, file=/tmp/kafka-logs/financial_transactions-18/00000000000000304723.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:26,934] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Producer state recovery took 12ms for snapshot load and 0ms for segment recovery from offset 304723 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:26,952] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-18, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=304723) with 1 segments, local-log-start-offset 0 and log-end-offset 304723 in 78ms (2/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:26,959] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:26,969] INFO Deleted producer state snapshot /tmp/kafka-logs/__consumer_offsets-29/00000000000000000006.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:26,977] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 8 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:26,993] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 8 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:26,994] INFO [ProducerStateManager partition=__consumer_offsets-29] Loading producer state from snapshot file 'SnapshotFile(offset=8, file=/tmp/kafka-logs/__consumer_offsets-29/00000000000000000008.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:26,996] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 8 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:26,998] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-29, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=29, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=8) with 1 segments, local-log-start-offset 0 and log-end-offset 8 in 45ms (3/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,033] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-12/00000000000000255590.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,033] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Loading producer state till offset 305079 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,034] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305079 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,034] INFO [ProducerStateManager partition=financial_transactions-12] Loading producer state from snapshot file 'SnapshotFile(offset=305079, file=/tmp/kafka-logs/financial_transactions-12/00000000000000305079.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,036] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 305079 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,040] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-12, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305079) with 1 segments, local-log-start-offset 0 and log-end-offset 305079 in 34ms (4/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,066] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,077] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-38, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=38, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 37ms (5/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,114] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-16/00000000000000255412.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,120] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Loading producer state till offset 304604 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,129] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 304604 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,140] INFO [ProducerStateManager partition=financial_transactions-16] Loading producer state from snapshot file 'SnapshotFile(offset=304604, file=/tmp/kafka-logs/financial_transactions-16/00000000000000304604.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,116] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 16838 (kafka.server.BrokerLifecycleManager)
[2025-05-22 17:40:27,143] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 304604 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,156] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-16, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=304604) with 1 segments, local-log-start-offset 0 and log-end-offset 304604 in 78ms (6/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,161] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,164] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-27, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=27, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (7/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,180] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,194] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-39, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=39, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 30ms (8/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,216] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,234] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-42, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=42, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 36ms (9/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,248] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,251] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-9, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (10/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,270] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,274] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-22, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=22, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 22ms (11/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,282] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,287] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-26, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=26, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (12/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,293] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,295] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-40, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=40, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (13/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,299] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,313] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-47, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=47, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 17ms (14/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,327] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,332] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-15, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (15/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,337] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,340] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-10, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (16/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,350] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-11/00000000000000256033.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,350] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Loading producer state till offset 305362 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,351] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305362 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,351] INFO [ProducerStateManager partition=financial_transactions-11] Loading producer state from snapshot file 'SnapshotFile(offset=305362, file=/tmp/kafka-logs/financial_transactions-11/00000000000000305362.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,356] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Producer state recovery took 5ms for snapshot load and 0ms for segment recovery from offset 305362 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,358] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-11, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305362) with 1 segments, local-log-start-offset 0 and log-end-offset 305362 in 17ms (17/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,361] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,375] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-2, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 17ms (18/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,383] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,393] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-18, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 16ms (19/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,397] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,400] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-46, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=46, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (20/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,413] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-1/00000000000000255371.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,414] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Loading producer state till offset 304782 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,415] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 304782 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,415] INFO [ProducerStateManager partition=financial_transactions-1] Loading producer state from snapshot file 'SnapshotFile(offset=304782, file=/tmp/kafka-logs/financial_transactions-1/00000000000000304782.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,416] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 304782 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,419] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-1, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=304782) with 1 segments, local-log-start-offset 0 and log-end-offset 304782 in 18ms (21/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,423] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,430] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-12, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (22/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,434] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,441] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-23, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=23, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (23/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,461] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,463] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-14, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 21ms (24/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,469] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,472] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-5, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (25/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,476] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,478] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-41, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=41, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (26/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,487] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-17/00000000000000255866.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,489] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Loading producer state till offset 305279 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,491] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305279 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,492] INFO [ProducerStateManager partition=financial_transactions-17] Loading producer state from snapshot file 'SnapshotFile(offset=305279, file=/tmp/kafka-logs/financial_transactions-17/00000000000000305279.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,493] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 305279 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,496] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-17, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305279) with 1 segments, local-log-start-offset 0 and log-end-offset 305279 in 17ms (27/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,508] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,513] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-1, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 16ms (28/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,518] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,523] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-19, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (29/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,529] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,532] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-3, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (30/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,539] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,542] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-16, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (31/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,548] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,550] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-25, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=25, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (32/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,557] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-15/00000000000000256237.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,557] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Loading producer state till offset 305563 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,557] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305563 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,559] INFO [ProducerStateManager partition=financial_transactions-15] Loading producer state from snapshot file 'SnapshotFile(offset=305563, file=/tmp/kafka-logs/financial_transactions-15/00000000000000305563.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,561] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 305563 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,564] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-15, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305563) with 1 segments, local-log-start-offset 0 and log-end-offset 305563 in 13ms (33/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,572] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-3/00000000000000255715.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,572] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Loading producer state till offset 305161 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,573] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305161 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,573] INFO [ProducerStateManager partition=financial_transactions-3] Loading producer state from snapshot file 'SnapshotFile(offset=305161, file=/tmp/kafka-logs/financial_transactions-3/00000000000000305161.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,582] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Producer state recovery took 9ms for snapshot load and 0ms for segment recovery from offset 305161 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,584] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-3, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305161) with 1 segments, local-log-start-offset 0 and log-end-offset 305161 in 19ms (34/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,590] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-10/00000000000000256290.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,590] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Loading producer state till offset 305673 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,591] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305673 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,591] INFO [ProducerStateManager partition=financial_transactions-10] Loading producer state from snapshot file 'SnapshotFile(offset=305673, file=/tmp/kafka-logs/financial_transactions-10/00000000000000305673.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,592] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 305673 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,594] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-10, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305673) with 1 segments, local-log-start-offset 0 and log-end-offset 305673 in 9ms (35/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,600] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-19/00000000000000255167.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,601] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Loading producer state till offset 304471 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,601] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 304471 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,602] INFO [ProducerStateManager partition=financial_transactions-19] Loading producer state from snapshot file 'SnapshotFile(offset=304471, file=/tmp/kafka-logs/financial_transactions-19/00000000000000304471.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,603] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 304471 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,607] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-19, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=304471) with 1 segments, local-log-start-offset 0 and log-end-offset 304471 in 12ms (36/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,614] INFO Deleted producer state snapshot /tmp/kafka-logs/_schemas-0/00000000000000000006.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,614] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 8 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,615] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 8 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,615] INFO [ProducerStateManager partition=_schemas-0] Loading producer state from snapshot file 'SnapshotFile(offset=8, file=/tmp/kafka-logs/_schemas-0/00000000000000000008.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,616] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 8 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,619] INFO Completed load of Log(dir=/tmp/kafka-logs/_schemas-0, topicId=RrE8eovWRKu4kLR3MRJ0fA, topic=_schemas, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=8) with 1 segments, local-log-start-offset 0 and log-end-offset 8 in 11ms (37/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,624] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,626] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-28, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=28, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (38/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,631] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,634] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-13, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (39/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,639] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,642] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-24, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=24, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (40/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,646] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,654] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-32, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=32, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (41/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,662] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-14/00000000000000255812.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,663] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Loading producer state till offset 305191 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,664] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305191 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,665] INFO [ProducerStateManager partition=financial_transactions-14] Loading producer state from snapshot file 'SnapshotFile(offset=305191, file=/tmp/kafka-logs/financial_transactions-14/00000000000000305191.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,667] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 305191 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,671] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-14, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305191) with 1 segments, local-log-start-offset 0 and log-end-offset 305191 in 16ms (42/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,678] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-2/00000000000000255541.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,683] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Loading producer state till offset 305054 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,683] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305054 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,684] INFO [ProducerStateManager partition=financial_transactions-2] Loading producer state from snapshot file 'SnapshotFile(offset=305054, file=/tmp/kafka-logs/financial_transactions-2/00000000000000305054.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,684] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 305054 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,687] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-2, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305054) with 1 segments, local-log-start-offset 0 and log-end-offset 305054 in 14ms (43/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,694] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,696] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-31, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=31, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (44/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,701] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,703] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-21, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=21, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (45/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,711] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-0/00000000000000257211.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,712] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 306853 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,713] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 306853 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,714] INFO [ProducerStateManager partition=financial_transactions-0] Loading producer state from snapshot file 'SnapshotFile(offset=306853, file=/tmp/kafka-logs/financial_transactions-0/00000000000000306853.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,716] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 306853 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,720] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-0, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=306853) with 1 segments, local-log-start-offset 0 and log-end-offset 306853 in 17ms (46/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,726] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-9/00000000000000255435.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,726] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Loading producer state till offset 304857 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,727] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 304857 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,727] INFO [ProducerStateManager partition=financial_transactions-9] Loading producer state from snapshot file 'SnapshotFile(offset=304857, file=/tmp/kafka-logs/financial_transactions-9/00000000000000304857.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,728] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 304857 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,732] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-9, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=304857) with 1 segments, local-log-start-offset 0 and log-end-offset 304857 in 12ms (47/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,737] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,739] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-6, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (48/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,742] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,744] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-11, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (49/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,753] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-5/00000000000000254990.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,754] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Loading producer state till offset 304275 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,755] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 304275 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,755] INFO [ProducerStateManager partition=financial_transactions-5] Loading producer state from snapshot file 'SnapshotFile(offset=304275, file=/tmp/kafka-logs/financial_transactions-5/00000000000000304275.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,756] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 304275 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,758] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-5, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=304275) with 1 segments, local-log-start-offset 0 and log-end-offset 304275 in 13ms (50/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,761] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,763] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-30, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=30, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (51/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,770] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-6/00000000000000255175.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,771] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Loading producer state till offset 304608 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,772] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 304608 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,773] INFO [ProducerStateManager partition=financial_transactions-6] Loading producer state from snapshot file 'SnapshotFile(offset=304608, file=/tmp/kafka-logs/financial_transactions-6/00000000000000304608.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,775] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 304608 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,777] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-6, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=304608) with 1 segments, local-log-start-offset 0 and log-end-offset 304608 in 13ms (52/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,780] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,782] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-43, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=43, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (53/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,790] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,792] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-7, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (54/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,797] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,800] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-33, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=33, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (55/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,804] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,808] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-36, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=36, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (56/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,812] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,814] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-45, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=45, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (57/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,819] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,822] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-0, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (58/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,827] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,829] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-20, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=20, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (59/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,841] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,843] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-4, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (60/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,853] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,856] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-8, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (61/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,862] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,866] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-49, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=49, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (62/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,870] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,873] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-34, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=34, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (63/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,877] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,880] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-48, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=48, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (64/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,890] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-4/00000000000000255335.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,891] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Loading producer state till offset 304575 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,892] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 304575 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,893] INFO [ProducerStateManager partition=financial_transactions-4] Loading producer state from snapshot file 'SnapshotFile(offset=304575, file=/tmp/kafka-logs/financial_transactions-4/00000000000000304575.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,895] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 0ms for segment recovery from offset 304575 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,898] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-4, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=304575) with 1 segments, local-log-start-offset 0 and log-end-offset 304575 in 17ms (65/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,901] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,903] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-44, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=44, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (66/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,908] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,913] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-37, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=37, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (67/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,919] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-7/00000000000000255844.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,919] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Loading producer state till offset 305027 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,920] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305027 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,920] INFO [ProducerStateManager partition=financial_transactions-7] Loading producer state from snapshot file 'SnapshotFile(offset=305027, file=/tmp/kafka-logs/financial_transactions-7/00000000000000305027.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,921] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 305027 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,923] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-7, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305027) with 1 segments, local-log-start-offset 0 and log-end-offset 305027 in 10ms (68/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,929] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,931] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-35, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=35, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (69/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,935] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,938] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-17, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (70/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,943] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-8/00000000000000255525.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,944] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Loading producer state till offset 304749 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,944] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 304749 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,944] INFO [ProducerStateManager partition=financial_transactions-8] Loading producer state from snapshot file 'SnapshotFile(offset=304749, file=/tmp/kafka-logs/financial_transactions-8/00000000000000304749.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,945] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 304749 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,947] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-8, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=304749) with 1 segments, local-log-start-offset 0 and log-end-offset 304749 in 8ms (71/71 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,950] INFO Loaded 71 logs in 1134ms (kafka.log.LogManager)
[2025-05-22 17:40:27,951] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-22 17:40:27,953] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-22 17:40:27,962] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-22 17:40:28,093] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-22 17:40:28,096] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-22 17:40:28,097] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-22 17:40:28,098] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,105] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,106] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-22 17:40:28,109] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-22 17:40:28,110] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-22 17:40:28,114] INFO [Broker id=6] Transitioning 71 partition(s) to local followers. (state.change.logger)
[2025-05-22 17:40:28,120] INFO [Broker id=6] Creating new partition financial_transactions-13 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,152] INFO [Partition financial_transactions-13 broker=6] Log loaded for partition financial_transactions-13 with initial high watermark 305468 (kafka.cluster.Partition)
[2025-05-22 17:40:28,155] INFO [Broker id=6] Follower financial_transactions-13 starts at leader epoch 11 from offset 305468 with partition epoch 17 and high watermark 305468. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,156] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,157] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,158] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,158] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,159] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,160] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,160] INFO [Broker id=6] Creating new partition financial_transactions-17 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,161] INFO [Partition financial_transactions-17 broker=6] Log loaded for partition financial_transactions-17 with initial high watermark 305279 (kafka.cluster.Partition)
[2025-05-22 17:40:28,161] INFO [Broker id=6] Follower financial_transactions-17 starts at leader epoch 8 from offset 305279 with partition epoch 16 and high watermark 305279. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:28,162] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,163] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,163] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:28,164] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,165] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,166] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:28,167] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,168] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,169] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,169] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,171] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,171] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,171] INFO [Broker id=6] Creating new partition financial_transactions-0 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,173] INFO [Partition financial_transactions-0 broker=6] Log loaded for partition financial_transactions-0 with initial high watermark 306847 (kafka.cluster.Partition)
[2025-05-22 17:40:28,173] INFO [Broker id=6] Follower financial_transactions-0 starts at leader epoch 10 from offset 306853 with partition epoch 16 and high watermark 306847. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:40:28,173] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,175] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,175] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,175] INFO [Broker id=6] Creating new partition financial_transactions-4 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,176] INFO [Partition financial_transactions-4 broker=6] Log loaded for partition financial_transactions-4 with initial high watermark 304575 (kafka.cluster.Partition)
[2025-05-22 17:40:28,177] INFO [Broker id=6] Follower financial_transactions-4 starts at leader epoch 7 from offset 304575 with partition epoch 16 and high watermark 304575. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:28,177] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,178] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,178] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,179] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,179] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,180] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,180] INFO [Broker id=6] Creating new partition financial_transactions-8 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,181] INFO [Partition financial_transactions-8 broker=6] Log loaded for partition financial_transactions-8 with initial high watermark 304736 (kafka.cluster.Partition)
[2025-05-22 17:40:28,181] INFO [Broker id=6] Follower financial_transactions-8 starts at leader epoch 11 from offset 304749 with partition epoch 17 and high watermark 304736. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,181] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,183] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,183] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:28,184] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,185] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,185] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:28,186] INFO [Broker id=6] Creating new partition financial_transactions-12 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,187] INFO [Partition financial_transactions-12 broker=6] Log loaded for partition financial_transactions-12 with initial high watermark 305079 (kafka.cluster.Partition)
[2025-05-22 17:40:28,187] INFO [Broker id=6] Follower financial_transactions-12 starts at leader epoch 10 from offset 305079 with partition epoch 16 and high watermark 305079. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:40:28,188] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,188] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,189] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,189] INFO [Broker id=6] Creating new partition financial_transactions-14 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,190] INFO [Partition financial_transactions-14 broker=6] Log loaded for partition financial_transactions-14 with initial high watermark 305191 (kafka.cluster.Partition)
[2025-05-22 17:40:28,191] INFO [Broker id=6] Follower financial_transactions-14 starts at leader epoch 7 from offset 305191 with partition epoch 16 and high watermark 305191. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:28,191] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,192] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,192] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,193] INFO [Broker id=6] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-22 17:40:28,194] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 8 (kafka.cluster.Partition)
[2025-05-22 17:40:28,194] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 19 from offset 8 with partition epoch 32 and high watermark 8. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,195] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,196] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,197] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,197] INFO [Broker id=6] Creating new partition financial_transactions-18 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,198] INFO [Partition financial_transactions-18 broker=6] Log loaded for partition financial_transactions-18 with initial high watermark 304723 (kafka.cluster.Partition)
[2025-05-22 17:40:28,199] INFO [Broker id=6] Follower financial_transactions-18 starts at leader epoch 8 from offset 304723 with partition epoch 16 and high watermark 304723. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:28,199] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,200] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,201] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,201] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,202] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,202] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:28,203] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,204] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,204] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,205] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,206] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,206] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:28,206] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,211] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,212] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,212] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,213] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,214] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,215] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,216] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 8 (kafka.cluster.Partition)
[2025-05-22 17:40:28,216] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 19 from offset 8 with partition epoch 32 and high watermark 8. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,217] INFO [Broker id=6] Creating new partition financial_transactions-1 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,217] INFO [Partition financial_transactions-1 broker=6] Log loaded for partition financial_transactions-1 with initial high watermark 304774 (kafka.cluster.Partition)
[2025-05-22 17:40:28,218] INFO [Broker id=6] Follower financial_transactions-1 starts at leader epoch 11 from offset 304782 with partition epoch 17 and high watermark 304774. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,218] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,219] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,219] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:28,219] INFO [Broker id=6] Creating new partition financial_transactions-5 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,220] INFO [Partition financial_transactions-5 broker=6] Log loaded for partition financial_transactions-5 with initial high watermark 304265 (kafka.cluster.Partition)
[2025-05-22 17:40:28,220] INFO [Broker id=6] Follower financial_transactions-5 starts at leader epoch 10 from offset 304275 with partition epoch 17 and high watermark 304265. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:40:28,221] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,222] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,222] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:28,223] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,224] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,224] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,224] INFO [Broker id=6] Creating new partition financial_transactions-9 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,225] INFO [Partition financial_transactions-9 broker=6] Log loaded for partition financial_transactions-9 with initial high watermark 304839 (kafka.cluster.Partition)
[2025-05-22 17:40:28,225] INFO [Broker id=6] Follower financial_transactions-9 starts at leader epoch 10 from offset 304857 with partition epoch 17 and high watermark 304839. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:40:28,226] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,226] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,227] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:28,227] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,227] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,228] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,228] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,229] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,229] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:28,230] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,230] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,231] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:28,231] INFO [Broker id=6] Creating new partition financial_transactions-15 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,233] INFO [Partition financial_transactions-15 broker=6] Log loaded for partition financial_transactions-15 with initial high watermark 305563 (kafka.cluster.Partition)
[2025-05-22 17:40:28,233] INFO [Broker id=6] Follower financial_transactions-15 starts at leader epoch 7 from offset 305563 with partition epoch 16 and high watermark 305563. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:28,233] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,234] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,234] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,235] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,236] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,236] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,237] INFO [Broker id=6] Creating new partition financial_transactions-19 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,238] INFO [Partition financial_transactions-19 broker=6] Log loaded for partition financial_transactions-19 with initial high watermark 304471 (kafka.cluster.Partition)
[2025-05-22 17:40:28,238] INFO [Broker id=6] Follower financial_transactions-19 starts at leader epoch 7 from offset 304471 with partition epoch 16 and high watermark 304471. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:28,238] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,239] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,239] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,240] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,241] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,241] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,241] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,242] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,243] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,243] INFO [Broker id=6] Creating new partition financial_transactions-2 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,244] INFO [Partition financial_transactions-2 broker=6] Log loaded for partition financial_transactions-2 with initial high watermark 305044 (kafka.cluster.Partition)
[2025-05-22 17:40:28,244] INFO [Broker id=6] Follower financial_transactions-2 starts at leader epoch 7 from offset 305054 with partition epoch 16 and high watermark 305044. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:28,245] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,246] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,246] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:28,247] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,248] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,248] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,249] INFO [Broker id=6] Creating new partition financial_transactions-6 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,249] INFO [Partition financial_transactions-6 broker=6] Log loaded for partition financial_transactions-6 with initial high watermark 304608 (kafka.cluster.Partition)
[2025-05-22 17:40:28,250] INFO [Broker id=6] Follower financial_transactions-6 starts at leader epoch 7 from offset 304608 with partition epoch 16 and high watermark 304608. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:28,250] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,251] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,251] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,251] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,252] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,252] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,253] INFO [Broker id=6] Creating new partition financial_transactions-10 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,254] INFO [Partition financial_transactions-10 broker=6] Log loaded for partition financial_transactions-10 with initial high watermark 305673 (kafka.cluster.Partition)
[2025-05-22 17:40:28,254] INFO [Broker id=6] Follower financial_transactions-10 starts at leader epoch 8 from offset 305673 with partition epoch 16 and high watermark 305673. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:28,254] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,255] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,255] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,256] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,257] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,257] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:28,257] INFO [Broker id=6] Creating new partition financial_transactions-16 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,258] INFO [Partition financial_transactions-16 broker=6] Log loaded for partition financial_transactions-16 with initial high watermark 304604 (kafka.cluster.Partition)
[2025-05-22 17:40:28,259] INFO [Broker id=6] Follower financial_transactions-16 starts at leader epoch 10 from offset 304604 with partition epoch 17 and high watermark 304604. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:40:28,259] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,260] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,261] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:28,261] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,262] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,262] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,263] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,264] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,264] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,265] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,266] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,267] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:28,267] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,268] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,268] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,269] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,270] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,271] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:28,271] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,273] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,274] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,274] INFO [Broker id=6] Creating new partition financial_transactions-3 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,275] INFO [Partition financial_transactions-3 broker=6] Log loaded for partition financial_transactions-3 with initial high watermark 305161 (kafka.cluster.Partition)
[2025-05-22 17:40:28,276] INFO [Broker id=6] Follower financial_transactions-3 starts at leader epoch 8 from offset 305161 with partition epoch 16 and high watermark 305161. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:28,276] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,277] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,277] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,278] INFO [Broker id=6] Creating new partition financial_transactions-7 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,279] INFO [Partition financial_transactions-7 broker=6] Log loaded for partition financial_transactions-7 with initial high watermark 305027 (kafka.cluster.Partition)
[2025-05-22 17:40:28,279] INFO [Broker id=6] Follower financial_transactions-7 starts at leader epoch 10 from offset 305027 with partition epoch 16 and high watermark 305027. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:40:28,280] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,281] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,282] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,283] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,286] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,286] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:28,289] INFO [Broker id=6] Creating new partition financial_transactions-11 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,291] INFO [Partition financial_transactions-11 broker=6] Log loaded for partition financial_transactions-11 with initial high watermark 305362 (kafka.cluster.Partition)
[2025-05-22 17:40:28,291] INFO [Broker id=6] Follower financial_transactions-11 starts at leader epoch 7 from offset 305362 with partition epoch 16 and high watermark 305362. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:28,292] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,293] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,294] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,295] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-6, __consumer_offsets-35, financial_transactions-11, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-22 17:40:28,296] INFO [Broker id=6] Stopped fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-22 17:40:28,301] INFO [Broker id=6] Started fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-22 17:40:28,317] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,318] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,318] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,319] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,319] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,319] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,319] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,320] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,320] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,320] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,320] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,321] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,321] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,321] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,322] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,322] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,322] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,322] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,323] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,323] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,323] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,323] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,324] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,324] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,324] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,325] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,325] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,325] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,326] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,326] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,323] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,326] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,327] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,327] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,327] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,328] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,328] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,328] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,328] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,328] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,329] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,329] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,329] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,330] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,330] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,330] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,330] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,331] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,331] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,331] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,331] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,331] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,332] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,332] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,332] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,333] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,333] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,333] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,334] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,335] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,335] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,335] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,335] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,335] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,336] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,336] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,336] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,337] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,337] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,338] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,338] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,338] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,338] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,339] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,339] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,339] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,340] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,340] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,340] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,340] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,341] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,341] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,341] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,341] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,342] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,342] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,342] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,343] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,343] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,343] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,343] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,343] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,344] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,344] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,344] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,345] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,346] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,346] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,346] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,346] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,347] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,347] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,348] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,348] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,348] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,349] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,350] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,350] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,351] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,351] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,351] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,351] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,352] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,347] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,352] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,352] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,352] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,353] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,353] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,353] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,353] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,354] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,354] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,354] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,355] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,355] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,355] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,355] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,356] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,356] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,356] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,356] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,357] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,357] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,357] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,358] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,358] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,359] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,359] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,359] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,360] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,360] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,360] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,361] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,360] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,361] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,361] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,361] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,362] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,362] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,366] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-22 17:40:28,372] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-22 17:40:28,377] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 16833 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:28,385] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-22 17:40:28,386] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-22 17:40:28,386] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-22 17:40:28,386] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-22 17:40:28,388] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-22 17:40:28,390] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-22 17:40:28,394] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-22 17:40:28,476] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-22 17:40:28,477] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-22 17:40:28,479] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-22 17:40:28,480] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-22 17:40:28,480] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2025-05-22 17:40:28,483] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-22 17:40:28,485] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-22 17:40:28,492] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-22 17:40:28,493] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-22 17:40:28,494] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-22 17:40:28,495] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-22 17:40:28,495] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-22 17:40:28,496] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-22 17:40:28,496] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-22 17:40:28,496] INFO Kafka startTimeMs: 1747935628496 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-22 17:40:28,503] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-22 17:40:28,966] INFO [Broker id=6] Transitioning 71 partition(s) to local followers. (state.change.logger)
[2025-05-22 17:40:28,968] INFO [Broker id=6] Follower financial_transactions-13 starts at leader epoch 12 from offset 305468 with partition epoch 18 and high watermark 305468. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:28,969] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 19 from offset 0 with partition epoch 33 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,970] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 19 from offset 0 with partition epoch 33 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,971] INFO [Broker id=6] Follower financial_transactions-17 starts at leader epoch 9 from offset 305279 with partition epoch 17 and high watermark 305279. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-22 17:40:28,972] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 15 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-22 17:40:28,972] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 15 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-22 17:40:28,975] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 12 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:28,976] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 12 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:28,977] INFO [Broker id=6] Follower financial_transactions-0 starts at leader epoch 11 from offset 306853 with partition epoch 17 and high watermark 306847. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,977] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 19 from offset 0 with partition epoch 33 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,978] INFO [Broker id=6] Follower financial_transactions-4 starts at leader epoch 8 from offset 304575 with partition epoch 17 and high watermark 304575. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:28,979] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 20 from offset 0 with partition epoch 33 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:40:28,979] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 20 from offset 0 with partition epoch 33 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:40:28,981] INFO [Broker id=6] Follower financial_transactions-8 starts at leader epoch 12 from offset 304749 with partition epoch 18 and high watermark 304736. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:28,983] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 15 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-22 17:40:28,984] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 18 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,985] INFO [Broker id=6] Follower financial_transactions-12 starts at leader epoch 11 from offset 305079 with partition epoch 17 and high watermark 305079. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,985] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 19 from offset 0 with partition epoch 33 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,986] INFO [Broker id=6] Follower financial_transactions-14 starts at leader epoch 8 from offset 305191 with partition epoch 17 and high watermark 305191. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:28,987] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 20 from offset 0 with partition epoch 33 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:40:28,988] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 20 from offset 8 with partition epoch 33 and high watermark 8. Current leader is 5. Previous leader Some(5) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:40:28,988] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 12 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:28,988] INFO [Broker id=6] Follower financial_transactions-18 starts at leader epoch 9 from offset 304723 with partition epoch 17 and high watermark 304723. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-22 17:40:28,989] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 12 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:28,990] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 18 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,992] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 12 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:28,994] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 18 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,996] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 20 from offset 0 with partition epoch 33 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:40:28,996] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 12 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:28,998] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 20 from offset 8 with partition epoch 33 and high watermark 8. Current leader is 5. Previous leader Some(5) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:40:28,999] INFO [Broker id=6] Follower financial_transactions-1 starts at leader epoch 12 from offset 304782 with partition epoch 18 and high watermark 304774. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:28,999] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 18 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:29,000] INFO [Broker id=6] Follower financial_transactions-5 starts at leader epoch 11 from offset 304275 with partition epoch 18 and high watermark 304265. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,001] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 15 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-22 17:40:29,002] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 19 from offset 0 with partition epoch 33 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:29,002] INFO [Broker id=6] Follower financial_transactions-9 starts at leader epoch 11 from offset 304857 with partition epoch 18 and high watermark 304839. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,003] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 18 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:29,003] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 12 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:29,004] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 18 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:29,004] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 18 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:29,006] INFO [Broker id=6] Follower financial_transactions-15 starts at leader epoch 8 from offset 305563 with partition epoch 17 and high watermark 305563. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:29,006] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 19 from offset 0 with partition epoch 33 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:29,007] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 19 from offset 0 with partition epoch 33 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:29,007] INFO [Broker id=6] Follower financial_transactions-19 starts at leader epoch 8 from offset 304471 with partition epoch 17 and high watermark 304471. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:29,008] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 20 from offset 0 with partition epoch 33 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:40:29,008] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 12 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:29,008] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 12 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:29,009] INFO [Broker id=6] Follower financial_transactions-2 starts at leader epoch 8 from offset 305054 with partition epoch 17 and high watermark 305044. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:29,010] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 18 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:29,010] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 19 from offset 0 with partition epoch 33 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:29,010] INFO [Broker id=6] Follower financial_transactions-6 starts at leader epoch 8 from offset 304608 with partition epoch 17 and high watermark 304608. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:29,011] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 12 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:29,011] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 12 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:29,011] INFO [Broker id=6] Follower financial_transactions-10 starts at leader epoch 9 from offset 305673 with partition epoch 17 and high watermark 305673. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-22 17:40:29,012] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 12 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:29,012] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 15 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-22 17:40:29,013] INFO [Broker id=6] Follower financial_transactions-16 starts at leader epoch 11 from offset 304604 with partition epoch 18 and high watermark 304604. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,015] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 15 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-22 17:40:29,027] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 12 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:29,027] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 12 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:29,028] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 18 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:29,028] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 20 from offset 0 with partition epoch 33 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:40:29,030] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 15 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-22 17:40:29,030] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 12 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:29,031] INFO [Broker id=6] Follower financial_transactions-3 starts at leader epoch 9 from offset 305161 with partition epoch 17 and high watermark 305161. Current leader is 5. Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-22 17:40:29,031] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 20 from offset 0 with partition epoch 33 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:40:29,031] INFO [Broker id=6] Follower financial_transactions-7 starts at leader epoch 11 from offset 305027 with partition epoch 17 and high watermark 305027. Current leader is 5. Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,032] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 12 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:40:29,032] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 15 from offset 0 with partition epoch 31 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-22 17:40:29,032] INFO [Broker id=6] Follower financial_transactions-11 starts at leader epoch 8 from offset 305362 with partition epoch 17 and high watermark 305362. Current leader is 5. Previous leader Some(5) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:29,033] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 20 from offset 0 with partition epoch 33 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:40:29,033] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-6, __consumer_offsets-35, financial_transactions-11, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-22 17:40:29,034] INFO [Broker id=6] Stopped fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-22 17:40:29,060] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,066] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(financial_transactions-13 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,305468), __consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),19,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),19,0), financial_transactions-17 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),9,305279), __consumer_offsets-9 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), __consumer_offsets-42 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), __consumer_offsets-21 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,0), __consumer_offsets-17 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,0), financial_transactions-0 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,306853), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),19,0), financial_transactions-4 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,304575), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),20,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),20,0), financial_transactions-8 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,304749), __consumer_offsets-38 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), __consumer_offsets-1 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),18,0), financial_transactions-12 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,305079), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),19,0), financial_transactions-14 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,305191), __consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),20,0), _schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),20,8), __consumer_offsets-45 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,0), financial_transactions-18 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),9,304723), __consumer_offsets-12 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,0), __consumer_offsets-41 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),18,0), __consumer_offsets-24 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,0), __consumer_offsets-20 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),18,0), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),20,0), __consumer_offsets-0 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),20,8), financial_transactions-1 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,304782), __consumer_offsets-25 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),18,0), financial_transactions-5 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,304275), __consumer_offsets-8 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),19,0), financial_transactions-9 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,304857), __consumer_offsets-4 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),18,0), __consumer_offsets-33 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,0), __consumer_offsets-15 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),18,0), __consumer_offsets-48 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),18,0), financial_transactions-15 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,305563), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),19,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),19,0), financial_transactions-19 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,304471), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),20,0), __consumer_offsets-19 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,0), __consumer_offsets-32 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,0), financial_transactions-2 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,305054), __consumer_offsets-28 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),18,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),19,0), financial_transactions-6 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,304608), __consumer_offsets-40 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,0), __consumer_offsets-3 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,0), financial_transactions-10 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),9,305673), __consumer_offsets-36 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,0), __consumer_offsets-47 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), financial_transactions-16 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,304604), __consumer_offsets-14 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), __consumer_offsets-43 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,0), __consumer_offsets-10 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,0), __consumer_offsets-22 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),18,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),20,0), __consumer_offsets-31 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), __consumer_offsets-27 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,0), financial_transactions-3 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),9,305161), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),20,0), financial_transactions-7 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),11,305027), __consumer_offsets-6 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),12,0), __consumer_offsets-35 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),15,0), financial_transactions-11 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=5, host=kafka-broker-2:19092),8,305362), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-2:19092),20,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-22 17:40:29,068] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,071] INFO [Broker id=6] Started fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-22 17:40:29,073] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,075] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,076] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,076] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,077] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,077] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,077] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,078] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,078] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,078] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,079] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,079] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,079] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,080] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,080] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,080] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,081] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,081] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,081] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,082] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,082] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,083] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,083] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,084] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,084] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,085] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,085] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,085] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,086] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,086] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,087] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,087] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,087] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,088] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,088] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,089] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,089] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,089] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,090] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,090] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,091] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,091] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,092] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,092] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,092] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,093] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,093] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,094] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,094] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,094] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,095] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,095] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,096] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,096] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,097] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,097] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,098] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,098] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,099] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,099] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,100] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,100] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,101] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,101] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,103] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,104] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,104] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,105] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,106] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,106] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,107] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,108] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,108] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,109] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,109] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,109] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,110] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,110] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,110] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,111] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,111] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,111] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,112] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,113] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,113] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,113] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,114] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,114] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,114] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,115] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,115] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,115] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,116] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,116] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,117] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,117] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,117] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,126] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,127] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,128] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,133] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,128] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,143] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,139] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,146] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,147] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,147] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,147] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,151] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,155] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,156] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,159] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,160] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,160] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,161] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,161] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,161] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,161] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,161] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,162] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,162] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,164] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,164] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,168] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,169] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,172] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,173] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,175] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,176] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,174] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,177] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,182] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,189] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,189] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,194] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,201] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,201] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,204] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,206] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,206] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,209] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,210] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,211] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,212] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,214] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,215] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,215] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,215] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,215] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,216] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,217] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,217] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,217] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,217] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,217] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,219] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,221] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,222] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,223] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,224] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,225] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,225] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,224] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,226] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,226] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,227] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,228] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,227] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,228] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,229] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,230] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,231] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,231] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,233] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,234] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,231] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,235] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,236] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,235] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,237] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,238] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,238] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,238] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,239] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,238] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,239] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,240] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,240] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,241] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,241] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,241] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,242] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,242] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,242] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,242] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,244] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,243] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,244] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,244] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,245] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,245] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,245] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,246] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,246] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,247] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,249] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,249] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,249] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,250] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,250] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,251] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,248] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,253] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,252] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,254] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,259] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,255] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,260] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,260] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,261] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,260] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,262] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,263] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,261] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,265] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,265] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,266] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,270] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,271] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,272] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,272] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,273] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,273] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,274] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,274] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,266] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,280] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,275] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,289] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,290] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,290] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,287] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,292] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,294] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,302] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,297] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition financial_transactions-0 with TruncationState(offset=306847, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=0, leaderEpoch=7, endOffset=306847) (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:40:29,304] INFO [UnifiedLog partition=financial_transactions-0, dir=/tmp/kafka-logs] Truncating to offset 306847 (kafka.log.UnifiedLog)
[2025-05-22 17:40:29,304] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,308] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,309] INFO [UnifiedLog partition=financial_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 306847 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:29,309] INFO [UnifiedLog partition=financial_transactions-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 306847 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:29,310] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-0/00000000000000306853.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:29,315] INFO [ProducerStateManager partition=financial_transactions-0] Wrote producer snapshot at offset 306847 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:29,316] INFO [UnifiedLog partition=financial_transactions-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 5ms for segment recovery from offset 306847 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:29,344] INFO [Broker id=6] Transitioning 2 partition(s) to local followers. (state.change.logger)
[2025-05-22 17:40:29,345] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=19, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:29,346] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=18, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-22 17:40:29,392] INFO [Broker id=6] Transitioning 18 partition(s) to local followers. (state.change.logger)
[2025-05-22 17:40:29,393] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=18, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-22 17:40:29,393] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:29,394] INFO [Broker id=6] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:29,394] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 6], partitionEpoch=34, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 19. (state.change.logger)
[2025-05-22 17:40:29,395] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:29,396] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 6], partitionEpoch=34, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 19. (state.change.logger)
[2025-05-22 17:40:29,396] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=32, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-22 17:40:29,397] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 4], partitionEpoch=32, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-22 17:40:29,397] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:29,399] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:29,400] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4], partitionEpoch=34, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 19. (state.change.logger)
[2025-05-22 17:40:29,401] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=18, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-22 17:40:29,403] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:29,404] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:29,405] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=19, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:29,406] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 4], partitionEpoch=32, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-22 17:40:29,407] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 4], partitionEpoch=32, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 18. (state.change.logger)
[2025-05-22 17:40:29,407] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4], partitionEpoch=34, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 19. (state.change.logger)
[2025-05-22 17:40:29,409] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,409] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,409] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,409] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,410] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,410] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,409] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,411] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,411] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,411] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,412] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,413] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,413] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,413] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,414] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,414] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,415] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,416] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,416] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,416] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,417] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,417] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,418] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,419] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,421] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,419] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,422] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,422] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,422] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,423] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,424] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,424] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,423] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,425] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,424] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,426] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,426] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,426] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,427] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,427] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,427] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,428] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,464] INFO [Broker id=6] Transitioning 4 partition(s) to local followers. (state.change.logger)
[2025-05-22 17:40:29,465] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:29,466] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=18, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-22 17:40:29,466] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=9, isr=[5, 6], partitionEpoch=18, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-22 17:40:29,467] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-22 17:40:29,919] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-22 17:40:29,919] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4, 6], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-22 17:40:29,965] INFO [Broker id=6] Transitioning 19 partition(s) to local followers. (state.change.logger)
[2025-05-22 17:40:29,967] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4, 6], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-22 17:40:29,967] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:29,968] INFO [Broker id=6] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:29,969] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:29,969] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 6, 4], partitionEpoch=35, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 19. (state.change.logger)
[2025-05-22 17:40:29,970] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 6, 4], partitionEpoch=35, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 19. (state.change.logger)
[2025-05-22 17:40:29,971] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=9, isr=[5, 6, 4], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-22 17:40:29,972] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=33, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-22 17:40:29,972] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 4, 6], partitionEpoch=33, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-22 17:40:29,975] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:29,976] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:29,977] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-22 17:40:29,977] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 19. (state.change.logger)
[2025-05-22 17:40:29,978] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:29,979] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:29,979] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:29,980] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 4, 6], partitionEpoch=33, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-22 17:40:29,980] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 4, 6], partitionEpoch=33, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 18. (state.change.logger)
[2025-05-22 17:40:29,981] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 19. (state.change.logger)
[2025-05-22 17:40:29,981] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,982] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,982] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,982] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,982] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,983] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,983] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,983] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,984] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,984] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,984] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,984] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,984] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,985] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,985] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,985] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,985] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,986] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,987] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,986] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,987] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,987] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,987] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,988] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,988] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,991] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,991] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,992] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,991] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,993] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,993] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,993] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,994] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,995] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,994] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,995] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,996] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,996] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,997] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,996] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,997] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,998] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,446] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-22 17:40:30,447] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=9, isr=[5, 6], partitionEpoch=18, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-22 17:40:30,933] INFO [Broker id=6] Transitioning 48 partition(s) to local followers. (state.change.logger)
[2025-05-22 17:40:30,933] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6], partitionEpoch=32, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 18. (state.change.logger)
[2025-05-22 17:40:30,934] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 4], partitionEpoch=32, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 18. (state.change.logger)
[2025-05-22 17:40:30,934] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=18, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-22 17:40:30,935] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4], partitionEpoch=34, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 19. (state.change.logger)
[2025-05-22 17:40:30,936] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4], partitionEpoch=34, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 19. (state.change.logger)
[2025-05-22 17:40:30,936] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=18, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-22 17:40:30,937] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:30,937] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:30,938] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6], partitionEpoch=32, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:30,940] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=18, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-22 17:40:30,942] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 4], partitionEpoch=32, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 18. (state.change.logger)
[2025-05-22 17:40:30,943] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 6], partitionEpoch=34, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 19. (state.change.logger)
[2025-05-22 17:40:30,944] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=18, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-22 17:40:30,944] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:30,945] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:30,946] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=9, isr=[5, 4], partitionEpoch=18, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-22 17:40:30,946] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:30,947] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=32, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-22 17:40:30,948] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 4], partitionEpoch=19, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-22 17:40:30,949] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=32, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-22 17:40:30,949] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6], partitionEpoch=32, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:30,950] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6], partitionEpoch=32, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:30,950] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6], partitionEpoch=32, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 18. (state.change.logger)
[2025-05-22 17:40:30,950] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:30,951] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6], partitionEpoch=32, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:30,951] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6], partitionEpoch=32, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 18. (state.change.logger)
[2025-05-22 17:40:30,951] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6], partitionEpoch=32, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 18. (state.change.logger)
[2025-05-22 17:40:30,952] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:30,952] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:30,953] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=32, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-22 17:40:30,954] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6], partitionEpoch=32, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:30,955] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:30,956] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=19, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:30,956] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6], partitionEpoch=32, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:30,957] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=9, isr=[5, 6], partitionEpoch=18, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-22 17:40:30,958] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 4], partitionEpoch=32, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 18. (state.change.logger)
[2025-05-22 17:40:30,958] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 4], partitionEpoch=19, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-22 17:40:30,959] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:30,959] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 4], partitionEpoch=32, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-22 17:40:30,960] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 4], partitionEpoch=18, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-22 17:40:30,960] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4], partitionEpoch=34, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 19. (state.change.logger)
[2025-05-22 17:40:30,960] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:30,961] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 4], partitionEpoch=19, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-22 17:40:30,961] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 4], partitionEpoch=32, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-22 17:40:30,961] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6], partitionEpoch=32, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 18. (state.change.logger)
[2025-05-22 17:40:30,962] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=18, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-22 17:40:30,962] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:30,962] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:30,963] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,963] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,964] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,964] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,964] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,965] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,965] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,965] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,965] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,965] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,966] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,966] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,966] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,968] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,969] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,969] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,971] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,975] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,976] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,976] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,977] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,977] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,978] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,978] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,978] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,979] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,979] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,979] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,982] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,975] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,987] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,989] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,990] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,990] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,989] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,990] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,991] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,992] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,993] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,992] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,993] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,994] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,994] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,994] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,994] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,995] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,995] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,995] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,996] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,995] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,996] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,996] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,997] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,997] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,998] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,997] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,998] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,998] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,999] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:30,999] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:30,999] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,000] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,000] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,000] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,001] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,001] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,001] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,002] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,002] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,002] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,002] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,003] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,003] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,003] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,004] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,004] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,004] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,004] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,005] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,005] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,005] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,006] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,005] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,006] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,006] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,007] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,007] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,008] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,007] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,008] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,009] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,009] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,009] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,010] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,009] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,010] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,010] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,011] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,011] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,011] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,012] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,012] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,012] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,013] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,013] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,013] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,013] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,014] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,015] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-22 17:40:31,015] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=9, isr=[5, 6, 4], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-22 17:40:31,466] INFO [Broker id=6] Transitioning 48 partition(s) to local followers. (state.change.logger)
[2025-05-22 17:40:31,467] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6, 4], partitionEpoch=33, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 18. (state.change.logger)
[2025-05-22 17:40:31,467] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 4, 6], partitionEpoch=33, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 18. (state.change.logger)
[2025-05-22 17:40:31,468] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4, 6], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-22 17:40:31,468] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 19. (state.change.logger)
[2025-05-22 17:40:31,469] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 19. (state.change.logger)
[2025-05-22 17:40:31,470] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4, 6], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-22 17:40:31,470] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6, 4], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:31,471] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:31,472] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6, 4], partitionEpoch=33, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:31,472] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4, 6], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-22 17:40:31,474] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 4, 6], partitionEpoch=33, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 18. (state.change.logger)
[2025-05-22 17:40:31,474] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 6, 4], partitionEpoch=35, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 19. (state.change.logger)
[2025-05-22 17:40:31,475] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4, 6], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-22 17:40:31,475] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:31,476] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:31,476] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=9, isr=[5, 4, 6], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-22 17:40:31,477] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:31,477] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=33, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-22 17:40:31,478] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 4, 6], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-22 17:40:31,478] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=33, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-22 17:40:31,478] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6, 4], partitionEpoch=33, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:31,479] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6, 4], partitionEpoch=33, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:31,480] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6, 4], partitionEpoch=33, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 18. (state.change.logger)
[2025-05-22 17:40:31,480] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:31,481] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6, 4], partitionEpoch=33, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:31,482] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6, 4], partitionEpoch=33, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 18. (state.change.logger)
[2025-05-22 17:40:31,482] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6, 4], partitionEpoch=33, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 18. (state.change.logger)
[2025-05-22 17:40:31,482] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6, 4], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:31,483] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6, 4], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:31,483] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=33, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-22 17:40:31,484] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6, 4], partitionEpoch=33, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:31,485] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6, 4], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:31,485] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:31,485] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6, 4], partitionEpoch=33, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:31,486] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=9, isr=[5, 6, 4], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-22 17:40:31,487] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 4, 6], partitionEpoch=33, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 18. (state.change.logger)
[2025-05-22 17:40:31,488] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 4, 6], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-22 17:40:31,488] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6, 4], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:31,489] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 4, 6], partitionEpoch=33, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-22 17:40:31,489] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 4, 6], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-22 17:40:31,490] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 19. (state.change.logger)
[2025-05-22 17:40:31,490] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:31,490] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 4, 6], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-22 17:40:31,491] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 4, 6], partitionEpoch=33, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 15. (state.change.logger)
[2025-05-22 17:40:31,491] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6, 4], partitionEpoch=33, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 18. (state.change.logger)
[2025-05-22 17:40:31,491] INFO [Broker id=6] Skipped the become-follower state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4, 6], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-22 17:40:31,492] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-22 17:40:31,492] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 20. (state.change.logger)
[2025-05-22 17:40:31,493] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,493] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,494] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,494] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,494] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,494] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,495] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,495] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,496] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,496] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,496] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,496] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,497] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,497] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,497] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,498] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,498] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,497] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,499] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,499] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,499] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,500] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,500] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,501] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,500] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,501] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,502] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,502] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,503] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,503] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,503] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,503] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,504] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,504] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,504] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,505] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,505] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,505] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,505] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,506] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,506] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,506] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,506] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,506] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,507] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,507] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,507] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,507] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,508] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,508] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,508] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,509] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,508] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,509] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,509] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,509] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,510] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,510] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,510] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,510] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,511] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,511] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,512] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,512] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,512] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,512] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,513] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,513] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,513] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,514] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,513] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,514] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,514] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,514] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,515] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,515] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,515] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,516] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,516] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,516] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,517] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,517] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,517] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,517] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,517] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,518] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,518] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,519] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,518] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,519] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,520] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,519] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,520] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,520] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,520] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,521] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,521] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,521] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,522] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,522] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,522] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,522] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,523] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,523] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,523] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:31,523] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,524] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:31,524] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,066] INFO [Broker id=6] Transitioning 24 partition(s) to local leaders. (state.change.logger)
[2025-05-22 17:45:27,072] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-47, __consumer_offsets-48, __consumer_offsets-14, financial_transactions-17, financial_transactions-18, __consumer_offsets-9, __consumer_offsets-41, __consumer_offsets-42, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-31, financial_transactions-0, __consumer_offsets-28, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-8, financial_transactions-7, __consumer_offsets-38, __consumer_offsets-35, financial_transactions-10, __consumer_offsets-4, __consumer_offsets-1, financial_transactions-12) (kafka.server.ReplicaFetcherManager)
[2025-05-22 17:45:27,078] INFO [Broker id=6] Leader __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 19 from offset 0 with partition epoch 34, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:45:27,087] INFO [Broker id=6] Leader __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 34, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-22 17:45:27,092] INFO [Broker id=6] Leader __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 19 from offset 0 with partition epoch 34, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:45:27,097] INFO [Broker id=6] Leader __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 34, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-22 17:45:27,101] INFO [Broker id=6] Leader financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 10 from offset 305294 with partition epoch 20, high watermark 305294, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-22 17:45:27,106] INFO [Broker id=6] Leader financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 10 from offset 304723 with partition epoch 20, high watermark 304723, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-22 17:45:27,111] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-13 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,112] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-13 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,112] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-8 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,113] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-8 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,113] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-13 has an older epoch (19) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,114] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-13 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,114] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-46 has an older epoch (19) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,114] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-46 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,114] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-30 has an older epoch (19) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,115] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-30 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,115] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-26 has an older epoch (20) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,116] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-26 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,116] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-5 has an older epoch (20) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,119] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-5 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,120] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-34 has an older epoch (19) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,121] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-34 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,124] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-16 has an older epoch (20) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,122] INFO [Broker id=6] Leader __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 34, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-22 17:45:27,125] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-16 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,126] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition _schemas-0 has an older epoch (20) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,127] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition _schemas-0 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,127] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-49 has an older epoch (20) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,128] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-49 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,128] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-29 has an older epoch (20) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,129] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-29 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,129] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-37 has an older epoch (19) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,130] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-37 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,130] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-11 has an older epoch (19) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,131] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-11 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,132] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-44 has an older epoch (19) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,133] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-44 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,133] INFO [Broker id=6] Leader __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 19 from offset 0 with partition epoch 34, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:45:27,133] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-23 has an older epoch (20) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,135] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-23 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,136] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-7 has an older epoch (19) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,137] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-7 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,138] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-18 has an older epoch (20) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,138] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-18 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,139] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-39 has an older epoch (20) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,140] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-39 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,140] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-2 has an older epoch (20) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,141] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition __consumer_offsets-2 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,141] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-1 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,142] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-1 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,142] INFO [Broker id=6] Leader __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 34, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-22 17:45:27,143] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-5 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,144] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-5 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,145] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-9 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,146] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-9 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,146] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-16 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,147] WARN [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Partition financial_transactions-16 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,150] INFO [Broker id=6] Leader __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 19 from offset 0 with partition epoch 34, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:45:27,162] INFO [Broker id=6] Leader __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 19 from offset 0 with partition epoch 34, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:45:27,167] INFO [Broker id=6] Leader __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 34, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-22 17:45:27,174] INFO [Broker id=6] Leader financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 12 from offset 306847 with partition epoch 20, high watermark 306847, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:45:27,182] INFO [Broker id=6] Leader __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 19 from offset 0 with partition epoch 34, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:45:27,187] INFO [Broker id=6] Leader financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 10 from offset 305161 with partition epoch 20, high watermark 305161, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-22 17:45:27,192] INFO [Broker id=6] Leader __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 19 from offset 0 with partition epoch 34, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:45:27,197] INFO [Broker id=6] Leader __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 34, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-22 17:45:27,201] INFO [Broker id=6] Leader financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 12 from offset 305027 with partition epoch 20, high watermark 305027, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:45:27,205] INFO [Broker id=6] Leader __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 34, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-22 17:45:27,209] INFO [Broker id=6] Leader __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 34, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 15. (state.change.logger)
[2025-05-22 17:45:27,213] INFO [Broker id=6] Leader financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 10 from offset 305695 with partition epoch 20, high watermark 305695, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 9. (state.change.logger)
[2025-05-22 17:45:27,217] INFO [Broker id=6] Leader __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 19 from offset 0 with partition epoch 34, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:45:27,222] INFO [Broker id=6] Leader __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 19 from offset 0 with partition epoch 34, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:45:27,226] INFO [Broker id=6] Leader financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 12 from offset 305079 with partition epoch 20, high watermark 305079, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader Some(5) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:45:27,230] INFO [Broker id=6] Transitioning 24 partition(s) to local followers. (state.change.logger)
[2025-05-22 17:45:27,230] INFO [Broker id=6] Follower financial_transactions-13 starts at leader epoch 13 from offset 305495 with partition epoch 21 and high watermark 305495. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-22 17:45:27,231] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 21 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,231] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 21 from offset 8 with partition epoch 36 and high watermark 8. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,231] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 20 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:45:27,232] INFO [Broker id=6] Follower financial_transactions-16 starts at leader epoch 12 from offset 304634 with partition epoch 21 and high watermark 304634. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:45:27,232] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 20 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:45:27,232] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 20 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:45:27,232] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 20 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:45:27,233] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 21 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,233] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 21 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,233] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 21 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,233] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 21 from offset 8 with partition epoch 36 and high watermark 8. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,234] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 20 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:45:27,234] INFO [Broker id=6] Follower financial_transactions-1 starts at leader epoch 13 from offset 304797 with partition epoch 21 and high watermark 304797. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-22 17:45:27,234] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 21 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,235] INFO [Broker id=6] Follower financial_transactions-5 starts at leader epoch 12 from offset 304293 with partition epoch 21 and high watermark 304293. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:45:27,235] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 20 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:45:27,235] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 21 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,236] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 21 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,236] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 20 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:45:27,236] INFO [Broker id=6] Follower financial_transactions-8 starts at leader epoch 13 from offset 304771 with partition epoch 21 and high watermark 304771. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-22 17:45:27,237] INFO [Broker id=6] Follower financial_transactions-9 starts at leader epoch 12 from offset 304868 with partition epoch 21 and high watermark 304868. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:45:27,237] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 21 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,237] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 20 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:45:27,238] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, financial_transactions-16, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-29, financial_transactions-1, financial_transactions-5, __consumer_offsets-39, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-22 17:45:27,238] INFO [Broker id=6] Stopped fetchers as part of become-follower for 24 partitions (state.change.logger)
[2025-05-22 17:45:27,241] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,242] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,242] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(financial_transactions-13 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,305495), __consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),20,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),20,0), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),20,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),20,0), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,0), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),20,0), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),20,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,0), financial_transactions-8 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,304771), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),20,0), __consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,0), _schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,8), financial_transactions-16 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,304634), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,8), financial_transactions-1 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,304797), financial_transactions-5 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,304293), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),20,0), financial_transactions-9 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,304868), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-22 17:45:27,242] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,243] INFO [Broker id=6] Started fetchers as part of become-follower for 24 partitions (state.change.logger)
[2025-05-22 17:45:27,243] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,244] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,244] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,244] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,245] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,245] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,245] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,245] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,246] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,246] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,246] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,246] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,247] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,247] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,247] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,247] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,248] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,248] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,248] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,248] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,249] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,249] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,249] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,250] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,250] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,250] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,250] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,251] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,251] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,251] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,253] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 15 in epoch 19 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,253] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,254] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 47 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,254] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,254] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 48 in epoch 19 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,255] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,255] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 14 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,255] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,255] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 9 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,256] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,256] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 41 in epoch 19 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,257] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,257] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 42 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,257] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,257] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 22 in epoch 19 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,258] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,258] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 20 in epoch 19 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,258] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,259] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 31 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,259] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,259] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-15 in 4 milliseconds for epoch 19, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,259] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 28 in epoch 19 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,259] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,260] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-47 in 6 milliseconds for epoch 16, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,260] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 25 in epoch 19 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,260] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-48 in 5 milliseconds for epoch 19, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,260] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,261] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 8 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,261] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-14 in 6 milliseconds for epoch 16, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,261] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,261] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-9 in 5 milliseconds for epoch 16, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,262] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 38 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,262] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,262] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-41 in 5 milliseconds for epoch 19, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,262] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 35 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,263] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-42 in 6 milliseconds for epoch 16, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,263] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,263] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-22 in 5 milliseconds for epoch 19, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,263] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 4 in epoch 19 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,264] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-20 in 5 milliseconds for epoch 19, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,264] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,264] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-31 in 5 milliseconds for epoch 16, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,265] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 1 in epoch 19 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,265] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-28 in 5 milliseconds for epoch 19, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,265] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,266] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-25 in 5 milliseconds for epoch 19, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,266] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[21] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,266] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,266] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-8 in 4 milliseconds for epoch 16, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,267] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,267] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,267] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-38 in 5 milliseconds for epoch 16, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,267] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,268] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-35 in 5 milliseconds for epoch 16, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,268] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,268] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-4 in 4 milliseconds for epoch 19, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,268] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,269] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,269] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-1 in 3 milliseconds for epoch 19, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,269] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,270] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,269] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[21]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,270] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[21] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,270] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,270] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,271] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,271] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[21] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,271] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,271] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,272] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[21] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,272] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,272] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,272] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[21] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,273] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,273] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[21]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,273] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,274] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,273] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[21]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,274] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[21] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,274] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,274] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[21]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,274] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,275] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,275] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[21]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,275] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[21] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,275] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,276] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,276] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[21]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,276] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[21] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,276] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,276] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,277] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[21]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,277] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,277] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,277] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[21]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,278] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[21] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,278] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,278] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,278] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,279] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,278] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[21]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,279] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:50:26,160] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:50:26,660] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 18:36:44,499] INFO [RaftManager id=6] Completed transition to Unattached(epoch=21, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=20, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=23133, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-22 18:36:44,589] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=21, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=23133, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=21, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-22 18:36:45,275] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 18:36:45,276] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 18:38:21,367] INFO [RaftManager id=6] Completed transition to Unattached(epoch=22, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=21, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=23140, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-22 18:38:21,422] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=22, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=23140, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=22, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-22 18:38:22,526] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 18:38:22,527] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 20:12:45,823] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 20:12:45,828] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 20:12:45,879] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 20:12:45,927] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 20:12:45,933] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 20:12:45,983] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 20:12:46,024] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 20:12:46,029] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 20:12:46,079] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 20:12:46,090] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 20:12:46,094] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 20:12:46,108] INFO [RaftManager id=6] Completed transition to Unattached(epoch=23, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=22, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=23790, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-22 20:12:46,162] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=23, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=23790, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=23, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-22 20:12:46,246] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 20:12:59,353] INFO [RaftManager id=6] Completed transition to Unattached(epoch=24, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=23, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=23798, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-22 20:12:59,368] INFO [RaftManager id=6] Completed transition to Unattached(epoch=25, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=24, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-22 20:12:59,394] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 20:12:59,400] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=25, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=23798, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=25, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-22 20:12:59,495] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 20:17:09,029] INFO [SnapshotGenerator id=6] Creating new KRaft snapshot file snapshot 00000000000000024298-0000000025 because we have waited at least 60 minute(s). (org.apache.kafka.image.publisher.SnapshotGenerator)
[2025-05-22 20:17:09,089] INFO [SnapshotEmitter id=6] Successfully wrote snapshot 00000000000000024298-0000000025 (org.apache.kafka.image.publisher.SnapshotEmitter)
[2025-05-22 20:22:19,034] INFO [RaftManager id=6] Completed transition to Unattached(epoch=26, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=25, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=24665, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-22 20:22:19,105] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=26, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=24665, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=26, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-22 20:22:19,653] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 20:22:19,654] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 20:22:39,293] INFO [RaftManager id=6] Completed transition to Unattached(epoch=27, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=26, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=24674, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-22 20:22:39,314] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=27, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=24674, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=27, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-22 20:22:39,321] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 20:22:39,322] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 20:22:39,325] INFO [RaftManager id=6] Completed transition to Unattached(epoch=28, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=27, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=24674, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-22 20:22:39,325] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 20:22:39,335] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=28, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=24674, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=28, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-22 20:22:39,338] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 24675 (kafka.log.UnifiedLog)
[2025-05-22 20:22:39,340] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 24675 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 20:22:39,340] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 24675 (kafka.log.UnifiedLog$)
[2025-05-22 20:22:39,341] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=16829, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000016829.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 20:22:39,346] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 24675 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 20:22:39,347] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 5ms for segment recovery from offset 24675 (kafka.log.UnifiedLog$)
[2025-05-22 20:22:39,347] INFO [RaftManager id=6] Truncated to offset 24675 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-22 20:22:39,426] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 20:32:39,468] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 20:32:39,468] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 20:41:27,082] INFO [RaftManager id=6] Completed transition to Unattached(epoch=29, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=28, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26181, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-22 20:41:27,102] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=29, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26181, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=29, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-22 20:41:27,150] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 20:41:27,151] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 20:42:14,743] INFO [RaftManager id=6] Completed transition to Unattached(epoch=30, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=29, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26189, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-22 20:42:14,759] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 20:42:14,763] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=30, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26189, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=30, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-22 20:42:14,773] INFO [RaftManager id=6] Completed transition to Unattached(epoch=31, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=30, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26189, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-22 20:42:14,783] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=31, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=26189, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=31, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-22 20:42:14,783] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Truncating to offset 26190 (kafka.log.UnifiedLog)
[2025-05-22 20:42:14,784] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 26190 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 20:42:14,785] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 26190 (kafka.log.UnifiedLog$)
[2025-05-22 20:42:14,785] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=24675, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000024675.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 20:42:14,788] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 26190 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 20:42:14,789] INFO [UnifiedLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 3ms for segment recovery from offset 26190 (kafka.log.UnifiedLog$)
[2025-05-22 20:42:14,789] INFO [RaftManager id=6] Truncated to offset 26190 from Fetch response from leader 2 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-22 20:42:14,860] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 20:52:14,937] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 20:52:14,938] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 21:04:12,886] INFO [RaftManager id=6] Completed transition to Unattached(epoch=32, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=31, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=28644, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-22 21:04:12,902] INFO [RaftManager id=6] Completed transition to Unattached(epoch=33, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=32, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-22 21:04:12,932] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=33, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=28644, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=33, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-22 21:04:13,708] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 21:04:13,721] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 21:05:05,874] INFO [RaftManager id=6] Completed transition to Unattached(epoch=34, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=33, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=28654, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-22 21:05:05,889] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=34, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional[LogOffsetMetadata(offset=28654, metadata=Optional.empty)], fetchingSnapshot=Optional.empty) from Unattached(epoch=34, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-22 21:05:07,666] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 21:05:07,667] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
