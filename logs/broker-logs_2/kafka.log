[2025-05-20 22:15:26,155] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 22:15:26,176] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 22:15:26,573] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 22:15:26,604] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:39092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 5
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 5
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = n[2025-05-20 22:15:26,608] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
ull
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 22:15:26,612] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:26,662] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 22:15:26,670] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:31,857] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 22:15:31,832] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 22:15:31,967] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 22:15:31,969] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:32,023] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 22:15:32,027] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:32,103] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:32,122] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:32,183] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 22:15:32,186] INFO [BrokerServer id=5] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-20 22:15:32,187] INFO [SharedServer id=5] Starting SharedServer (kafka.server.SharedServer)
[2025-05-20 22:15:32,205] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:32,241] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:32,278] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:32,304] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 22:15:32,317] INFO [BrokerServer id=6] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-20 22:15:32,320] INFO [SharedServer id=6] Starting SharedServer (kafka.server.SharedServer)
[2025-05-20 22:15:32,323] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:32,325] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:32,327] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 1ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:32,333] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:32,370] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-20 22:15:32,401] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:15:32,410] INFO [RaftManager id=5] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:15:32,413] INFO [RaftManager id=5] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:15:32,461] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:32,464] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:32,467] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:32,486] INFO [RaftManager id=5] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1558) from null (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:15:32,487] INFO [kafka-5-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:15:32,491] INFO [kafka-5-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:15:32,491] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-20 22:15:32,512] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:15:32,513] INFO [RaftManager id=6] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:15:32,516] INFO [RaftManager id=6] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:15:32,563] INFO [BrokerServer id=5] Starting broker (kafka.server.BrokerServer)
[2025-05-20 22:15:32,573] INFO [RaftManager id=6] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1288) from null (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:15:32,579] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:32,572] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,582] INFO [kafka-6-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:15:32,590] WARN [RaftManager id=5] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,565] INFO [RaftManager id=5] Registered the listener org.apache.kafka.image.loader.MetadataLoader@403821805 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:15:32,594] INFO [kafka-6-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:15:32,564] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:32,591] INFO [broker-5-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:15:32,590] INFO [broker-5-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:15:32,616] INFO [broker-5-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:15:32,625] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:15:32,646] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,647] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,649] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:32,650] INFO [BrokerServer id=6] Starting broker (kafka.server.BrokerServer)
[2025-05-20 22:15:32,653] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,654] WARN [RaftManager id=5] Connection to node 2 (kafka-controller-2/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,666] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:32,679] INFO [BrokerServer id=5] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 22:15:32,681] INFO [BrokerServer id=5] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 22:15:32,697] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:32,708] INFO [RaftManager id=6] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1382762497 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:15:32,725] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,727] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,730] WARN [RaftManager id=5] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,735] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,738] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,740] INFO [broker-6-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:15:32,738] WARN [RaftManager id=5] Connection to node 2 (kafka-controller-2/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,742] INFO [broker-6-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:15:32,743] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,744] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,752] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:32,761] INFO [broker-6-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:15:32,764] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:15:32,773] INFO [broker-5-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:32,783] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:15:32,811] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:32,813] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,817] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,821] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,828] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,835] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,839] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,853] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:32,852] INFO [BrokerServer id=6] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 22:15:32,859] INFO [BrokerServer id=6] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 22:15:32,865] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,867] WARN [RaftManager id=5] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,868] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,882] WARN [RaftManager id=5] Connection to node 2 (kafka-controller-2/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,885] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,884] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,886] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,889] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,890] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,893] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,898] INFO [broker-6-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:32,911] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:15:32,922] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:32,956] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:32,998] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:32,999] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,017] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,018] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,020] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,021] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,024] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,057] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,083] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,091] WARN [RaftManager id=5] Connection to node 2 (kafka-controller-2/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,110] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,120] WARN [RaftManager id=5] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,123] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,124] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,127] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,161] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,215] INFO [RaftManager id=6] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,216] WARN [RaftManager id=6] Connection to node 2 (kafka-controller-2/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,229] INFO [RaftManager id=6] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,229] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,230] WARN [RaftManager id=6] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,236] INFO [RaftManager id=6] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,237] WARN [RaftManager id=6] Connection to node 1 (kafka-controller-1/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,229] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 22:15:33,263] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,283] INFO [SocketServer listenerType=BROKER, nodeId=5] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-20 22:15:33,286] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 22:15:33,302] INFO [SocketServer listenerType=BROKER, nodeId=5] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-20 22:15:33,322] INFO [broker-5-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,332] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,364] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,386] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 22:15:33,465] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,466] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,467] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,480] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-20 22:15:33,482] INFO [ExpirationReaper-5-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,503] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 22:15:33,503] INFO [ExpirationReaper-5-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,514] INFO [ExpirationReaper-5-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,518] INFO [ExpirationReaper-5-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,516] INFO [ExpirationReaper-5-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,511] INFO [SocketServer listenerType=BROKER, nodeId=6] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-20 22:15:33,540] INFO [ExpirationReaper-5-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,541] INFO [ExpirationReaper-5-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,549] INFO [broker-6-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,568] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,570] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,587] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-05-20 22:15:33,590] INFO [broker-5-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,592] INFO [BrokerLifecycleManager id=5] Incarnation 6BbBQoNISUmkl9rag8Mmdw of broker 5 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:15:33,617] INFO [ExpirationReaper-5-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,621] INFO [RaftManager id=5] Completed transition to Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1558) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:15:33,613] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,674] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,675] INFO [ExpirationReaper-6-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,682] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,681] INFO [ExpirationReaper-6-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,681] INFO [ExpirationReaper-6-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,685] INFO [ExpirationReaper-6-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,719] INFO [BrokerServer id=5] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 22:15:33,720] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,727] INFO [BrokerServer id=5] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 22:15:33,728] INFO [BrokerServer id=5] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 22:15:33,735] INFO [ExpirationReaper-6-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,737] INFO [ExpirationReaper-6-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,736] INFO [ExpirationReaper-6-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,766] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-05-20 22:15:33,767] INFO [broker-6-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,769] INFO [BrokerLifecycleManager id=6] Incarnation Krozf-MbRJe0B6F2yKvbaw of broker 6 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:15:33,789] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,789] INFO [ExpirationReaper-6-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:15:33,801] INFO [RaftManager id=6] Completed transition to Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1288) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:15:33,803] INFO [BrokerServer id=6] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 22:15:33,803] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,806] INFO [BrokerServer id=6] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 22:15:33,806] INFO [BrokerServer id=6] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 22:15:33,825] INFO [RaftManager id=6] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:15:33,829] INFO [RaftManager id=5] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=8, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=8, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:15:33,831] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,868] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,876] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,876] INFO [broker-5-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,877] INFO [broker-6-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,878] INFO [broker-5-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,883] INFO [broker-6-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,877] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,901] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:33,906] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,932] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:33,993] INFO [NodeToControllerChannelManager id=6 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:33,994] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:34,003] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:15:34,005] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:34,013] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,033] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,044] INFO [broker-6-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:34,055] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:15:34,105] INFO [BrokerLifecycleManager id=6] Successfully registered broker 6 with broker epoch 1532 (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:15:34,117] INFO [MetadataLoader id=6] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,124] INFO [BrokerLifecycleManager id=5] Successfully registered broker 5 with broker epoch 1534 (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:15:34,135] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,138] INFO [RaftManager id=6] High watermark set to Optional[LogOffsetMetadata(offset=1531, metadata=Optional.empty)] for the first time for epoch 8 (org.apache.kafka.raft.FollowerState)
[2025-05-20 22:15:34,208] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 1531 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,236] INFO [RaftManager id=5] High watermark set to Optional[LogOffsetMetadata(offset=1531, metadata=Optional.empty)] for the first time for epoch 8 (org.apache.kafka.raft.FollowerState)
[2025-05-20 22:15:34,238] INFO [MetadataLoader id=5] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1531 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,286] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 1531 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,408] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 1530, but the high water mark is 1535 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,425] INFO [MetadataLoader id=6] initializeNewPublishers: The loader is still catching up because we have loaded up to offset 1530, but the high water mark is 1535 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,432] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 1530, but the high water mark is 1535 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,454] INFO [MetadataLoader id=5] initializeNewPublishers: The loader is still catching up because we have loaded up to offset 1530, but the high water mark is 1536 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,458] INFO [MetadataLoader id=6] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 1535 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,462] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 1534 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,462] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing MetadataVersionPublisher(id=6) with a snapshot at offset 1534 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,463] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 1534 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,464] INFO [BrokerMetadataPublisher id=6] Publishing initial metadata at offset OffsetAndEpoch(offset=1534, epoch=8) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-20 22:15:34,468] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 1534, but the high water mark is 1536 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,473] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 1536 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,469] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:15:34,477] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 1535 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,479] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing MetadataVersionPublisher(id=5) with a snapshot at offset 1535 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,480] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 1535 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,481] INFO [BrokerMetadataPublisher id=5] Publishing initial metadata at offset OffsetAndEpoch(offset=1535, epoch=8) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-20 22:15:34,486] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-05-20 22:15:34,493] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:15:34,498] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-05-20 22:15:34,507] INFO [BrokerLifecycleManager id=5] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:15:34,516] INFO [BrokerServer id=5] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 22:15:34,515] INFO Loaded 0 logs in 38ms (kafka.log.LogManager)
[2025-05-20 22:15:34,517] INFO [BrokerServer id=5] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 22:15:34,521] INFO [BrokerLifecycleManager id=6] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:15:34,523] INFO [BrokerServer id=6] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 22:15:34,524] INFO [BrokerServer id=6] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 22:15:34,526] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-20 22:15:34,532] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-20 22:15:34,546] INFO Loaded 0 logs in 50ms (kafka.log.LogManager)
[2025-05-20 22:15:34,547] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-20 22:15:34,551] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-20 22:15:34,559] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-20 22:15:34,585] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-20 22:15:34,606] INFO [BrokerLifecycleManager id=5] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:15:34,623] INFO [BrokerLifecycleManager id=6] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:15:34,786] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:15:34,789] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:15:34,790] INFO [AddPartitionsToTxnSenderThread-6]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:15:34,791] INFO [GroupCoordinator 6]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:34,793] INFO [GroupCoordinator 6]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:34,795] INFO [TransactionCoordinator id=6] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:15:34,798] INFO [TxnMarkerSenderThread-6]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:15:34,798] INFO [TransactionCoordinator id=6] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:15:34,805] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:15:34,810] INFO [MetadataLoader id=6] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=6) with a snapshot at offset 1534 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,811] INFO [BrokerServer id=6] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 22:15:34,819] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:15:34,819] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:49092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 6
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 6
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = n[2025-05-20 22:15:34,822] INFO [AddPartitionsToTxnSenderThread-5]: Starting (kafka.server.AddPartitionsToTxnManager)
ull
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 22:15:34,824] INFO [GroupCoordinator 5]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:34,825] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:34,833] INFO [GroupCoordinator 5]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:34,839] INFO [TransactionCoordinator id=5] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:15:34,835] INFO [BrokerServer id=6] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 22:15:34,848] INFO [TransactionCoordinator id=5] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:15:34,849] INFO [TxnMarkerSenderThread-5]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:15:34,868] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=5) with a snapshot at offset 1535 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:15:34,868] INFO [BrokerServer id=5] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 22:15:34,872] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:39092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 5
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 5
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 22:15:34,875] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:15:34,886] INFO [BrokerServer id=5] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 22:15:34,892] INFO [BrokerLifecycleManager id=6] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:15:34,894] INFO [BrokerServer id=6] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 22:15:34,896] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 22:15:34,896] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 22:15:34,897] INFO [SocketServer listenerType=BROKER, nodeId=6] Enabling request processing. (kafka.network.SocketServer)
[2025-05-20 22:15:34,899] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 22:15:34,902] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 22:15:34,912] INFO [BrokerServer id=6] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 22:15:34,913] INFO [BrokerServer id=6] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 22:15:34,914] INFO [BrokerServer id=6] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 22:15:34,915] INFO [BrokerServer id=6] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 22:15:34,915] INFO [BrokerServer id=6] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-20 22:15:34,916] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:15:34,916] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:15:34,918] INFO Kafka startTimeMs: 1747779334916 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:15:34,923] INFO [BrokerLifecycleManager id=5] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:15:34,924] INFO [BrokerServer id=5] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 22:15:34,925] INFO [KafkaRaftServer nodeId=6] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-20 22:15:34,927] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 22:15:34,928] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 22:15:34,929] INFO [SocketServer listenerType=BROKER, nodeId=5] Enabling request processing. (kafka.network.SocketServer)
[2025-05-20 22:15:34,932] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 22:15:34,937] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 22:15:34,948] INFO [BrokerServer id=5] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 22:15:34,949] INFO [BrokerServer id=5] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 22:15:34,949] INFO [BrokerServer id=5] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 22:15:34,953] INFO [BrokerServer id=5] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 22:15:34,954] INFO [BrokerServer id=5] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-20 22:15:34,955] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:15:34,956] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:15:34,957] INFO Kafka startTimeMs: 1747779334955 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:15:34,961] INFO [KafkaRaftServer nodeId=5] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-20 22:15:40,566] INFO [Broker id=5] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:15:40,569] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:15:40,569] INFO [Broker id=5] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-20 22:15:40,573] INFO [Broker id=6] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-20 22:15:40,590] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:40,592] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:40,595] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-05-20 22:15:40,595] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-05-20 22:15:40,599] INFO [Partition _schemas-0 broker=5] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-05-20 22:15:40,600] INFO [Partition _schemas-0 broker=6] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-05-20 22:15:40,601] INFO [Partition _schemas-0 broker=5] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:40,601] INFO [Partition _schemas-0 broker=6] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:40,602] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:40,602] INFO [Broker id=5] Follower _schemas-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:40,604] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:15:40,604] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:15:40,605] INFO [Broker id=6] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-05-20 22:15:40,605] INFO [Broker id=5] Stopped fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-05-20 22:15:40,643] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:40,644] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:40,647] INFO [ReplicaFetcherManager on broker 5] Added fetcher to broker 4 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:15:40,648] INFO [Broker id=5] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-05-20 22:15:40,649] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:40,649] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(_schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:15:40,650] INFO [Broker id=6] Started fetchers as part of become-follower for 1 partitions (state.change.logger)
[2025-05-20 22:15:40,651] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:40,652] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:40,656] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:40,664] INFO [DynamicConfigPublisher broker id=5] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 22:15:40,670] INFO [DynamicConfigPublisher broker id=6] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 22:15:41,554] INFO [Broker id=6] Transitioning 17 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:15:41,556] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-47, __consumer_offsets-48, __consumer_offsets-14, __consumer_offsets-9, __consumer_offsets-41, __consumer_offsets-42, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-31, __consumer_offsets-28, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-38, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-1) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:15:41,563] INFO [Broker id=5] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:15:41,564] INFO [Broker id=6] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,566] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-45, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-0, __consumer_offsets-32, __consumer_offsets-27, __consumer_offsets-40, __consumer_offsets-6, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-33) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:15:41,567] INFO [Broker id=5] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,582] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,582] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,584] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,584] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,585] INFO [Partition __consumer_offsets-45 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-05-20 22:15:41,585] INFO [Partition __consumer_offsets-45 broker=5] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,585] INFO [Partition __consumer_offsets-15 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-05-20 22:15:41,586] INFO [Partition __consumer_offsets-15 broker=6] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,587] INFO [Broker id=5] Leader __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,589] INFO [Broker id=6] Leader __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,609] INFO [Broker id=5] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,613] INFO [Broker id=6] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,615] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,618] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,619] INFO [Partition __consumer_offsets-43 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-05-20 22:15:41,619] INFO [Partition __consumer_offsets-43 broker=5] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,624] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,627] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,628] INFO [Partition __consumer_offsets-47 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-05-20 22:15:41,628] INFO [Broker id=5] Leader __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,629] INFO [Partition __consumer_offsets-47 broker=6] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,629] INFO [Broker id=6] Leader __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,634] INFO [Broker id=5] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,640] INFO [Broker id=6] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,647] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,650] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,650] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,651] INFO [Partition __consumer_offsets-48 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-05-20 22:15:41,651] INFO [Partition __consumer_offsets-48 broker=6] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,653] INFO [Broker id=6] Leader __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,653] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,655] INFO [Partition __consumer_offsets-12 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-05-20 22:15:41,655] INFO [Partition __consumer_offsets-12 broker=5] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,656] INFO [Broker id=5] Leader __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,661] INFO [Broker id=6] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,662] INFO [Broker id=5] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,666] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,668] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,668] INFO [Partition __consumer_offsets-14 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-05-20 22:15:41,669] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,669] INFO [Partition __consumer_offsets-14 broker=6] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,669] INFO [Broker id=6] Leader __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,670] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,670] INFO [Partition __consumer_offsets-10 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-05-20 22:15:41,671] INFO [Partition __consumer_offsets-10 broker=5] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,671] INFO [Broker id=5] Leader __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,675] INFO [Broker id=6] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,677] INFO [Broker id=5] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,680] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,680] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,681] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,682] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,682] INFO [Partition __consumer_offsets-9 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-05-20 22:15:41,682] INFO [Partition __consumer_offsets-9 broker=6] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,682] INFO [Partition __consumer_offsets-24 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-05-20 22:15:41,683] INFO [Broker id=6] Leader __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,683] INFO [Partition __consumer_offsets-24 broker=5] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,684] INFO [Broker id=5] Leader __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,688] INFO [Broker id=5] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,688] INFO [Broker id=6] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,693] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,693] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,694] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,694] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,695] INFO [Partition __consumer_offsets-21 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-05-20 22:15:41,695] INFO [Partition __consumer_offsets-41 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-05-20 22:15:41,695] INFO [Partition __consumer_offsets-41 broker=6] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,695] INFO [Partition __consumer_offsets-21 broker=5] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,696] INFO [Broker id=6] Leader __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,696] INFO [Broker id=5] Leader __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,701] INFO [Broker id=6] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,701] INFO [Broker id=5] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,705] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,705] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,706] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,707] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,707] INFO [Partition __consumer_offsets-19 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-05-20 22:15:41,707] INFO [Partition __consumer_offsets-42 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-05-20 22:15:41,707] INFO [Partition __consumer_offsets-19 broker=5] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,707] INFO [Partition __consumer_offsets-42 broker=6] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,707] INFO [Broker id=5] Leader __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,708] INFO [Broker id=6] Leader __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,713] INFO [Broker id=5] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,714] INFO [Broker id=6] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,717] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,719] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,719] INFO [Partition __consumer_offsets-17 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-05-20 22:15:41,719] INFO [Partition __consumer_offsets-17 broker=5] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,720] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,720] INFO [Broker id=5] Leader __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,721] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,721] INFO [Partition __consumer_offsets-22 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-05-20 22:15:41,721] INFO [Partition __consumer_offsets-22 broker=6] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,722] INFO [Broker id=6] Leader __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,726] INFO [Broker id=5] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,727] INFO [Broker id=6] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,730] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,732] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,732] INFO [Partition __consumer_offsets-0 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,733] INFO [Partition __consumer_offsets-0 broker=5] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,733] INFO [Broker id=5] Leader __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,733] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,735] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,735] INFO [Partition __consumer_offsets-20 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-05-20 22:15:41,736] INFO [Partition __consumer_offsets-20 broker=6] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,736] INFO [Broker id=6] Leader __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,740] INFO [Broker id=5] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,742] INFO [Broker id=6] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,745] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,746] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,746] INFO [Partition __consumer_offsets-32 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-05-20 22:15:41,747] INFO [Partition __consumer_offsets-32 broker=5] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,747] INFO [Broker id=5] Leader __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,747] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,748] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,749] INFO [Partition __consumer_offsets-31 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-05-20 22:15:41,749] INFO [Partition __consumer_offsets-31 broker=6] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,749] INFO [Broker id=6] Leader __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,754] INFO [Broker id=5] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,755] INFO [Broker id=6] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,759] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,761] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,761] INFO [Partition __consumer_offsets-27 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-05-20 22:15:41,762] INFO [Partition __consumer_offsets-27 broker=5] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,762] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,762] INFO [Broker id=5] Leader __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,763] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,764] INFO [Partition __consumer_offsets-28 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-05-20 22:15:41,765] INFO [Partition __consumer_offsets-28 broker=6] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,767] INFO [Broker id=6] Leader __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,772] INFO [Broker id=5] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,773] INFO [Broker id=6] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,776] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,778] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,778] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,779] INFO [Partition __consumer_offsets-40 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-05-20 22:15:41,779] INFO [Partition __consumer_offsets-40 broker=5] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,779] INFO [Broker id=5] Leader __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,782] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,782] INFO [Partition __consumer_offsets-25 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-05-20 22:15:41,782] INFO [Partition __consumer_offsets-25 broker=6] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,783] INFO [Broker id=6] Leader __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,786] INFO [Broker id=5] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,788] INFO [Broker id=6] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,792] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,794] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,794] INFO [Partition __consumer_offsets-6 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-05-20 22:15:41,795] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,795] INFO [Partition __consumer_offsets-6 broker=5] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,796] INFO [Broker id=5] Leader __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,796] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,797] INFO [Partition __consumer_offsets-8 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-05-20 22:15:41,797] INFO [Partition __consumer_offsets-8 broker=6] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,797] INFO [Broker id=6] Leader __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,802] INFO [Broker id=6] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,803] INFO [Broker id=5] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,810] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,811] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,811] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,812] INFO [Partition __consumer_offsets-38 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-05-20 22:15:41,812] INFO [Partition __consumer_offsets-38 broker=6] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,812] INFO [Broker id=6] Leader __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,813] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,813] INFO [Partition __consumer_offsets-3 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-05-20 22:15:41,813] INFO [Partition __consumer_offsets-3 broker=5] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,814] INFO [Broker id=5] Leader __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,818] INFO [Broker id=6] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,819] INFO [Broker id=5] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,825] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,826] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,827] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,827] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,827] INFO [Partition __consumer_offsets-36 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-05-20 22:15:41,828] INFO [Partition __consumer_offsets-35 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-05-20 22:15:41,828] INFO [Partition __consumer_offsets-36 broker=5] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,828] INFO [Partition __consumer_offsets-35 broker=6] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,829] INFO [Broker id=6] Leader __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,829] INFO [Broker id=5] Leader __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,834] INFO [Broker id=6] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,834] INFO [Broker id=5] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,840] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,841] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,842] INFO [Partition __consumer_offsets-33 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-05-20 22:15:41,842] INFO [Partition __consumer_offsets-33 broker=5] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,842] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,842] INFO [Broker id=5] Leader __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,843] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,844] INFO [Partition __consumer_offsets-4 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-05-20 22:15:41,844] INFO [Partition __consumer_offsets-4 broker=6] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,845] INFO [Broker id=6] Leader __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,848] INFO [Broker id=5] Transitioning 34 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:15:41,849] INFO [Broker id=5] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,849] INFO [Broker id=6] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,853] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,854] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,856] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,856] INFO [Partition __consumer_offsets-1 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-05-20 22:15:41,856] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,857] INFO [Partition __consumer_offsets-1 broker=6] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,857] INFO [Partition __consumer_offsets-15 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-05-20 22:15:41,857] INFO [Broker id=6] Leader __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 22:15:41,858] INFO [Partition __consumer_offsets-15 broker=5] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,858] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,859] INFO [Broker id=5] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,863] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,865] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,865] INFO [Partition __consumer_offsets-48 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-05-20 22:15:41,865] INFO [Partition __consumer_offsets-48 broker=5] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,866] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,866] INFO [Broker id=5] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,867] INFO [Broker id=6] Transitioning 33 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:15:41,867] INFO [Broker id=6] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,871] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,872] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,873] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,873] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,874] INFO [Partition __consumer_offsets-13 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-05-20 22:15:41,875] INFO [Partition __consumer_offsets-13 broker=5] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,875] INFO [Partition __consumer_offsets-13 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-05-20 22:15:41,875] INFO [Broker id=5] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,876] INFO [Partition __consumer_offsets-13 broker=6] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,876] INFO [Broker id=5] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,876] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,877] INFO [Broker id=6] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,880] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,882] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,883] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,884] INFO [Partition __consumer_offsets-46 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-05-20 22:15:41,885] INFO [Partition __consumer_offsets-46 broker=5] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,885] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,885] INFO [Partition __consumer_offsets-46 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-05-20 22:15:41,885] INFO [Broker id=5] Follower __consumer_offsets-46 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,885] INFO [Partition __consumer_offsets-46 broker=6] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,886] INFO [Broker id=5] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,886] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,890] INFO [Broker id=6] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,891] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,892] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,893] INFO [Partition __consumer_offsets-11 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-05-20 22:15:41,893] INFO [Partition __consumer_offsets-11 broker=5] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,894] INFO [Broker id=5] Follower __consumer_offsets-11 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,894] INFO [Broker id=5] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,896] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,898] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,898] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,898] INFO [Partition __consumer_offsets-11 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-05-20 22:15:41,899] INFO [Partition __consumer_offsets-11 broker=6] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,899] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,899] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,900] INFO [Broker id=6] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,900] INFO [Partition __consumer_offsets-44 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-05-20 22:15:41,900] INFO [Partition __consumer_offsets-44 broker=5] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,901] INFO [Broker id=5] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,901] INFO [Broker id=5] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,906] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,906] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,907] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,907] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,908] INFO [Partition __consumer_offsets-9 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-05-20 22:15:41,908] INFO [Partition __consumer_offsets-44 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-05-20 22:15:41,908] INFO [Partition __consumer_offsets-9 broker=5] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,908] INFO [Partition __consumer_offsets-44 broker=6] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,908] INFO [Broker id=5] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,909] INFO [Broker id=5] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,909] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,909] INFO [Broker id=6] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,914] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,915] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,915] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,916] INFO [Partition __consumer_offsets-42 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-05-20 22:15:41,917] INFO [Partition __consumer_offsets-42 broker=5] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,917] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,917] INFO [Broker id=5] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,918] INFO [Partition __consumer_offsets-23 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-05-20 22:15:41,918] INFO [Broker id=5] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,918] INFO [Partition __consumer_offsets-23 broker=6] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,919] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,919] INFO [Broker id=6] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,923] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,924] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,925] INFO [Partition __consumer_offsets-23 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-05-20 22:15:41,926] INFO [Partition __consumer_offsets-23 broker=5] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,926] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,927] INFO [Broker id=5] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,927] INFO [Broker id=5] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,929] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,929] INFO [Partition __consumer_offsets-21 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-05-20 22:15:41,929] INFO [Partition __consumer_offsets-21 broker=6] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,930] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,930] INFO [Broker id=6] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,932] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,933] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,934] INFO [Partition __consumer_offsets-30 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-05-20 22:15:41,934] INFO [Partition __consumer_offsets-30 broker=5] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,935] INFO [Broker id=5] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,935] INFO [Broker id=5] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,936] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,937] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,938] INFO [Partition __consumer_offsets-19 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-05-20 22:15:41,938] INFO [Partition __consumer_offsets-19 broker=6] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,938] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,938] INFO [Broker id=6] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,941] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,942] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,942] INFO [Partition __consumer_offsets-28 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-05-20 22:15:41,943] INFO [Partition __consumer_offsets-28 broker=5] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,943] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,943] INFO [Broker id=5] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,944] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,945] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,946] INFO [Partition __consumer_offsets-17 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-05-20 22:15:41,946] INFO [Partition __consumer_offsets-17 broker=6] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,947] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,947] INFO [Broker id=6] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,949] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,950] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,952] INFO [Partition __consumer_offsets-26 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-05-20 22:15:41,952] INFO [Partition __consumer_offsets-26 broker=5] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,953] INFO [Broker id=5] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,953] INFO [Broker id=5] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,955] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,957] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,957] INFO [Partition __consumer_offsets-32 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-05-20 22:15:41,957] INFO [Partition __consumer_offsets-32 broker=6] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,958] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,958] INFO [Broker id=6] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,958] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,960] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,960] INFO [Partition __consumer_offsets-7 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-05-20 22:15:41,960] INFO [Partition __consumer_offsets-7 broker=5] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,961] INFO [Broker id=5] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,961] INFO [Broker id=5] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,964] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,966] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,967] INFO [Partition __consumer_offsets-30 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-05-20 22:15:41,967] INFO [Partition __consumer_offsets-30 broker=6] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,967] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,968] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,968] INFO [Broker id=6] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,969] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,970] INFO [Partition __consumer_offsets-5 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-05-20 22:15:41,970] INFO [Partition __consumer_offsets-5 broker=5] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,970] INFO [Broker id=5] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,971] INFO [Broker id=5] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,974] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,976] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,976] INFO [Partition __consumer_offsets-26 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-05-20 22:15:41,976] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,977] INFO [Partition __consumer_offsets-26 broker=6] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,978] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,978] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,978] INFO [Partition __consumer_offsets-38 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-05-20 22:15:41,978] INFO [Broker id=6] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,979] INFO [Partition __consumer_offsets-38 broker=5] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,979] INFO [Broker id=5] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,980] INFO [Broker id=5] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,985] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,985] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,986] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,986] INFO [Partition __consumer_offsets-7 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-05-20 22:15:41,986] INFO [Partition __consumer_offsets-7 broker=6] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,987] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,987] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,987] INFO [Broker id=6] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,987] INFO [Partition __consumer_offsets-1 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-05-20 22:15:41,988] INFO [Partition __consumer_offsets-1 broker=5] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,989] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,990] INFO [Broker id=5] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,994] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,995] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,996] INFO [Partition __consumer_offsets-40 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-05-20 22:15:41,996] INFO [Partition __consumer_offsets-40 broker=6] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:41,996] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:41,997] INFO [Broker id=6] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:41,998] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:41,999] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:41,999] INFO [Partition __consumer_offsets-34 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-05-20 22:15:42,000] INFO [Partition __consumer_offsets-34 broker=5] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,000] INFO [Broker id=5] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,000] INFO [Broker id=5] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,002] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,004] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,004] INFO [Partition __consumer_offsets-5 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-05-20 22:15:42,005] INFO [Partition __consumer_offsets-5 broker=6] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,005] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,005] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,006] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,006] INFO [Partition __consumer_offsets-47 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-05-20 22:15:42,007] INFO [Partition __consumer_offsets-47 broker=5] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,007] INFO [Broker id=5] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,008] INFO [Broker id=5] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,009] INFO [Broker id=6] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,014] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,015] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,015] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,015] INFO [Partition __consumer_offsets-16 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-05-20 22:15:42,016] INFO [Partition __consumer_offsets-16 broker=5] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,016] INFO [Broker id=5] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,016] INFO [Broker id=5] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,017] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,018] INFO [Partition __consumer_offsets-3 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-05-20 22:15:42,018] INFO [Partition __consumer_offsets-3 broker=6] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,018] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,019] INFO [Broker id=6] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,021] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,024] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,024] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,025] INFO [Partition __consumer_offsets-14 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-05-20 22:15:42,025] INFO [Partition __consumer_offsets-14 broker=5] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,025] INFO [Broker id=5] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,026] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,026] INFO [Partition __consumer_offsets-36 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-05-20 22:15:42,026] INFO [Broker id=5] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,027] INFO [Partition __consumer_offsets-36 broker=6] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,027] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,027] INFO [Broker id=6] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,032] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,032] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,033] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,034] INFO [Partition __consumer_offsets-41 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-05-20 22:15:42,034] INFO [Partition __consumer_offsets-41 broker=5] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,034] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,034] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,035] INFO [Partition __consumer_offsets-34 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-05-20 22:15:42,035] INFO [Broker id=5] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,035] INFO [Partition __consumer_offsets-34 broker=6] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,036] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,036] INFO [Broker id=6] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,041] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,042] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,043] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,043] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,044] INFO [Partition __consumer_offsets-22 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-05-20 22:15:42,044] INFO [Partition __consumer_offsets-16 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-05-20 22:15:42,044] INFO [Partition __consumer_offsets-22 broker=5] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,044] INFO [Partition __consumer_offsets-16 broker=6] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,045] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,045] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,045] INFO [Broker id=5] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,045] INFO [Broker id=6] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,050] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,050] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,051] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,053] INFO [Partition __consumer_offsets-45 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-05-20 22:15:42,053] INFO [Partition __consumer_offsets-45 broker=6] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,053] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,054] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,054] INFO [Partition __consumer_offsets-20 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-05-20 22:15:42,054] INFO [Broker id=6] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,054] INFO [Partition __consumer_offsets-20 broker=5] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,055] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,055] INFO [Broker id=5] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,060] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,062] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,062] INFO [Partition __consumer_offsets-49 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-05-20 22:15:42,062] INFO [Partition __consumer_offsets-49 broker=5] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,063] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,063] INFO [Broker id=5] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,063] INFO [Broker id=5] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,065] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,066] INFO [Partition __consumer_offsets-43 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-05-20 22:15:42,066] INFO [Partition __consumer_offsets-43 broker=6] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,067] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,067] INFO [Broker id=6] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,068] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,070] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,070] INFO [Partition __consumer_offsets-18 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-05-20 22:15:42,071] INFO [Partition __consumer_offsets-18 broker=5] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,071] INFO [Broker id=5] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,072] INFO [Broker id=5] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,073] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,075] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,076] INFO [Partition __consumer_offsets-12 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-05-20 22:15:42,076] INFO [Partition __consumer_offsets-12 broker=6] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,077] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,077] INFO [Broker id=6] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,077] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,079] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,079] INFO [Partition __consumer_offsets-31 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-05-20 22:15:42,079] INFO [Partition __consumer_offsets-31 broker=5] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,080] INFO [Broker id=5] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,080] INFO [Broker id=5] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,082] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,083] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,083] INFO [Partition __consumer_offsets-10 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-05-20 22:15:42,083] INFO [Partition __consumer_offsets-10 broker=6] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,084] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,084] INFO [Broker id=6] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,085] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,086] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,087] INFO [Partition __consumer_offsets-29 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-05-20 22:15:42,087] INFO [Partition __consumer_offsets-29 broker=5] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,088] INFO [Broker id=5] Follower __consumer_offsets-29 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,088] INFO [Broker id=5] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,089] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,090] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,091] INFO [Partition __consumer_offsets-24 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-05-20 22:15:42,091] INFO [Partition __consumer_offsets-24 broker=6] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,091] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,092] INFO [Broker id=6] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,094] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,095] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,096] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,096] INFO [Partition __consumer_offsets-25 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-05-20 22:15:42,096] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,097] INFO [Partition __consumer_offsets-25 broker=5] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,097] INFO [Partition __consumer_offsets-49 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-05-20 22:15:42,097] INFO [Partition __consumer_offsets-49 broker=6] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,097] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,097] INFO [Broker id=5] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,097] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,098] INFO [Broker id=6] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,103] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,103] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,104] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,104] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,104] INFO [Partition __consumer_offsets-18 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-05-20 22:15:42,105] INFO [Partition __consumer_offsets-18 broker=6] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,105] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,105] INFO [Broker id=6] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,107] INFO [Partition __consumer_offsets-39 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-05-20 22:15:42,110] INFO [Partition __consumer_offsets-39 broker=5] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,110] INFO [Broker id=5] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,111] INFO [Broker id=5] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,111] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,112] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,113] INFO [Partition __consumer_offsets-0 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,114] INFO [Partition __consumer_offsets-0 broker=6] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,115] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,116] INFO [Broker id=6] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,119] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,120] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,120] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,121] INFO [Partition __consumer_offsets-8 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-05-20 22:15:42,121] INFO [Partition __consumer_offsets-8 broker=5] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,121] INFO [Broker id=5] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,122] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,122] INFO [Broker id=5] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,122] INFO [Partition __consumer_offsets-29 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-05-20 22:15:42,122] INFO [Partition __consumer_offsets-29 broker=6] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,123] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,124] INFO [Broker id=6] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,126] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,128] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,128] INFO [Partition __consumer_offsets-37 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-05-20 22:15:42,128] INFO [Partition __consumer_offsets-37 broker=5] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,129] INFO [Broker id=5] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,129] INFO [Broker id=5] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,129] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,130] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,131] INFO [Partition __consumer_offsets-27 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-05-20 22:15:42,131] INFO [Partition __consumer_offsets-27 broker=6] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,132] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,132] INFO [Broker id=6] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,135] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,136] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,136] INFO [Partition __consumer_offsets-35 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-05-20 22:15:42,136] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,137] INFO [Partition __consumer_offsets-35 broker=5] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,137] INFO [Broker id=5] Follower __consumer_offsets-35 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,138] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,138] INFO [Broker id=5] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,143] INFO [Partition __consumer_offsets-39 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-05-20 22:15:42,144] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,144] INFO [Partition __consumer_offsets-39 broker=6] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,144] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,145] INFO [Broker id=6] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,145] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,146] INFO [Partition __consumer_offsets-4 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-05-20 22:15:42,147] INFO [Partition __consumer_offsets-4 broker=5] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,147] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,148] INFO [Broker id=5] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,152] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,152] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,153] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,154] INFO [Partition __consumer_offsets-2 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-05-20 22:15:42,154] INFO [Partition __consumer_offsets-2 broker=5] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,154] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,155] INFO [Broker id=5] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,155] INFO [Partition __consumer_offsets-37 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-05-20 22:15:42,155] INFO [Partition __consumer_offsets-37 broker=6] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,156] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,156] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-14, __consumer_offsets-41, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:15:42,156] INFO [Broker id=5] Stopped fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-05-20 22:15:42,157] INFO [Broker id=6] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,159] INFO [ReplicaFetcherManager on broker 5] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:15:42,161] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,162] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,163] INFO [Partition __consumer_offsets-6 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-05-20 22:15:42,164] INFO [Partition __consumer_offsets-6 broker=6] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,164] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,165] INFO [Broker id=6] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,165] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,166] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,166] INFO [ReplicaFetcherManager on broker 5] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-47 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-48 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-14 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-41 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-9 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-42 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-22 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-20 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-31 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-28 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-25 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-8 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-38 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-35 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-4 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0), __consumer_offsets-1 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:15:42,166] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,166] INFO [Broker id=5] Started fetchers as part of become-follower for 34 partitions (state.change.logger)
[2025-05-20 22:15:42,167] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,167] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,168] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,168] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,169] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,169] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,170] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,170] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,170] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,170] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,171] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,171] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,171] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,171] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,172] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,172] INFO [Partition __consumer_offsets-33 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-05-20 22:15:42,172] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,172] INFO [Partition __consumer_offsets-33 broker=6] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,172] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,172] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,172] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 5. Previous leader Some(5) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,173] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,173] INFO [Broker id=6] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:15:42,173] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,173] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,174] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,174] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,175] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,175] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,175] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,176] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,176] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,176] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,177] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,177] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:15:42,177] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,178] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,178] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,178] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:15:42,178] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,179] INFO [Partition __consumer_offsets-2 broker=6] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-05-20 22:15:42,179] INFO [Partition __consumer_offsets-2 broker=6] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:15:42,179] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 22:15:42,182] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-5, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-37, __consumer_offsets-6, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:15:42,182] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,183] INFO [Broker id=6] Stopped fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-05-20 22:15:42,185] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,186] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:15:42,187] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,187] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,190] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,190] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,191] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,192] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,192] INFO [ReplicaFetcherThread-0-5]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,193] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,193] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,193] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,194] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,193] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,194] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,194] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,192] INFO [ReplicaFetcherManager on broker 6] Added fetcher to broker 5 for partitions HashMap(__consumer_offsets-45 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-43 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-12 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-10 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-24 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-21 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-19 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-17 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-32 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-0 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-27 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-40 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-6 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-3 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-36 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0), __consumer_offsets-33 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=5, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:15:42,194] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,194] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,195] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,194] INFO [Broker id=6] Started fetchers as part of become-follower for 33 partitions (state.change.logger)
[2025-05-20 22:15:42,195] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,195] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,195] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-45 in 8 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,196] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,196] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,196] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,196] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-43 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,197] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,197] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,197] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,197] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-12 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,197] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,197] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,198] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-10 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,198] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,198] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,199] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,199] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-24 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,199] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,199] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,199] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,199] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-21 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,200] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,200] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,200] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,200] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,200] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,200] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-19 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,200] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,201] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,201] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,201] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-17 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,201] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,201] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,201] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,202] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,202] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-0 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,202] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,202] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,203] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-32 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,203] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,203] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,204] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,204] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,204] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,204] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-27 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,205] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,205] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,206] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-40 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,206] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,206] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,207] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-6 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,208] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-3 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,207] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,208] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-36 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,209] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-33 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,209] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,210] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,210] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,210] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,211] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,211] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,212] INFO [ReplicaFetcher replicaId=6, leaderId=5, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,213] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,214] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,220] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,222] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,223] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,224] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,224] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,225] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,225] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,225] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,226] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,217] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,226] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,227] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,227] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,227] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,227] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,227] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,227] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,228] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,228] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,229] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,229] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,229] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-15 in 5 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,228] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,229] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,230] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-47 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,230] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,230] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,230] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,231] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,231] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,231] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-48 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,231] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,232] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,231] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,232] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,231] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,232] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-14 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,232] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,232] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,233] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,233] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,233] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-9 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,233] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,233] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,234] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,234] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-41 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,234] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,234] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,235] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,235] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-42 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,235] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,235] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,235] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-22 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,236] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,236] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,235] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,236] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-20 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,236] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,237] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,237] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-31 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,237] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,236] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,238] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,238] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,238] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,238] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,238] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-28 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,238] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,238] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,239] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,239] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,240] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,239] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-25 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,239] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,240] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,240] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,241] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,241] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,241] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-8 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,241] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,242] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,242] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,242] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-38 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,243] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,241] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,242] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,243] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,243] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,243] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,244] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,243] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,243] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,244] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,244] INFO [GroupCoordinator 6]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,244] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,244] INFO [GroupMetadataManager brokerId=6] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,245] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,245] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,245] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,245] INFO [GroupMetadataManager brokerId=6] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,245] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,244] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,246] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,246] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,246] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,247] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,247] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,247] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,248] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,248] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,248] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,249] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,249] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,249] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,248] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,250] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,250] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,247] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,250] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,251] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,251] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,252] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,252] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,252] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,253] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,253] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,253] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,253] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,251] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,254] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,252] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,254] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,255] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,254] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,255] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,255] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,255] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,255] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,256] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,256] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,256] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,256] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,256] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,256] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,257] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,256] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,256] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,257] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,257] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,257] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,257] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,258] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,258] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,258] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,258] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,258] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,258] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,258] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,259] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,259] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,259] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,259] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,259] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,259] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,260] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,259] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,260] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,260] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,260] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,260] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,261] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,261] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,261] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,261] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,261] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,261] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,261] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,262] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,262] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,262] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,262] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,262] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,262] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,263] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,263] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,263] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,263] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,263] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,263] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,263] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,263] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,264] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,264] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,264] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,264] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,264] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,264] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,265] INFO [DynamicConfigPublisher broker id=5] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 22:15:42,265] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,265] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,266] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,266] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,266] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,267] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,267] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,266] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,267] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,268] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,268] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,269] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,269] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,270] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,270] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,270] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,271] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,271] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,271] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,272] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,271] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,272] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,272] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,272] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,273] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,274] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,274] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,274] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,275] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,275] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,276] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,276] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,276] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,276] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,277] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,277] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,278] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,278] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,279] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,280] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,280] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,280] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,280] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,280] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,281] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,281] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,281] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,282] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,282] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:15:42,283] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,283] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,284] INFO [DynamicConfigPublisher broker id=6] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 22:15:42,284] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:15:42,401] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,401] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,401] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,401] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,402] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,402] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,402] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,402] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,403] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,403] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,403] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,403] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,404] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,404] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,404] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,404] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,405] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,405] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,405] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,405] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,406] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,406] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,406] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,407] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,407] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,407] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,407] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,407] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,408] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,408] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,408] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,408] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,408] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,408] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,408] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,409] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,409] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,409] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,409] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,409] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,409] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,409] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,410] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,410] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,411] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,411] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,411] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,412] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,412] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,412] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,413] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,413] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,413] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,414] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,414] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,414] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,415] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,415] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,416] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,416] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,416] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,416] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,417] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,417] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,418] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,418] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:15:42,418] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:15:42,418] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:16:19,788] INFO [Partition __consumer_offsets-9 broker=6] Shrinking ISR from 6,5,4 to 6. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 5, endOffset: -1, lastCaughtUpTimeMs: 1747779341677) (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341677). (kafka.cluster.Partition)
[2025-05-20 22:16:19,795] INFO [Partition __consumer_offsets-42 broker=6] Shrinking ISR from 6,5,4 to 6. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 5, endOffset: -1, lastCaughtUpTimeMs: 1747779341702) (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341702). (kafka.cluster.Partition)
[2025-05-20 22:16:19,796] INFO [Partition __consumer_offsets-38 broker=6] Shrinking ISR from 6,5,4 to 6. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 5, endOffset: -1, lastCaughtUpTimeMs: 1747779341804) (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341804). (kafka.cluster.Partition)
[2025-05-20 22:16:19,797] INFO [Partition __consumer_offsets-1 broker=6] Shrinking ISR from 6,4,5 to 6. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341850) (brokerId: 5, endOffset: -1, lastCaughtUpTimeMs: 1747779341850). (kafka.cluster.Partition)
[2025-05-20 22:16:19,797] INFO [Partition __consumer_offsets-41 broker=6] Shrinking ISR from 6,4,5 to 6. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341690) (brokerId: 5, endOffset: -1, lastCaughtUpTimeMs: 1747779341690). (kafka.cluster.Partition)
[2025-05-20 22:16:19,798] INFO [Partition __consumer_offsets-20 broker=6] Shrinking ISR from 6,4,5 to 6. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341729) (brokerId: 5, endOffset: -1, lastCaughtUpTimeMs: 1747779341729). (kafka.cluster.Partition)
[2025-05-20 22:16:19,798] INFO [Partition __consumer_offsets-25 broker=6] Shrinking ISR from 6,4,5 to 6. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341774) (brokerId: 5, endOffset: -1, lastCaughtUpTimeMs: 1747779341774). (kafka.cluster.Partition)
[2025-05-20 22:16:19,799] INFO [Partition __consumer_offsets-8 broker=6] Shrinking ISR from 6,5,4 to 6. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 5, endOffset: -1, lastCaughtUpTimeMs: 1747779341790) (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341790). (kafka.cluster.Partition)
[2025-05-20 22:16:19,799] INFO [Partition __consumer_offsets-4 broker=6] Shrinking ISR from 6,4,5 to 6. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341835) (brokerId: 5, endOffset: -1, lastCaughtUpTimeMs: 1747779341835). (kafka.cluster.Partition)
[2025-05-20 22:16:19,800] INFO [Partition __consumer_offsets-15 broker=6] Shrinking ISR from 6,4,5 to 6. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341567) (brokerId: 5, endOffset: -1, lastCaughtUpTimeMs: 1747779341567). (kafka.cluster.Partition)
[2025-05-20 22:16:19,800] INFO [Partition __consumer_offsets-48 broker=6] Shrinking ISR from 6,4,5 to 6. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341642) (brokerId: 5, endOffset: -1, lastCaughtUpTimeMs: 1747779341642). (kafka.cluster.Partition)
[2025-05-20 22:16:19,801] INFO [Partition __consumer_offsets-28 broker=6] Shrinking ISR from 6,4,5 to 6. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341756) (brokerId: 5, endOffset: -1, lastCaughtUpTimeMs: 1747779341756). (kafka.cluster.Partition)
[2025-05-20 22:16:19,801] INFO [Partition __consumer_offsets-47 broker=6] Shrinking ISR from 6,5,4 to 6. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 5, endOffset: -1, lastCaughtUpTimeMs: 1747779341615) (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341615). (kafka.cluster.Partition)
[2025-05-20 22:16:19,802] INFO [Partition __consumer_offsets-14 broker=6] Shrinking ISR from 6,5,4 to 6. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 5, endOffset: -1, lastCaughtUpTimeMs: 1747779341663) (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341663). (kafka.cluster.Partition)
[2025-05-20 22:16:19,802] INFO [Partition __consumer_offsets-22 broker=6] Shrinking ISR from 6,4,5 to 6. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341715) (brokerId: 5, endOffset: -1, lastCaughtUpTimeMs: 1747779341715). (kafka.cluster.Partition)
[2025-05-20 22:16:19,803] INFO [Partition __consumer_offsets-31 broker=6] Shrinking ISR from 6,5,4 to 6. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 5, endOffset: -1, lastCaughtUpTimeMs: 1747779341743) (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341743). (kafka.cluster.Partition)
[2025-05-20 22:16:19,803] INFO [Partition __consumer_offsets-35 broker=6] Shrinking ISR from 6,5,4 to 6. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 5, endOffset: -1, lastCaughtUpTimeMs: 1747779341819) (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341819). (kafka.cluster.Partition)
[2025-05-20 22:16:19,813] INFO [Partition __consumer_offsets-21 broker=5] Shrinking ISR from 5,6,4 to 5. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 6, endOffset: -1, lastCaughtUpTimeMs: 1747779341690) (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341690). (kafka.cluster.Partition)
[2025-05-20 22:16:19,821] INFO [Partition __consumer_offsets-17 broker=5] Shrinking ISR from 5,6,4 to 5. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 6, endOffset: -1, lastCaughtUpTimeMs: 1747779341714) (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341714). (kafka.cluster.Partition)
[2025-05-20 22:16:19,822] INFO [Partition __consumer_offsets-45 broker=5] Shrinking ISR from 5,4,6 to 5. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341570) (brokerId: 6, endOffset: -1, lastCaughtUpTimeMs: 1747779341570). (kafka.cluster.Partition)
[2025-05-20 22:16:19,822] INFO [Partition __consumer_offsets-12 broker=5] Shrinking ISR from 5,4,6 to 5. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341637) (brokerId: 6, endOffset: -1, lastCaughtUpTimeMs: 1747779341637). (kafka.cluster.Partition)
[2025-05-20 22:16:19,823] INFO [Partition __consumer_offsets-24 broker=5] Shrinking ISR from 5,6,4 to 5. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 6, endOffset: -1, lastCaughtUpTimeMs: 1747779341677) (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341677). (kafka.cluster.Partition)
[2025-05-20 22:16:19,823] INFO [Partition __consumer_offsets-0 broker=5] Shrinking ISR from 5,6,4 to 5. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 6, endOffset: -1, lastCaughtUpTimeMs: 1747779341727) (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341727). (kafka.cluster.Partition)
[2025-05-20 22:16:19,824] INFO [Partition __consumer_offsets-33 broker=5] Shrinking ISR from 5,4,6 to 5. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341836) (brokerId: 6, endOffset: -1, lastCaughtUpTimeMs: 1747779341836). (kafka.cluster.Partition)
[2025-05-20 22:16:19,824] INFO [Partition __consumer_offsets-19 broker=5] Shrinking ISR from 5,6,4 to 5. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 6, endOffset: -1, lastCaughtUpTimeMs: 1747779341702) (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341702). (kafka.cluster.Partition)
[2025-05-20 22:16:19,824] INFO [Partition __consumer_offsets-32 broker=5] Shrinking ISR from 5,4,6 to 5. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341741) (brokerId: 6, endOffset: -1, lastCaughtUpTimeMs: 1747779341741). (kafka.cluster.Partition)
[2025-05-20 22:16:19,825] INFO [Partition __consumer_offsets-40 broker=5] Shrinking ISR from 5,6,4 to 5. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 6, endOffset: -1, lastCaughtUpTimeMs: 1747779341773) (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341773). (kafka.cluster.Partition)
[2025-05-20 22:16:19,826] INFO [Partition __consumer_offsets-3 broker=5] Shrinking ISR from 5,6,4 to 5. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 6, endOffset: -1, lastCaughtUpTimeMs: 1747779341805) (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341805). (kafka.cluster.Partition)
[2025-05-20 22:16:19,826] INFO [Partition __consumer_offsets-36 broker=5] Shrinking ISR from 5,4,6 to 5. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341820) (brokerId: 6, endOffset: -1, lastCaughtUpTimeMs: 1747779341820). (kafka.cluster.Partition)
[2025-05-20 22:16:19,827] INFO [Partition __consumer_offsets-43 broker=5] Shrinking ISR from 5,4,6 to 5. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341611) (brokerId: 6, endOffset: -1, lastCaughtUpTimeMs: 1747779341611). (kafka.cluster.Partition)
[2025-05-20 22:16:19,827] INFO [Partition __consumer_offsets-10 broker=5] Shrinking ISR from 5,4,6 to 5. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341664) (brokerId: 6, endOffset: -1, lastCaughtUpTimeMs: 1747779341664). (kafka.cluster.Partition)
[2025-05-20 22:16:19,828] INFO [Partition __consumer_offsets-27 broker=5] Shrinking ISR from 5,6,4 to 5. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 6, endOffset: -1, lastCaughtUpTimeMs: 1747779341755) (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341755). (kafka.cluster.Partition)
[2025-05-20 22:16:19,828] INFO [Partition __consumer_offsets-6 broker=5] Shrinking ISR from 5,4,6 to 5. Leader: (highWatermark: 0, endOffset: 0). Out of sync replicas: (brokerId: 4, endOffset: -1, lastCaughtUpTimeMs: 1747779341787) (brokerId: 6, endOffset: -1, lastCaughtUpTimeMs: 1747779341787). (kafka.cluster.Partition)
[2025-05-20 22:16:19,844] INFO [Partition __consumer_offsets-21 broker=5] ISR updated to 5  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,844] INFO [Partition __consumer_offsets-9 broker=6] ISR updated to 6  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,883] INFO [Broker id=5] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:16:19,883] INFO [Broker id=6] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:16:19,884] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-21) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:16:19,884] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-9) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:16:19,886] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:19,886] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:19,887] INFO [Broker id=6] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:16:19,887] INFO [Broker id=5] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:16:19,888] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:19,889] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:19,890] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:19,891] INFO [Partition __consumer_offsets-45 broker=5] ISR updated to 5  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,891] INFO [Partition __consumer_offsets-15 broker=6] ISR updated to 6  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,892] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:19,893] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:19,893] INFO [Partition __consumer_offsets-43 broker=5] ISR updated to 5  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,894] INFO [Partition __consumer_offsets-47 broker=6] ISR updated to 6  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,894] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:19,894] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:19,894] INFO [Partition __consumer_offsets-12 broker=5] ISR updated to 5  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,895] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:19,895] INFO [Partition __consumer_offsets-10 broker=5] ISR updated to 5  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,894] INFO [Partition __consumer_offsets-48 broker=6] ISR updated to 6  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,896] INFO [Partition __consumer_offsets-14 broker=6] ISR updated to 6  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,896] INFO [Partition __consumer_offsets-24 broker=5] ISR updated to 5  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,896] INFO [Partition __consumer_offsets-41 broker=6] ISR updated to 6  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,897] INFO [Partition __consumer_offsets-42 broker=6] ISR updated to 6  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,898] INFO [Partition __consumer_offsets-22 broker=6] ISR updated to 6  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,898] INFO [Partition __consumer_offsets-20 broker=6] ISR updated to 6  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,898] INFO [Partition __consumer_offsets-19 broker=5] ISR updated to 5  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,899] INFO [Partition __consumer_offsets-31 broker=6] ISR updated to 6  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,899] INFO [Partition __consumer_offsets-28 broker=6] ISR updated to 6  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,899] INFO [Partition __consumer_offsets-17 broker=5] ISR updated to 5  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,900] INFO [Partition __consumer_offsets-25 broker=6] ISR updated to 6  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,900] INFO [Partition __consumer_offsets-0 broker=5] ISR updated to 5  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,900] INFO [Partition __consumer_offsets-32 broker=5] ISR updated to 5  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,900] INFO [Partition __consumer_offsets-27 broker=5] ISR updated to 5  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,901] INFO [Partition __consumer_offsets-8 broker=6] ISR updated to 6  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,901] INFO [Partition __consumer_offsets-40 broker=5] ISR updated to 5  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,902] INFO [Partition __consumer_offsets-38 broker=6] ISR updated to 6  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,903] INFO [Partition __consumer_offsets-35 broker=6] ISR updated to 6  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,904] INFO [Partition __consumer_offsets-4 broker=6] ISR updated to 6  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,904] INFO [Partition __consumer_offsets-6 broker=5] ISR updated to 5  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,904] INFO [Partition __consumer_offsets-1 broker=6] ISR updated to 6  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,906] INFO [Partition __consumer_offsets-3 broker=5] ISR updated to 5  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,912] INFO [Partition __consumer_offsets-36 broker=5] ISR updated to 5  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:19,916] INFO [Partition __consumer_offsets-33 broker=5] ISR updated to 5  and version updated to 1 (kafka.cluster.Partition)
[2025-05-20 22:16:20,069] INFO [Broker id=5] Transitioning 15 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:16:20,069] INFO [Broker id=6] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:16:20,070] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-45, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-0, __consumer_offsets-32, __consumer_offsets-27, __consumer_offsets-40, __consumer_offsets-6, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-33) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:16:20,071] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,072] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,072] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-47, __consumer_offsets-48, __consumer_offsets-14, __consumer_offsets-41, __consumer_offsets-42, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-31, __consumer_offsets-28, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-38, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-1) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:16:20,073] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,073] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,074] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,075] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,075] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,076] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,076] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,076] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,076] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,077] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,077] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,078] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,078] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,078] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,078] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,079] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,080] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,080] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,080] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,081] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,081] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,082] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,082] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,082] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,083] INFO [Broker id=5] Transitioning 16 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:16:20,083] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,083] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,083] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,084] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,084] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,084] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,084] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,084] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,085] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,085] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,085] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,085] INFO [Broker id=6] Skipped the become-leader state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 0. Current high watermark 0, ISR [6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:16:20,085] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,085] INFO [Broker id=6] Transitioning 15 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:16:20,086] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,086] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,086] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,086] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,087] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,087] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,087] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,087] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,087] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,088] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,088] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,088] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,089] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,089] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=0, isr=[6], partitionEpoch=1, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,089] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,089] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,089] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,090] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,090] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,090] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,090] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,090] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,091] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,091] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,091] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,091] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,091] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,092] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,091] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,092] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,092] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,092] INFO [Broker id=6] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=0, isr=[5], partitionEpoch=1, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 0. (state.change.logger)
[2025-05-20 22:16:20,092] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,092] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,093] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,093] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,093] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,093] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,094] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,094] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,094] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,095] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,095] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,095] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,094] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,095] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,095] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,096] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,096] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,096] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,096] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,096] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,096] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,097] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,097] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,097] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,097] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,097] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,097] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,098] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,098] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,098] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,098] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,099] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,099] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,099] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,099] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,099] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,100] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,099] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,100] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,100] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,100] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,100] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,100] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,100] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,101] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,101] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,101] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,101] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,101] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,101] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,102] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,102] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,102] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,102] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,102] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,102] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,103] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,103] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,103] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,103] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,104] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,104] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,104] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,104] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,104] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,105] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,104] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,105] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,106] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,106] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,106] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,106] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,106] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,107] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,107] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,107] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,107] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,108] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,108] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,108] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,108] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[0] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:20,108] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,109] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:20,109] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[0]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,372] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 22:16:22,377] INFO [BrokerServer id=5] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
[2025-05-20 22:16:22,377] INFO [BrokerServer id=5] shutting down (kafka.server.BrokerServer)
[2025-05-20 22:16:22,379] INFO [BrokerLifecycleManager id=5] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:16:22,386] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 22:16:22,389] INFO [BrokerServer id=6] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
[2025-05-20 22:16:22,390] INFO [BrokerServer id=6] shutting down (kafka.server.BrokerServer)
[2025-05-20 22:16:22,394] INFO [BrokerLifecycleManager id=6] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:16:22,428] INFO [BrokerLifecycleManager id=5] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:16:22,428] INFO [BrokerLifecycleManager id=6] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:16:22,514] INFO [Broker id=5] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:16:22,514] INFO [Broker id=6] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:16:22,514] INFO [Broker id=5] Follower __consumer_offsets-13 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,515] INFO [Broker id=6] Follower __consumer_offsets-13 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,515] INFO [Broker id=5] Follower __consumer_offsets-46 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,515] INFO [Broker id=6] Follower __consumer_offsets-46 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,516] INFO [Broker id=5] Follower __consumer_offsets-9 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,516] INFO [Broker id=6] Follower __consumer_offsets-9 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,516] INFO [Broker id=6] Follower __consumer_offsets-42 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,516] INFO [Broker id=5] Follower __consumer_offsets-42 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,517] INFO [Broker id=6] Follower __consumer_offsets-21 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,517] INFO [Broker id=5] Follower __consumer_offsets-21 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,517] INFO [Broker id=6] Follower __consumer_offsets-17 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,517] INFO [Broker id=5] Follower __consumer_offsets-17 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,518] INFO [Broker id=6] Follower __consumer_offsets-30 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,518] INFO [Broker id=5] Follower __consumer_offsets-30 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,518] INFO [Broker id=6] Follower __consumer_offsets-26 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,518] INFO [Broker id=6] Follower __consumer_offsets-5 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,519] INFO [Broker id=5] Follower __consumer_offsets-26 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,519] INFO [Broker id=6] Follower __consumer_offsets-38 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,520] INFO [Broker id=5] Follower __consumer_offsets-5 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,520] INFO [Broker id=6] Follower __consumer_offsets-1 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,521] INFO [Broker id=5] Follower __consumer_offsets-38 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,521] INFO [Broker id=6] Follower __consumer_offsets-34 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,522] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,522] INFO [BrokerLifecycleManager id=5] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:16:22,523] INFO [Broker id=6] Follower __consumer_offsets-16 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,523] INFO [Broker id=5] Follower __consumer_offsets-34 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,523] INFO [BrokerLifecycleManager id=5] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,523] INFO [BrokerLifecycleManager id=6] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:16:22,524] INFO [Broker id=6] Follower _schemas-0 starts at leader epoch 2 from offset 2 with partition epoch 3 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,524] INFO [Broker id=5] Follower __consumer_offsets-16 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,524] INFO [BrokerLifecycleManager id=6] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,525] INFO [BrokerLifecycleManager id=5] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:16:22,525] INFO [Broker id=6] Follower __consumer_offsets-45 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,525] INFO [broker-5-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,526] INFO [BrokerLifecycleManager id=6] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:16:22,527] INFO [Broker id=6] Follower __consumer_offsets-12 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,525] INFO [Broker id=5] Follower _schemas-0 starts at leader epoch 2 from offset 2 with partition epoch 3 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,528] INFO [Broker id=5] Follower __consumer_offsets-45 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,528] INFO [Broker id=6] Follower __consumer_offsets-41 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,529] INFO [broker-6-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,531] INFO [Broker id=5] Follower __consumer_offsets-12 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,531] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,531] INFO [Broker id=6] Follower __consumer_offsets-24 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,533] INFO [broker-5-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,534] INFO [broker-6-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,529] INFO [SocketServer listenerType=BROKER, nodeId=6] Stopping socket server request processors (kafka.network.SocketServer)
[2025-05-20 22:16:22,534] INFO [Broker id=6] Follower __consumer_offsets-20 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,528] INFO [broker-5-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,533] INFO [broker-6-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,538] INFO [Broker id=6] Follower __consumer_offsets-49 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,538] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:16:22,536] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Node 4 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:16:22,533] INFO [Broker id=5] Follower __consumer_offsets-24 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,539] INFO [Broker id=6] Follower __consumer_offsets-0 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,540] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,540] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Cancelled in-flight FETCH request with correlation id 86 due to node 4 being disconnected (elapsed time since creation: 341ms, elapsed time since send: 341ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:16:22,541] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Client requested connection close from node 4 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:16:22,542] INFO [Broker id=5] Follower __consumer_offsets-49 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,531] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Node 4 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:16:22,528] INFO [SocketServer listenerType=BROKER, nodeId=5] Stopping socket server request processors (kafka.network.SocketServer)
[2025-05-20 22:16:22,543] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Cancelled in-flight FETCH request with correlation id 85 due to node 4 being disconnected (elapsed time since creation: 336ms, elapsed time since send: 336ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:16:22,543] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:16:22,543] INFO [Broker id=5] Follower __consumer_offsets-0 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,544] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Client requested connection close from node 4 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:16:22,544] INFO [Broker id=5] Follower __consumer_offsets-29 starts at leader epoch 2 from offset 2 with partition epoch 3 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,543] INFO [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Error sending fetch request (sessionId=365021977, epoch=86) to node 4: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 4 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 22:16:22,544] INFO [Broker id=6] Follower __consumer_offsets-29 starts at leader epoch 2 from offset 2 with partition epoch 3 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,550] INFO [Broker id=6] Follower __consumer_offsets-25 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,551] INFO [SocketServer listenerType=BROKER, nodeId=6] Stopped socket server request processors (kafka.network.SocketServer)
[2025-05-20 22:16:22,551] INFO [Broker id=6] Follower __consumer_offsets-8 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,552] INFO [Broker id=6] Follower __consumer_offsets-37 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,552] INFO [Broker id=6] Follower __consumer_offsets-4 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,547] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Error sending fetch request (sessionId=182131660, epoch=85) to node 4: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 4 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 22:16:22,547] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,553] INFO [SocketServer listenerType=BROKER, nodeId=5] Stopped socket server request processors (kafka.network.SocketServer)
[2025-05-20 22:16:22,553] INFO [Broker id=5] Follower __consumer_offsets-8 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,553] INFO [Broker id=6] Follower __consumer_offsets-33 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,553] INFO [Broker id=5] Follower __consumer_offsets-37 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,553] INFO [Broker id=6] Follower __consumer_offsets-15 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,554] INFO [Broker id=6] Follower __consumer_offsets-48 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,554] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,554] INFO [Broker id=6] Follower __consumer_offsets-11 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,554] INFO [Broker id=5] Follower __consumer_offsets-33 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,554] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,554] WARN [ReplicaFetcher replicaId=6, leaderId=4, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=6, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=365021977, epoch=86), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 4 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 22:16:22,555] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,554] INFO [Broker id=6] Follower __consumer_offsets-44 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,555] INFO [Broker id=5] Follower __consumer_offsets-11 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,555] INFO [Broker id=6] Follower __consumer_offsets-23 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,556] INFO [Broker id=5] Follower __consumer_offsets-44 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,556] INFO [Broker id=6] Follower __consumer_offsets-19 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,556] INFO [Broker id=5] Follower __consumer_offsets-23 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,556] INFO [Broker id=6] Follower __consumer_offsets-32 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,556] INFO [Broker id=5] Follower __consumer_offsets-19 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,557] INFO [Broker id=6] Follower __consumer_offsets-28 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,557] INFO [Broker id=5] Follower __consumer_offsets-32 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,557] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,557] INFO [Broker id=6] Follower __consumer_offsets-7 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,557] INFO [Broker id=5] Follower __consumer_offsets-7 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,557] INFO [Broker id=6] Follower __consumer_offsets-40 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,558] INFO [Broker id=5] Follower __consumer_offsets-40 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,558] INFO [Broker id=6] Follower __consumer_offsets-3 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,558] INFO [Broker id=5] Follower __consumer_offsets-3 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,558] INFO [Broker id=6] Follower __consumer_offsets-36 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,559] WARN [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=5, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=182131660, epoch=85), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 4 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 22:16:22,559] INFO [Broker id=6] Follower __consumer_offsets-47 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,560] INFO [Broker id=6] Follower __consumer_offsets-14 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,559] INFO [Broker id=5] Follower __consumer_offsets-36 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,560] INFO [Broker id=6] Follower __consumer_offsets-43 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,560] INFO [Broker id=5] Follower __consumer_offsets-47 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,560] INFO [Broker id=6] Follower __consumer_offsets-10 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,561] INFO [Broker id=5] Follower __consumer_offsets-14 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,561] INFO [Broker id=6] Follower __consumer_offsets-22 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,561] INFO [Broker id=5] Follower __consumer_offsets-43 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,561] INFO [Broker id=6] Follower __consumer_offsets-18 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,561] INFO [Broker id=5] Follower __consumer_offsets-10 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,561] INFO [Broker id=6] Follower __consumer_offsets-31 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,562] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,562] INFO [Broker id=6] Follower __consumer_offsets-27 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,562] INFO [Broker id=5] Follower __consumer_offsets-18 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,562] INFO [Broker id=6] Follower __consumer_offsets-39 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,562] INFO [Broker id=6] Follower __consumer_offsets-6 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,562] INFO [Broker id=5] Follower __consumer_offsets-31 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,563] INFO [Broker id=6] Follower __consumer_offsets-35 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,563] INFO [Broker id=5] Follower __consumer_offsets-27 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,563] INFO [Broker id=6] Follower __consumer_offsets-2 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,563] INFO [Broker id=5] Follower __consumer_offsets-39 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,563] INFO [Broker id=5] Follower __consumer_offsets-6 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,564] INFO [Broker id=5] Follower __consumer_offsets-35 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:16:22,564] INFO [Broker id=5] Follower __consumer_offsets-2 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:16:22,577] INFO [ReplicaFetcherManager on broker 6] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:16:22,578] INFO [ReplicaAlterLogDirsManager on broker 6] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:16:22,579] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:16:22,580] INFO [ReplicaAlterLogDirsManager on broker 5] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:16:22,580] INFO [Broker id=6] Stopped fetchers as part of controlled shutdown for 51 partitions (state.change.logger)
[2025-05-20 22:16:22,581] INFO [ReplicaFetcherThread-0-4]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:16:22,582] INFO [ReplicaFetcherThread-0-4]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:16:22,582] INFO [ReplicaFetcherThread-0-4]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:16:22,583] INFO [Broker id=5] Stopped fetchers as part of controlled shutdown for 51 partitions (state.change.logger)
[2025-05-20 22:16:22,583] INFO [ReplicaFetcherThread-0-4]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:16:22,584] INFO [ReplicaFetcherThread-0-5]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:16:22,584] INFO [ReplicaFetcherThread-0-4]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:16:22,585] INFO [ReplicaFetcherThread-0-5]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:16:22,584] INFO [ReplicaFetcherThread-0-4]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:16:22,585] INFO [ReplicaFetcherThread-0-5]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:16:22,587] INFO [ReplicaFetcherThread-0-6]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:16:22,587] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,588] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,588] INFO [ReplicaFetcherThread-0-6]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:16:22,588] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,588] INFO [ReplicaFetcherThread-0-6]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:16:22,588] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,588] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,589] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,590] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,590] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,591] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,591] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,591] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,592] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,591] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,592] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,592] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,592] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,592] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,592] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,592] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,592] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,593] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,593] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,593] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,593] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,593] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,594] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,594] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,594] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,595] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,595] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,595] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,595] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,595] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,595] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,595] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,596] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,596] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,596] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,596] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,596] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,597] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,596] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,597] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,597] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,597] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,597] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,597] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,598] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,598] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,598] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,598] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,598] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,599] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,599] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,599] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,599] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,599] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,598] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,599] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,599] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,601] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,600] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,601] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,601] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,601] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,601] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,601] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,602] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,602] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,603] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,603] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,602] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,603] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,603] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,603] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,604] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,604] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,604] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,604] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,604] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,604] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,605] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,605] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,605] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,605] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,605] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,605] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,605] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,605] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,606] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,606] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,606] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,606] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,606] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,607] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,607] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,607] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,607] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,607] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,607] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,608] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,608] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,608] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,608] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,608] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,608] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,609] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,609] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,609] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,609] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,609] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,610] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,610] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,610] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,610] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,610] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,610] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,610] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,611] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,611] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,611] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,611] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,611] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,611] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,612] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,612] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,612] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,612] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,612] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,612] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,613] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,613] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,613] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,613] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,613] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,613] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,614] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,614] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,614] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,614] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,615] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,615] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,614] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,615] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,615] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,615] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,616] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,616] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,616] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,616] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,616] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,617] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,616] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,617] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,617] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,617] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,617] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,617] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,617] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,618] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,618] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,618] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,618] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,618] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,619] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,619] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,619] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,620] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,620] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,620] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,620] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,620] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,620] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,621] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,621] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,620] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,621] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,621] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,621] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,622] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,622] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,621] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,622] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,622] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,622] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,622] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,622] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,622] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,623] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,623] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,623] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,623] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,623] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,623] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,624] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,624] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,624] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,624] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,624] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,624] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,624] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,625] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,625] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,625] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,625] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,625] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,625] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,626] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,626] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,626] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,626] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,626] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,627] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,627] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,627] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,627] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,627] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,627] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,627] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,628] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,628] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,628] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,628] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,628] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,629] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,629] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,629] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,629] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,629] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,629] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,630] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,630] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,630] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,630] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,630] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,629] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,630] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,631] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,631] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,631] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,631] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,631] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,631] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,632] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,632] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,632] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,632] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,632] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,632] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,633] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,633] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,633] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,633] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,633] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,633] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,634] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,634] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,634] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,634] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,634] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,635] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,634] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,635] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,635] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,635] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,636] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,635] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,636] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,636] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,636] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,636] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,637] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,636] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,636] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,637] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,637] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,637] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,638] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,638] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,637] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,637] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,638] INFO [GroupCoordinator 6]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,638] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,639] INFO [GroupMetadataManager brokerId=6] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,638] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,638] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,639] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,639] INFO [GroupMetadataManager brokerId=6] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,639] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,639] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,640] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,640] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,640] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,641] INFO [data-plane Kafka Request Handler on Broker 6], shutting down (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 22:16:22,641] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,641] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,641] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,642] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,642] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,642] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,643] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:16:22,643] INFO [data-plane Kafka Request Handler on Broker 6], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 22:16:22,644] INFO [ExpirationReaper-6-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,644] INFO [data-plane Kafka Request Handler on Broker 5], shutting down (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 22:16:22,645] INFO [ExpirationReaper-6-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,646] INFO [data-plane Kafka Request Handler on Broker 5], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 22:16:22,646] INFO [ExpirationReaper-5-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,648] INFO [ExpirationReaper-5-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,648] INFO [ExpirationReaper-5-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,648] INFO [ExpirationReaper-6-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,650] INFO [KafkaApi-5] Shutdown complete. (kafka.server.KafkaApis)
[2025-05-20 22:16:22,650] INFO [KafkaApi-6] Shutdown complete. (kafka.server.KafkaApis)
[2025-05-20 22:16:22,654] INFO [TransactionCoordinator id=5] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:16:22,655] INFO [Transaction State Manager 5]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2025-05-20 22:16:22,655] INFO [TransactionCoordinator id=6] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:16:22,656] INFO [TxnMarkerSenderThread-5]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:16:22,656] INFO [TxnMarkerSenderThread-5]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:16:22,656] INFO [TxnMarkerSenderThread-5]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:16:22,657] INFO [Transaction State Manager 6]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2025-05-20 22:16:22,657] INFO [TxnMarkerSenderThread-6]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:16:22,657] INFO [TxnMarkerSenderThread-6]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:16:22,657] INFO [TxnMarkerSenderThread-6]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:16:22,659] INFO [TransactionCoordinator id=5] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:16:22,660] INFO [GroupCoordinator 5]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,661] INFO [TransactionCoordinator id=6] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:16:22,661] INFO [ExpirationReaper-5-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,661] INFO [ExpirationReaper-5-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,662] INFO [GroupCoordinator 6]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,662] INFO [ExpirationReaper-5-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,663] INFO [ExpirationReaper-5-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,663] INFO [ExpirationReaper-6-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,664] INFO [ExpirationReaper-6-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,664] INFO [ExpirationReaper-5-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,664] INFO [ExpirationReaper-6-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,664] INFO [ExpirationReaper-5-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,665] INFO [ExpirationReaper-6-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,665] INFO [ExpirationReaper-6-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,665] INFO [GroupCoordinator 5]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,665] INFO [ExpirationReaper-6-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,666] INFO [AssignmentsManager id=5]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,667] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,667] INFO [GroupCoordinator 6]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:16:22,667] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,667] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,668] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:16:22,668] INFO [AssignmentsManager id=6]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,668] INFO [AssignmentsManager id=5]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,669] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,669] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,669] INFO [ReplicaManager broker=5] Shutting down (kafka.server.ReplicaManager)
[2025-05-20 22:16:22,669] INFO [broker-6-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,670] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:16:22,671] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:16:22,671] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:16:22,671] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:16:22,672] INFO [AssignmentsManager id=6]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,672] INFO [ReplicaFetcherManager on broker 5] shutting down (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:16:22,673] INFO [ReplicaManager broker=6] Shutting down (kafka.server.ReplicaManager)
[2025-05-20 22:16:22,673] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:16:22,674] INFO [ReplicaFetcherManager on broker 5] shutdown completed (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:16:22,674] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:16:22,674] INFO [ReplicaAlterLogDirsManager on broker 5] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:16:22,674] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:16:22,675] INFO [ReplicaAlterLogDirsManager on broker 5] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:16:22,675] INFO [ReplicaFetcherManager on broker 6] shutting down (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:16:22,676] INFO [ExpirationReaper-5-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,676] INFO [ExpirationReaper-5-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,676] INFO [ExpirationReaper-5-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,677] INFO [ReplicaFetcherManager on broker 6] shutdown completed (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:16:22,678] INFO [ReplicaAlterLogDirsManager on broker 6] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:16:22,678] INFO [ExpirationReaper-5-RemoteFetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,678] INFO [ReplicaAlterLogDirsManager on broker 6] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:16:22,679] INFO [ExpirationReaper-6-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,679] INFO [ExpirationReaper-5-RemoteFetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,679] INFO [ExpirationReaper-5-RemoteFetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,680] INFO [ExpirationReaper-6-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,680] INFO [ExpirationReaper-5-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,680] INFO [ExpirationReaper-6-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,681] INFO [ExpirationReaper-6-RemoteFetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,681] INFO [ExpirationReaper-5-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,681] INFO [ExpirationReaper-5-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,681] INFO [ExpirationReaper-6-RemoteFetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,681] INFO [ExpirationReaper-6-RemoteFetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,682] INFO [ExpirationReaper-5-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,682] INFO [ExpirationReaper-6-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,683] INFO [ExpirationReaper-5-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,683] INFO [ExpirationReaper-6-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,683] INFO [ExpirationReaper-5-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,683] INFO [ExpirationReaper-6-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,684] INFO [ExpirationReaper-5-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,684] INFO [ExpirationReaper-6-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,684] INFO [ExpirationReaper-5-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,684] INFO [ExpirationReaper-5-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,685] INFO [ExpirationReaper-6-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,685] INFO [ExpirationReaper-6-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,686] INFO [ExpirationReaper-6-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,686] INFO [ExpirationReaper-6-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,686] INFO [ExpirationReaper-6-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:16:22,694] INFO [AddPartitionsToTxnSenderThread-5]: Shutting down (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:16:22,694] INFO [AddPartitionsToTxnSenderThread-5]: Stopped (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:16:22,695] INFO [AddPartitionsToTxnSenderThread-5]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:16:22,695] INFO [AddPartitionsToTxnSenderThread-6]: Shutting down (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:16:22,696] INFO [ReplicaManager broker=5] Shut down completely (kafka.server.ReplicaManager)
[2025-05-20 22:16:22,696] INFO [AddPartitionsToTxnSenderThread-6]: Stopped (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:16:22,696] INFO [AddPartitionsToTxnSenderThread-6]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:16:22,696] INFO [broker-5-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,697] INFO [broker-5-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,697] INFO [broker-5-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,697] INFO [ReplicaManager broker=6] Shut down completely (kafka.server.ReplicaManager)
[2025-05-20 22:16:22,698] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:16:22,698] INFO [broker-6-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,698] INFO [broker-6-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,699] INFO [broker-5-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,698] INFO [broker-6-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,699] INFO [broker-5-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,699] INFO [broker-5-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,700] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:16:22,700] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:16:22,700] INFO [broker-6-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,701] INFO Shutting down. (kafka.log.LogManager)
[2025-05-20 22:16:22,701] INFO [broker-6-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,701] INFO [broker-6-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:16:22,702] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:16:22,703] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
[2025-05-20 22:16:22,703] INFO Shutting down. (kafka.log.LogManager)
[2025-05-20 22:16:22,703] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:16:22,704] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:16:22,704] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
[2025-05-20 22:16:22,704] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:16:22,705] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:16:22,705] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:16:22,705] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:16:22,716] INFO [ProducerStateManager partition=__consumer_offsets-29] Wrote producer snapshot at offset 2 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:16:22,717] INFO [ProducerStateManager partition=__consumer_offsets-29] Wrote producer snapshot at offset 2 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:16:22,820] INFO [ProducerStateManager partition=_schemas-0] Wrote producer snapshot at offset 2 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:16:22,820] INFO [ProducerStateManager partition=_schemas-0] Wrote producer snapshot at offset 2 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:16:22,918] INFO Shutdown complete. (kafka.log.LogManager)
[2025-05-20 22:16:22,920] INFO [broker-6-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,921] INFO [broker-6-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,922] INFO [broker-6-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,922] INFO [broker-6-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,922] INFO Shutdown complete. (kafka.log.LogManager)
[2025-05-20 22:16:22,922] INFO [broker-6-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,922] INFO [broker-6-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,923] INFO [broker-5-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,923] INFO [broker-6-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,924] INFO [broker-6-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,924] INFO [broker-6-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,924] INFO [broker-5-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,924] INFO [broker-5-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,925] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,925] INFO [broker-5-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,925] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,925] INFO [broker-5-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,926] INFO [broker-5-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,927] INFO [broker-6-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,928] INFO [broker-5-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,929] INFO [broker-5-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,929] INFO [broker-5-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,930] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,931] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,931] INFO [SocketServer listenerType=BROKER, nodeId=6] Shutting down socket server (kafka.network.SocketServer)
[2025-05-20 22:16:22,931] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:16:22,933] INFO [SocketServer listenerType=BROKER, nodeId=5] Shutting down socket server (kafka.network.SocketServer)
[2025-05-20 22:16:22,949] INFO [SocketServer listenerType=BROKER, nodeId=6] Shutdown completed (kafka.network.SocketServer)
[2025-05-20 22:16:22,951] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
[2025-05-20 22:16:22,952] INFO [BrokerLifecycleManager id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,953] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:16:22,956] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:16:22,956] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:16:22,959] INFO [SocketServer listenerType=BROKER, nodeId=5] Shutdown completed (kafka.network.SocketServer)
[2025-05-20 22:16:22,959] INFO [SharedServer id=6] Stopping SharedServer (kafka.server.SharedServer)
[2025-05-20 22:16:22,961] INFO [MetadataLoader id=6] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,961] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
[2025-05-20 22:16:22,961] INFO [SnapshotGenerator id=6] close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,962] INFO [BrokerLifecycleManager id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,963] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:16:22,964] INFO [SnapshotGenerator id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,964] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:16:22,964] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:16:22,965] INFO [MetadataLoader id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,967] INFO [SnapshotGenerator id=6] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,970] INFO [SharedServer id=5] Stopping SharedServer (kafka.server.SharedServer)
[2025-05-20 22:16:22,971] INFO [MetadataLoader id=5] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,971] INFO [SnapshotGenerator id=5] close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,972] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:16:22,972] INFO [SnapshotGenerator id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,973] INFO [MetadataLoader id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,975] INFO [SnapshotGenerator id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:16:22,975] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:16:23,001] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:16:23,001] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:16:23,002] INFO [kafka-6-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:16:23,003] INFO [RaftManager id=6] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:16:23,003] INFO [RaftManager id=6] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:16:23,004] INFO [RaftManager id=6] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:16:23,004] INFO [kafka-6-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:16:23,004] INFO [kafka-6-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:16:23,009] INFO [kafka-6-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:16:23,009] INFO [kafka-6-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:16:23,009] INFO [kafka-6-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:16:23,026] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 1820 with 0 producer ids in 15 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:16:23,030] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:16:23,031] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:16:23,031] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:16:23,032] INFO App info kafka.server for 6 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:16:23,033] INFO [BrokerServer id=6] shut down completed (kafka.server.BrokerServer)
[2025-05-20 22:16:23,037] INFO [BrokerServer id=6] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
[2025-05-20 22:16:23,044] INFO App info kafka.server for 6 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:16:23,145] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:16:23,145] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:16:23,147] INFO [kafka-5-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:16:23,147] INFO [RaftManager id=5] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:16:23,148] INFO [RaftManager id=5] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:16:23,148] INFO [RaftManager id=5] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:16:23,148] INFO [kafka-5-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:16:23,148] INFO [kafka-5-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:16:23,153] INFO [kafka-5-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:16:23,154] INFO [kafka-5-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:16:23,154] INFO [kafka-5-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:16:23,158] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 1821 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:16:23,161] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:16:23,162] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:16:23,162] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:16:23,163] INFO App info kafka.server for 5 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:16:23,163] INFO [BrokerServer id=5] shut down completed (kafka.server.BrokerServer)
[2025-05-20 22:16:23,164] INFO [BrokerServer id=5] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
[2025-05-20 22:16:23,164] INFO App info kafka.server for 5 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:21:35,406] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 22:21:35,957] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-2:19092,PLAINTEXT_HOST://localhost:39092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 5
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 5
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 22:21:35,997] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 22:21:36,007] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:41,417] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 22:21:41,529] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 22:21:41,533] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:41,698] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:41,713] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:41,738] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 22:21:41,752] INFO [BrokerServer id=5] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-20 22:21:41,753] INFO [SharedServer id=5] Starting SharedServer (kafka.server.SharedServer)
[2025-05-20 22:21:41,756] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:41,918] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:41,920] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:41,925] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:41,937] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-20 22:21:41,977] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:21:41,981] INFO [RaftManager id=5] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:21:41,984] INFO [RaftManager id=5] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:21:42,029] INFO [RaftManager id=5] Completed transition to Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1410) from null (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:42,032] INFO [kafka-5-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:21:42,050] INFO [kafka-5-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:21:42,094] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,097] INFO [BrokerServer id=5] Starting broker (kafka.server.BrokerServer)
[2025-05-20 22:21:42,127] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:21:42,152] INFO [broker-5-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:21:42,157] INFO [broker-5-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:21:42,166] INFO [broker-5-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:21:42,169] INFO [RaftManager id=5] Registered the listener org.apache.kafka.image.loader.MetadataLoader@198480695 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:21:42,198] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,206] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:21:42,284] INFO [BrokerServer id=5] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 22:21:42,300] INFO [BrokerServer id=5] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 22:21:42,322] INFO [broker-5-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:42,323] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,329] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:21:42,427] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,503] INFO [RaftManager id=5] Completed transition to Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=0, voters=[1, 2, 3], electionTimeoutMs=1410) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:42,508] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:21:42,513] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:21:42,529] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,594] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:21:42,621] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:21:42,631] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,748] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,849] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,950] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:42,956] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 22:21:42,961] INFO [RaftManager id=5] Completed transition to Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=9, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:42,978] INFO [SocketServer listenerType=BROKER, nodeId=5] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-20 22:21:42,980] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 22:21:42,990] INFO [SocketServer listenerType=BROKER, nodeId=5] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-20 22:21:43,008] INFO [broker-5-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:43,023] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:43,036] INFO [ExpirationReaper-5-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,080] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,086] INFO [ExpirationReaper-5-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,107] INFO [ExpirationReaper-5-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,111] INFO [ExpirationReaper-5-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,120] INFO [RaftManager id=5] Completed transition to Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=10, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:43,121] INFO [ExpirationReaper-5-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,133] INFO [ExpirationReaper-5-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,141] INFO [ExpirationReaper-5-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,183] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,185] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-05-20 22:21:43,195] INFO [broker-5-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:43,196] INFO [BrokerLifecycleManager id=5] Incarnation xr36lZYISCyszx17VGssbQ of broker 5 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:43,215] INFO [ExpirationReaper-5-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:21:43,286] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,295] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,295] INFO [BrokerServer id=5] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 22:21:43,297] INFO [BrokerServer id=5] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 22:21:43,298] INFO [BrokerServer id=5] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 22:21:43,396] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,497] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,597] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,698] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,799] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:43,838] INFO [RaftManager id=5] Completed transition to Unattached(epoch=12, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=11, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:43,900] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,001] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,102] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,126] INFO [RaftManager id=5] Completed transition to Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from Unattached(epoch=12, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:44,182] INFO [RaftManager id=5] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=13, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=13, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:21:44,202] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,203] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,220] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:21:44,221] INFO [broker-5-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,228] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,235] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,247] INFO [broker-5-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,272] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,304] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,381] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:21:44,382] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,405] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,433] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:21:44,498] INFO [BrokerLifecycleManager id=5] Successfully registered broker 5 with broker epoch 1825 (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:44,500] INFO [RaftManager id=5] High watermark set to Optional[LogOffsetMetadata(offset=1825, metadata=Optional.empty)] for the first time for epoch 13 (org.apache.kafka.raft.FollowerState)
[2025-05-20 22:21:44,506] INFO [MetadataLoader id=5] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1825 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,522] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:44,533] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 1825 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,642] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:44,680] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 1824, but the high water mark is 1829 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,684] INFO [MetadataLoader id=5] initializeNewPublishers: The loader is still catching up because we have loaded up to offset 1824, but the high water mark is 1829 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,780] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 1828, but the high water mark is 1830 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,783] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 1830 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,794] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 1829 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,807] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing MetadataVersionPublisher(id=5) with a snapshot at offset 1829 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,820] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 1829 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:44,821] INFO [BrokerMetadataPublisher id=5] Publishing initial metadata at offset OffsetAndEpoch(offset=1829, epoch=13) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-20 22:21:44,831] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:21:44,864] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:44,876] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
[2025-05-20 22:21:44,891] INFO Loaded 0 logs in 50ms (kafka.log.LogManager)
[2025-05-20 22:21:44,894] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-20 22:21:44,895] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-20 22:21:44,904] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-20 22:21:45,068] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:21:45,080] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:21:45,082] INFO [AddPartitionsToTxnSenderThread-5]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:21:45,083] INFO [GroupCoordinator 5]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:45,089] INFO [GroupCoordinator 5]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:45,092] INFO [TransactionCoordinator id=5] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:21:45,107] INFO [TxnMarkerSenderThread-5]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:21:45,107] INFO [TransactionCoordinator id=5] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:21:45,118] INFO [Broker id=5] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:21:45,121] INFO [Broker id=5] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,138] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,145] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,148] INFO [Partition __consumer_offsets-13 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2025-05-20 22:21:45,153] INFO [Partition __consumer_offsets-13 broker=5] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,163] INFO [Broker id=5] Follower __consumer_offsets-13 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,166] INFO [Broker id=5] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,183] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,187] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,187] INFO [Partition __consumer_offsets-46 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2025-05-20 22:21:45,193] INFO [Partition __consumer_offsets-46 broker=5] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,193] INFO [Broker id=5] Follower __consumer_offsets-46 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,194] INFO [Broker id=5] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,206] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,210] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,213] INFO [Partition __consumer_offsets-9 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2025-05-20 22:21:45,215] INFO [Partition __consumer_offsets-9 broker=5] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,217] INFO [Broker id=5] Follower __consumer_offsets-9 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,222] INFO [Broker id=5] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,229] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,236] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,239] INFO [Partition __consumer_offsets-42 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2025-05-20 22:21:45,242] INFO [Partition __consumer_offsets-42 broker=5] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,243] INFO [Broker id=5] Follower __consumer_offsets-42 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,245] INFO [Broker id=5] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,256] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,262] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,262] INFO [Partition __consumer_offsets-21 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2025-05-20 22:21:45,263] INFO [Partition __consumer_offsets-21 broker=5] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,264] INFO [Broker id=5] Follower __consumer_offsets-21 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,264] INFO [Broker id=5] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,265] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:45,279] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,283] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,293] INFO [Partition __consumer_offsets-17 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2025-05-20 22:21:45,296] INFO [Partition __consumer_offsets-17 broker=5] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,297] INFO [Broker id=5] Follower __consumer_offsets-17 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,298] INFO [Broker id=5] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,312] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,319] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,319] INFO [Partition __consumer_offsets-30 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2025-05-20 22:21:45,319] INFO [Partition __consumer_offsets-30 broker=5] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,320] INFO [Broker id=5] Follower __consumer_offsets-30 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,322] INFO [Broker id=5] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,330] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,333] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,334] INFO [Partition __consumer_offsets-26 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2025-05-20 22:21:45,335] INFO [Partition __consumer_offsets-26 broker=5] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,336] INFO [Broker id=5] Follower __consumer_offsets-26 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,337] INFO [Broker id=5] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,351] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,353] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,354] INFO [Partition __consumer_offsets-5 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2025-05-20 22:21:45,355] INFO [Partition __consumer_offsets-5 broker=5] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,355] INFO [Broker id=5] Follower __consumer_offsets-5 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,356] INFO [Broker id=5] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,367] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,368] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,369] INFO [Partition __consumer_offsets-38 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2025-05-20 22:21:45,370] INFO [Partition __consumer_offsets-38 broker=5] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,372] INFO [Broker id=5] Follower __consumer_offsets-38 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,375] INFO [Broker id=5] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,389] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,403] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,404] INFO [Partition __consumer_offsets-1 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2025-05-20 22:21:45,404] INFO [Partition __consumer_offsets-1 broker=5] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,405] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,407] INFO [Broker id=5] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,418] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,419] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,426] INFO [Partition __consumer_offsets-34 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2025-05-20 22:21:45,431] INFO [Partition __consumer_offsets-34 broker=5] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,432] INFO [Broker id=5] Follower __consumer_offsets-34 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,435] INFO [Broker id=5] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,453] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,458] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,459] INFO [Partition __consumer_offsets-16 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2025-05-20 22:21:45,466] INFO [Partition __consumer_offsets-16 broker=5] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,476] INFO [Broker id=5] Follower __consumer_offsets-16 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,478] INFO [Broker id=5] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-20 22:21:45,490] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,506] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2025-05-20 22:21:45,508] INFO [Partition _schemas-0 broker=5] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,512] INFO [Partition _schemas-0 broker=5] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,518] INFO [Broker id=5] Follower _schemas-0 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,519] INFO [Broker id=5] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,532] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,534] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,535] INFO [Partition __consumer_offsets-45 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2025-05-20 22:21:45,539] INFO [Partition __consumer_offsets-45 broker=5] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,540] INFO [Broker id=5] Follower __consumer_offsets-45 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,542] INFO [Broker id=5] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,552] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,554] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,560] INFO [Partition __consumer_offsets-12 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2025-05-20 22:21:45,570] INFO [Partition __consumer_offsets-12 broker=5] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,580] INFO [Broker id=5] Follower __consumer_offsets-12 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,583] INFO [Broker id=5] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,595] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,597] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,599] INFO [Partition __consumer_offsets-41 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2025-05-20 22:21:45,601] INFO [Partition __consumer_offsets-41 broker=5] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,603] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,605] INFO [Broker id=5] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,610] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,619] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,622] INFO [Partition __consumer_offsets-24 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2025-05-20 22:21:45,625] INFO [Partition __consumer_offsets-24 broker=5] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,627] INFO [Broker id=5] Follower __consumer_offsets-24 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,628] INFO [Broker id=5] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,640] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,642] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,642] INFO [Partition __consumer_offsets-20 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2025-05-20 22:21:45,643] INFO [Partition __consumer_offsets-20 broker=5] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,645] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,645] INFO [Broker id=5] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,650] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,659] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,660] INFO [Partition __consumer_offsets-49 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2025-05-20 22:21:45,660] INFO [Partition __consumer_offsets-49 broker=5] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,663] INFO [Broker id=5] Follower __consumer_offsets-49 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,664] INFO [Broker id=5] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,674] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,676] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,679] INFO [Partition __consumer_offsets-0 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,680] INFO [Partition __consumer_offsets-0 broker=5] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,680] INFO [Broker id=5] Follower __consumer_offsets-0 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,681] INFO [Broker id=5] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,689] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,695] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,695] INFO [Partition __consumer_offsets-29 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2025-05-20 22:21:45,696] INFO [Partition __consumer_offsets-29 broker=5] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,697] INFO [Broker id=5] Follower __consumer_offsets-29 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,698] INFO [Broker id=5] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,705] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,711] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,713] INFO [Partition __consumer_offsets-25 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2025-05-20 22:21:45,714] INFO [Partition __consumer_offsets-25 broker=5] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,715] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,715] INFO [Broker id=5] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,725] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,736] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,737] INFO [Partition __consumer_offsets-8 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2025-05-20 22:21:45,739] INFO [Partition __consumer_offsets-8 broker=5] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,742] INFO [Broker id=5] Follower __consumer_offsets-8 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,744] INFO [Broker id=5] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,753] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,756] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,756] INFO [Partition __consumer_offsets-37 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2025-05-20 22:21:45,756] INFO [Partition __consumer_offsets-37 broker=5] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,757] INFO [Broker id=5] Follower __consumer_offsets-37 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,757] INFO [Broker id=5] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,771] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,774] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,777] INFO [Partition __consumer_offsets-4 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2025-05-20 22:21:45,778] INFO [Partition __consumer_offsets-4 broker=5] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,781] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,783] INFO [Broker id=5] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,795] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,797] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,798] INFO [Partition __consumer_offsets-33 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2025-05-20 22:21:45,799] INFO [Partition __consumer_offsets-33 broker=5] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,800] INFO [Broker id=5] Follower __consumer_offsets-33 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,800] INFO [Broker id=5] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,808] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,812] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,816] INFO [Partition __consumer_offsets-15 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2025-05-20 22:21:45,817] INFO [Partition __consumer_offsets-15 broker=5] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,819] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,820] INFO [Broker id=5] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,825] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,839] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,840] INFO [Partition __consumer_offsets-48 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2025-05-20 22:21:45,841] INFO [Partition __consumer_offsets-48 broker=5] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,841] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,843] INFO [Broker id=5] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,858] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,860] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,864] INFO [Partition __consumer_offsets-11 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2025-05-20 22:21:45,865] INFO [Partition __consumer_offsets-11 broker=5] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,866] INFO [Broker id=5] Follower __consumer_offsets-11 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,868] INFO [Broker id=5] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,882] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,884] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,885] INFO [Partition __consumer_offsets-44 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2025-05-20 22:21:45,886] INFO [Partition __consumer_offsets-44 broker=5] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,888] INFO [Broker id=5] Follower __consumer_offsets-44 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,889] INFO [Broker id=5] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,897] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,899] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,901] INFO [Partition __consumer_offsets-23 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2025-05-20 22:21:45,904] INFO [Partition __consumer_offsets-23 broker=5] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,905] INFO [Broker id=5] Follower __consumer_offsets-23 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:45,906] INFO [Broker id=5] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,936] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,941] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,942] INFO [Partition __consumer_offsets-19 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2025-05-20 22:21:45,943] INFO [Partition __consumer_offsets-19 broker=5] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,944] INFO [Broker id=5] Follower __consumer_offsets-19 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,946] INFO [Broker id=5] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,955] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:45,959] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:45,963] INFO [Partition __consumer_offsets-32 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2025-05-20 22:21:45,968] INFO [Partition __consumer_offsets-32 broker=5] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:45,974] INFO [Broker id=5] Follower __consumer_offsets-32 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:45,976] INFO [Broker id=5] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:45,994] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,002] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,004] INFO [Partition __consumer_offsets-28 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2025-05-20 22:21:46,007] INFO [Partition __consumer_offsets-28 broker=5] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,008] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,008] INFO [Broker id=5] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,031] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,033] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,033] INFO [Partition __consumer_offsets-7 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2025-05-20 22:21:46,034] INFO [Partition __consumer_offsets-7 broker=5] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,035] INFO [Broker id=5] Follower __consumer_offsets-7 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,035] INFO [Broker id=5] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,052] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,063] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,072] INFO [Partition __consumer_offsets-40 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2025-05-20 22:21:46,073] INFO [Partition __consumer_offsets-40 broker=5] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,074] INFO [Broker id=5] Follower __consumer_offsets-40 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,074] INFO [Broker id=5] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,085] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:46,095] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,097] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,098] INFO [Partition __consumer_offsets-3 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2025-05-20 22:21:46,098] INFO [Partition __consumer_offsets-3 broker=5] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,100] INFO [Broker id=5] Follower __consumer_offsets-3 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,101] INFO [Broker id=5] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,115] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,119] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,119] INFO [Partition __consumer_offsets-36 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2025-05-20 22:21:46,121] INFO [Partition __consumer_offsets-36 broker=5] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,123] INFO [Broker id=5] Follower __consumer_offsets-36 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,126] INFO [Broker id=5] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,132] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,134] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,138] INFO [Partition __consumer_offsets-47 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2025-05-20 22:21:46,139] INFO [Partition __consumer_offsets-47 broker=5] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,142] INFO [Broker id=5] Follower __consumer_offsets-47 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,143] INFO [Broker id=5] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,149] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,153] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,154] INFO [Partition __consumer_offsets-14 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2025-05-20 22:21:46,154] INFO [Partition __consumer_offsets-14 broker=5] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,154] INFO [Broker id=5] Follower __consumer_offsets-14 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,155] INFO [Broker id=5] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,166] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,169] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,170] INFO [Partition __consumer_offsets-43 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2025-05-20 22:21:46,170] INFO [Partition __consumer_offsets-43 broker=5] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,175] INFO [Broker id=5] Follower __consumer_offsets-43 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,175] INFO [Broker id=5] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,185] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,188] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,190] INFO [Partition __consumer_offsets-10 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2025-05-20 22:21:46,192] INFO [Partition __consumer_offsets-10 broker=5] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,194] INFO [Broker id=5] Follower __consumer_offsets-10 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,195] INFO [Broker id=5] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,213] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,214] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,215] INFO [Partition __consumer_offsets-22 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2025-05-20 22:21:46,215] INFO [Partition __consumer_offsets-22 broker=5] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,216] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,217] INFO [Broker id=5] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,222] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,229] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,230] INFO [Partition __consumer_offsets-18 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2025-05-20 22:21:46,231] INFO [Partition __consumer_offsets-18 broker=5] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,232] INFO [Broker id=5] Follower __consumer_offsets-18 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,233] INFO [Broker id=5] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,245] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,254] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,257] INFO [Partition __consumer_offsets-31 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2025-05-20 22:21:46,258] INFO [Partition __consumer_offsets-31 broker=5] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,260] INFO [Broker id=5] Follower __consumer_offsets-31 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,260] INFO [Broker id=5] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,268] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,272] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,282] INFO [Partition __consumer_offsets-27 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2025-05-20 22:21:46,282] INFO [Partition __consumer_offsets-27 broker=5] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,286] INFO [Broker id=5] Follower __consumer_offsets-27 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,288] INFO [Broker id=5] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,305] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,308] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,313] INFO [Partition __consumer_offsets-39 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2025-05-20 22:21:46,316] INFO [Partition __consumer_offsets-39 broker=5] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,316] INFO [Broker id=5] Follower __consumer_offsets-39 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,317] INFO [Broker id=5] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,324] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,327] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,328] INFO [Partition __consumer_offsets-6 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2025-05-20 22:21:46,330] INFO [Partition __consumer_offsets-6 broker=5] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,337] INFO [Broker id=5] Follower __consumer_offsets-6 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,337] INFO [Broker id=5] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,349] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,351] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,351] INFO [Partition __consumer_offsets-35 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2025-05-20 22:21:46,353] INFO [Partition __consumer_offsets-35 broker=5] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,355] INFO [Broker id=5] Follower __consumer_offsets-35 starts at leader epoch 1 from offset 0 with partition epoch 2 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:21:46,355] INFO [Broker id=5] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:21:46,374] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:21:46,376] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
[2025-05-20 22:21:46,376] INFO [Partition __consumer_offsets-2 broker=5] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2025-05-20 22:21:46,376] INFO [Partition __consumer_offsets-2 broker=5] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:21:46,377] INFO [Broker id=5] Follower __consumer_offsets-2 starts at leader epoch 2 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:21:46,380] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:21:46,382] INFO [Broker id=5] Stopped fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 22:21:46,403] INFO [Broker id=5] Started fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 22:21:46,424] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,426] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,431] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,431] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,435] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,442] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,436] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,445] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,445] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,445] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,446] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,447] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,447] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,449] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,449] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,450] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,449] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,452] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,451] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,460] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,460] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,461] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,462] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,462] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,462] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,463] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,463] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,463] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,463] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,464] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,464] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,464] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,465] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,465] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,465] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,465] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,466] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,466] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,466] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,467] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,461] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,467] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,467] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,468] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,468] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,469] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,469] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,469] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,469] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,470] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,470] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,467] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,471] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,472] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,473] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,473] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,474] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,474] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,474] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,475] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,476] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,476] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,476] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,476] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,477] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,479] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,479] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,480] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,480] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,481] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,481] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,481] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,481] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,482] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,482] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,483] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,483] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,480] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,484] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,484] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,485] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,485] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,484] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,486] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,486] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,486] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,487] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,487] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,487] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,487] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,486] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,488] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,489] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,490] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,488] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,494] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,495] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,495] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,495] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,496] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,496] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,497] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,497] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,500] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,501] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,501] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,502] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,503] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,503] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,507] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,507] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,508] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,508] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,508] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,509] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,509] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,511] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,514] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,495] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,523] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,523] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,523] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,524] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,525] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,526] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,529] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,529] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,530] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,531] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,537] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,522] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,539] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,540] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,540] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,542] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,543] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,544] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,544] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,544] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,545] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,546] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,548] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:46,548] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,555] INFO [DynamicConfigPublisher broker id=5] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 22:21:46,540] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,559] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,561] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,563] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,564] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,564] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,565] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:46,567] INFO [DynamicConfigPublisher broker id=5] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 22:21:46,594] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=5) with a snapshot at offset 1829 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:21:46,945] ERROR Failed to propagate directory assignments because the Controller returned error STALE_BROKER_EPOCH (org.apache.kafka.server.AssignmentsManager)
[2025-05-20 22:21:47,461] INFO [Broker id=5] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:21:47,464] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,473] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,475] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,476] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,481] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,482] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,482] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,483] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,484] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,485] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,486] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,487] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,488] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,489] INFO [Broker id=5] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,491] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,493] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,494] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,500] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,503] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,504] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,506] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,508] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,510] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,511] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,512] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,514] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,516] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,520] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,525] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,527] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,528] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,529] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,531] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,532] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,533] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,535] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,535] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,538] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,539] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,541] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,544] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,545] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,546] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,547] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,551] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,552] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,553] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,553] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,554] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=3, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,555] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=3, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:21:47,557] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=4, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:21:47,566] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,568] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,570] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,570] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,572] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,573] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,573] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,576] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,578] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,579] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,582] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,587] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,587] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,588] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,589] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,589] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,590] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,590] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,590] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,591] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,592] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,592] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,593] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,593] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,593] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,595] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,599] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,599] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,600] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,601] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,601] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,605] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,607] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,607] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,607] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,607] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,607] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,608] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,608] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,608] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,609] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,609] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,609] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,610] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,611] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,611] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,611] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,612] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,612] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,612] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,613] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,613] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,613] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,614] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,614] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,615] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,618] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,620] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,621] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,621] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,621] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,622] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,622] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,622] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,622] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,622] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,622] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,623] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,620] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,623] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,624] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,624] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,624] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,624] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,624] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,623] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,625] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,625] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,625] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,625] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,626] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,626] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,626] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,626] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,626] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,627] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,627] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,627] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,628] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,629] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,629] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,629] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,630] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,630] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,630] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,631] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,631] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,631] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,632] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,632] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,632] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,635] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,635] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,636] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,636] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,637] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,638] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,636] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,641] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,642] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,647] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,647] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,649] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,654] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,654] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,655] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,655] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,655] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,656] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,657] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,657] ERROR Failed to propagate directory assignments because the Controller returned error STALE_BROKER_EPOCH (org.apache.kafka.server.AssignmentsManager)
[2025-05-20 22:21:47,657] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,658] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,658] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,658] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,659] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,659] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,659] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,660] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,660] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,660] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,662] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,664] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,665] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,666] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,667] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,667] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,669] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,669] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,670] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,670] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,670] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,670] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,671] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,671] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,671] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,672] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,672] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,672] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:21:47,676] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,682] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:21:47,712] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:48,567] ERROR Failed to propagate directory assignments because the Controller returned error STALE_BROKER_EPOCH (org.apache.kafka.server.AssignmentsManager)
[2025-05-20 22:21:49,874] ERROR Failed to propagate directory assignments because the Controller returned error STALE_BROKER_EPOCH (org.apache.kafka.server.AssignmentsManager)
[2025-05-20 22:21:50,975] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:21:51,994] ERROR Failed to propagate directory assignments because the Controller returned error STALE_BROKER_EPOCH (org.apache.kafka.server.AssignmentsManager)
[2025-05-20 22:21:55,753] ERROR Failed to propagate directory assignments because the Controller returned error STALE_BROKER_EPOCH (org.apache.kafka.server.AssignmentsManager)
[2025-05-20 22:21:57,372] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:22:02,542] ERROR Failed to propagate directory assignments because the Controller returned error STALE_BROKER_EPOCH (org.apache.kafka.server.AssignmentsManager)
[2025-05-20 22:22:06,372] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:22:15,374] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:22:15,999] ERROR Failed to propagate directory assignments because the Controller returned error STALE_BROKER_EPOCH (org.apache.kafka.server.AssignmentsManager)
[2025-05-20 22:22:24,377] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:22:33,379] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:22:42,379] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:22:42,393] ERROR Failed to propagate directory assignments because the Controller returned error STALE_BROKER_EPOCH (org.apache.kafka.server.AssignmentsManager)
[2025-05-20 22:22:51,382] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:23:00,220] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:23:09,095] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:23:17,934] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:23:26,918] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:23:33,794] ERROR Failed to propagate directory assignments because the Controller returned error STALE_BROKER_EPOCH (org.apache.kafka.server.AssignmentsManager)
[2025-05-20 22:23:35,886] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:23:44,804] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:23:53,807] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:02,809] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:11,653] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:20,656] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:29,516] WARN [BrokerLifecycleManager id=5] Broker 5 sent a heartbeat request but received error STALE_BROKER_EPOCH. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:30,917] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 22:24:30,925] INFO App info kafka.server for 5 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:24:50,787] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 22:24:51,188] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-2:19092,PLAINTEXT_HOST://localhost:39092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 5
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 5
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 22:24:51,218] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 22:24:51,228] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:24:56,550] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 22:24:56,692] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 22:24:56,696] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:24:56,917] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:24:56,972] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:24:57,007] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 22:24:57,022] INFO [BrokerServer id=5] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-20 22:24:57,027] INFO [SharedServer id=5] Starting SharedServer (kafka.server.SharedServer)
[2025-05-20 22:24:57,035] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:24:57,132] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
[2025-05-20 22:24:57,137] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:57,140] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:57,143] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:57,291] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2218 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:24:57,301] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 2218 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:57,304] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2218 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:57,305] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=2218, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000002218.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:24:57,307] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 2218 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:57,339] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-20 22:24:57,357] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:24:57,363] INFO [RaftManager id=5] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:24:57,507] INFO [RaftManager id=5] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:24:57,694] INFO [RaftManager id=5] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=13, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from null (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:24:57,702] INFO [kafka-5-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:24:57,707] INFO [kafka-5-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:24:57,768] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:57,775] INFO [BrokerServer id=5] Starting broker (kafka.server.BrokerServer)
[2025-05-20 22:24:57,781] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:24:57,827] INFO [broker-5-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:57,837] INFO [broker-5-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:57,837] INFO [broker-5-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:57,855] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:24:57,886] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:57,926] INFO [BrokerServer id=5] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 22:24:57,939] INFO [BrokerServer id=5] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 22:24:57,969] INFO [broker-5-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:57,972] INFO [RaftManager id=5] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1622577965 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:24:57,980] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:24:57,990] INFO [broker-5-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:57,992] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,105] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,212] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,245] INFO [RaftManager id=5] Completed transition to Unattached(epoch=15, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=13, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:24:58,314] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,473] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,577] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,583] INFO [RaftManager id=5] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=15, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=15, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 22:24:58,685] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,788] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,871] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 22:24:58,891] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,910] INFO [SocketServer listenerType=BROKER, nodeId=5] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-20 22:24:58,912] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 22:24:58,922] INFO [SocketServer listenerType=BROKER, nodeId=5] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-20 22:24:58,938] INFO [broker-5-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:58,939] INFO [broker-5-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:58,945] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:58,946] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:58,953] INFO [ExpirationReaper-5-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:58,961] INFO [ExpirationReaper-5-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:58,962] INFO [ExpirationReaper-5-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:58,965] INFO [ExpirationReaper-5-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:58,967] INFO [ExpirationReaper-5-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:58,985] INFO [ExpirationReaper-5-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:58,993] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:58,989] INFO [ExpirationReaper-5-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:59,017] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
[2025-05-20 22:24:59,043] INFO [BrokerLifecycleManager id=5] Incarnation xWxdOfLoQY2vmqb8q80L1g of broker 5 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:59,044] INFO [broker-5-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:59,065] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:24:59,076] INFO [ExpirationReaper-5-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:24:59,096] INFO [RaftManager id=5] High watermark set to Optional[LogOffsetMetadata(offset=2224, metadata=Optional.empty)] for the first time for epoch 15 (org.apache.kafka.raft.FollowerState)
[2025-05-20 22:24:59,105] INFO [MetadataLoader id=5] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 2224 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,114] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 2224 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,128] INFO [BrokerServer id=5] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 22:24:59,270] INFO [BrokerLifecycleManager id=5] Successfully registered broker 5 with broker epoch 2226 (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:59,337] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 2223, but the high water mark is 2227 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,337] INFO [MetadataLoader id=5] initializeNewPublishers: The loader is still catching up because we have loaded up to offset 2223, but the high water mark is 2227 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,338] INFO [MetadataLoader id=5] initializeNewPublishers: The loader is still catching up because we have loaded up to offset 2223, but the high water mark is 2227 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,343] INFO [BrokerServer id=5] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 22:24:59,343] INFO [BrokerServer id=5] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 22:24:59,356] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 2227 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,361] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 2226 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,363] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing MetadataVersionPublisher(id=5) with a snapshot at offset 2226 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,363] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 2226 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:24:59,365] INFO [BrokerMetadataPublisher id=5] Publishing initial metadata at offset OffsetAndEpoch(offset=2226, epoch=15) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-20 22:24:59,369] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:24:59,399] INFO [BrokerLifecycleManager id=5] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:59,400] INFO [BrokerServer id=5] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 22:24:59,401] INFO [BrokerServer id=5] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 22:24:59,401] INFO Recovering 51 logs from /tmp/kafka-logs since no clean shutdown file was found (kafka.log.LogManager)
[2025-05-20 22:24:59,416] INFO [BrokerLifecycleManager id=5] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:24:59,465] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-29. (kafka.log.LogLoader)
[2025-05-20 22:24:59,476] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,486] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,495] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,502] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,513] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,524] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,557] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-29, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=29, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 87ms (1/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:24:59,576] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-38. (kafka.log.LogLoader)
[2025-05-20 22:24:59,589] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,605] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,607] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,611] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,616] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,623] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,661] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-38, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=38, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 98ms (2/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:24:59,702] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-27. (kafka.log.LogLoader)
[2025-05-20 22:24:59,709] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,722] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,722] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,732] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,734] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,735] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,746] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-27, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=27, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 77ms (3/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:24:59,757] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-39. (kafka.log.LogLoader)
[2025-05-20 22:24:59,824] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,830] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,846] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,865] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,896] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,916] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,931] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-39, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=39, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 177ms (4/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:24:59,977] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-42. (kafka.log.LogLoader)
[2025-05-20 22:24:59,978] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,979] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:24:59,985] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,033] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,037] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,047] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,055] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-42, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=42, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 102ms (5/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,061] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-9. (kafka.log.LogLoader)
[2025-05-20 22:25:00,066] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,066] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,067] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,073] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,074] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,080] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,088] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-9, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 31ms (6/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,102] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-22. (kafka.log.LogLoader)
[2025-05-20 22:25:00,116] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,118] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,121] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,123] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,125] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,127] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,143] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-22, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=22, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 54ms (7/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,161] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-26. (kafka.log.LogLoader)
[2025-05-20 22:25:00,163] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,177] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,193] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,210] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,214] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,226] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,229] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-26, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=26, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 77ms (8/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,244] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-40. (kafka.log.LogLoader)
[2025-05-20 22:25:00,245] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,246] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,246] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,248] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,261] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,275] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,280] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-40, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=40, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 40ms (9/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,299] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-47. (kafka.log.LogLoader)
[2025-05-20 22:25:00,304] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,305] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,310] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,317] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,319] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,320] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,325] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-47, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=47, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 44ms (10/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,329] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-15. (kafka.log.LogLoader)
[2025-05-20 22:25:00,331] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,336] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,336] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,338] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,341] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,351] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,381] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-15, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 56ms (11/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,386] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-10. (kafka.log.LogLoader)
[2025-05-20 22:25:00,415] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,416] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,420] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,422] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,432] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,437] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,447] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-10, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 56ms (12/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,467] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-2. (kafka.log.LogLoader)
[2025-05-20 22:25:00,488] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,490] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,501] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,506] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,507] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,511] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,515] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-2, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 60ms (13/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,528] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-18. (kafka.log.LogLoader)
[2025-05-20 22:25:00,532] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,535] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,536] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,538] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,539] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,539] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,630] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-18, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 105ms (14/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,641] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-46. (kafka.log.LogLoader)
[2025-05-20 22:25:00,646] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,649] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,651] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,653] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,662] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,663] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,665] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-46, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=46, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 32ms (15/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,668] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-12. (kafka.log.LogLoader)
[2025-05-20 22:25:00,678] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,680] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,681] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,694] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,699] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,703] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,715] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-12, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 48ms (16/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,721] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-23. (kafka.log.LogLoader)
[2025-05-20 22:25:00,724] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,727] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,728] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,729] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,734] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,740] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,743] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-23, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=23, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 27ms (17/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,748] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-14. (kafka.log.LogLoader)
[2025-05-20 22:25:00,754] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,755] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,756] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,758] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,763] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,764] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,775] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-14, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 30ms (18/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,778] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-5. (kafka.log.LogLoader)
[2025-05-20 22:25:00,790] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,794] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,796] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,798] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,798] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,804] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,810] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-5, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 34ms (19/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,812] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-41. (kafka.log.LogLoader)
[2025-05-20 22:25:00,814] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,815] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,816] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,818] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,825] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,826] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,829] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-41, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=41, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (20/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,846] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-1. (kafka.log.LogLoader)
[2025-05-20 22:25:00,854] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,856] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,865] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,869] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,870] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,870] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,873] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-1, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 43ms (21/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,884] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-19. (kafka.log.LogLoader)
[2025-05-20 22:25:00,889] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,890] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,892] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,895] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,895] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,896] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,905] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-19, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 25ms (22/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,920] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-3. (kafka.log.LogLoader)
[2025-05-20 22:25:00,921] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,922] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,922] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,925] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,928] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,932] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,939] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-3, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 31ms (23/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,944] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-16. (kafka.log.LogLoader)
[2025-05-20 22:25:00,946] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,946] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,949] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,962] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,963] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,963] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:00,980] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-16, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 25ms (24/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:00,986] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-25. (kafka.log.LogLoader)
[2025-05-20 22:25:00,998] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,001] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,003] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,011] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,012] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,015] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,020] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-25, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=25, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 36ms (25/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,024] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for _schemas-0. (kafka.log.LogLoader)
[2025-05-20 22:25:01,025] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,026] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,026] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,038] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,039] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,040] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,042] INFO Completed load of Log(dir=/tmp/kafka-logs/_schemas-0, topicId=RrE8eovWRKu4kLR3MRJ0fA, topic=_schemas, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 20ms (26/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,045] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-28. (kafka.log.LogLoader)
[2025-05-20 22:25:01,046] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,047] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,047] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,049] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,050] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,050] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,052] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-28, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=28, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (27/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,056] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-13. (kafka.log.LogLoader)
[2025-05-20 22:25:01,057] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,057] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,058] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,061] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,061] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,062] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,064] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-13, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (28/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,066] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-24. (kafka.log.LogLoader)
[2025-05-20 22:25:01,067] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,067] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,067] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,070] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,071] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,071] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,073] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-24, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=24, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (29/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,076] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-32. (kafka.log.LogLoader)
[2025-05-20 22:25:01,080] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,080] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,081] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,083] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,083] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,083] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,085] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-32, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=32, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (30/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,088] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-31. (kafka.log.LogLoader)
[2025-05-20 22:25:01,089] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,090] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,092] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,094] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,095] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,096] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,101] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-31, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=31, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (31/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,108] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-21. (kafka.log.LogLoader)
[2025-05-20 22:25:01,109] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,109] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,109] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,112] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,112] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,113] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,115] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-21, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=21, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (32/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,117] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-6. (kafka.log.LogLoader)
[2025-05-20 22:25:01,119] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,120] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,121] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,124] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,124] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,125] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,127] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-6, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (33/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,131] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-11. (kafka.log.LogLoader)
[2025-05-20 22:25:01,133] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,134] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,137] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,140] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,140] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,141] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,142] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-11, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (34/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,146] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-30. (kafka.log.LogLoader)
[2025-05-20 22:25:01,146] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,147] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,147] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,151] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,152] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,155] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,158] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-30, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=30, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (35/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,163] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-43. (kafka.log.LogLoader)
[2025-05-20 22:25:01,164] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,164] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,165] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,166] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,166] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,167] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,170] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-43, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=43, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (36/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,173] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-7. (kafka.log.LogLoader)
[2025-05-20 22:25:01,175] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,175] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,176] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,178] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,179] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,179] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,181] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-7, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (37/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,183] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-33. (kafka.log.LogLoader)
[2025-05-20 22:25:01,183] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,185] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,185] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,188] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,189] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,189] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,193] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-33, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=33, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (38/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,195] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-36. (kafka.log.LogLoader)
[2025-05-20 22:25:01,196] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,196] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,198] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,202] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,202] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,202] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,205] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-36, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=36, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (39/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,210] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-45. (kafka.log.LogLoader)
[2025-05-20 22:25:01,213] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,214] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,215] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,222] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,224] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,230] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,232] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-45, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=45, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 26ms (40/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,243] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-0. (kafka.log.LogLoader)
[2025-05-20 22:25:01,247] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,248] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,252] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,255] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,256] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,256] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,257] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-0, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 24ms (41/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,262] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-20. (kafka.log.LogLoader)
[2025-05-20 22:25:01,265] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,267] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,268] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,271] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,271] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,271] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,273] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-20, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=20, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (42/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,276] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-4. (kafka.log.LogLoader)
[2025-05-20 22:25:01,278] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,280] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,281] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,284] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,288] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,289] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,291] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-4, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 17ms (43/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,293] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-8. (kafka.log.LogLoader)
[2025-05-20 22:25:01,293] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,294] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,294] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,295] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,296] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,296] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,298] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-8, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (44/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,300] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-49. (kafka.log.LogLoader)
[2025-05-20 22:25:01,301] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,301] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,301] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,303] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,303] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,304] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,305] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-49, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=49, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (45/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,307] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-34. (kafka.log.LogLoader)
[2025-05-20 22:25:01,308] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,308] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,308] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,310] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,310] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,310] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,312] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-34, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=34, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (46/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,314] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-48. (kafka.log.LogLoader)
[2025-05-20 22:25:01,315] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,316] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,316] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,318] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,318] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,318] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,319] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-48, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=48, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (47/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,321] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-44. (kafka.log.LogLoader)
[2025-05-20 22:25:01,322] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,322] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,323] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,324] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,324] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,324] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,326] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-44, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=44, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (48/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,328] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-37. (kafka.log.LogLoader)
[2025-05-20 22:25:01,328] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,328] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,329] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,330] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,331] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,331] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,332] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-37, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=37, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (49/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,334] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-35. (kafka.log.LogLoader)
[2025-05-20 22:25:01,335] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,335] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,335] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,337] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,339] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,340] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,343] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-35, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=35, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (50/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,345] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __consumer_offsets-17. (kafka.log.LogLoader)
[2025-05-20 22:25:01,346] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,346] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,346] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,348] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,348] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,349] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 22:25:01,350] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-17, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (51/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 22:25:01,355] INFO Loaded 51 logs in 1980ms (unclean log dirs = ArrayBuffer(/tmp/kafka-logs)) (kafka.log.LogManager)
[2025-05-20 22:25:01,356] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-20 22:25:01,357] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-20 22:25:01,368] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-20 22:25:01,424] INFO [BrokerLifecycleManager id=5] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:25:01,505] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:25:01,508] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:25:01,512] INFO [AddPartitionsToTxnSenderThread-5]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:25:01,524] INFO [GroupCoordinator 5]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,528] INFO [GroupCoordinator 5]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,540] INFO [TransactionCoordinator id=5] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:25:01,560] INFO [TxnMarkerSenderThread-5]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:25:01,560] INFO [TransactionCoordinator id=5] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:25:01,586] INFO [Broker id=5] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:01,591] INFO [Broker id=5] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,609] INFO [Partition __consumer_offsets-13 broker=5] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,624] INFO [Broker id=5] Follower __consumer_offsets-13 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,626] INFO [Broker id=5] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,628] INFO [Partition __consumer_offsets-46 broker=5] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,630] INFO [Broker id=5] Follower __consumer_offsets-46 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,631] INFO [Broker id=5] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,634] INFO [Partition __consumer_offsets-9 broker=5] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,635] INFO [Broker id=5] Follower __consumer_offsets-9 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,636] INFO [Broker id=5] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,642] INFO [Partition __consumer_offsets-42 broker=5] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,645] INFO [Broker id=5] Follower __consumer_offsets-42 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,645] INFO [Broker id=5] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,647] INFO [Partition __consumer_offsets-21 broker=5] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,648] INFO [Broker id=5] Follower __consumer_offsets-21 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,648] INFO [Broker id=5] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,650] INFO [Partition __consumer_offsets-17 broker=5] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,651] INFO [Broker id=5] Follower __consumer_offsets-17 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,653] INFO [Broker id=5] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,655] INFO [Partition __consumer_offsets-30 broker=5] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,656] INFO [Broker id=5] Follower __consumer_offsets-30 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,658] INFO [Broker id=5] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,661] INFO [Partition __consumer_offsets-26 broker=5] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,667] INFO [Broker id=5] Follower __consumer_offsets-26 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,672] INFO [Broker id=5] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,675] INFO [Partition __consumer_offsets-5 broker=5] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,676] INFO [Broker id=5] Follower __consumer_offsets-5 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,677] INFO [Broker id=5] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,681] INFO [Partition __consumer_offsets-38 broker=5] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,682] INFO [Broker id=5] Follower __consumer_offsets-38 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,686] INFO [Broker id=5] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,688] INFO [Partition __consumer_offsets-1 broker=5] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,689] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,690] INFO [Broker id=5] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,692] INFO [Partition __consumer_offsets-34 broker=5] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,696] INFO [Broker id=5] Follower __consumer_offsets-34 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,696] INFO [Broker id=5] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,704] INFO [Partition __consumer_offsets-16 broker=5] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,708] INFO [Broker id=5] Follower __consumer_offsets-16 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,711] INFO [Broker id=5] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-20 22:25:01,712] INFO [Partition _schemas-0 broker=5] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,712] INFO [Broker id=5] Follower _schemas-0 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,715] INFO [Broker id=5] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,719] INFO [Partition __consumer_offsets-45 broker=5] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,721] INFO [Broker id=5] Follower __consumer_offsets-45 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,732] INFO [Broker id=5] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,733] INFO [Partition __consumer_offsets-12 broker=5] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,734] INFO [Broker id=5] Follower __consumer_offsets-12 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,734] INFO [Broker id=5] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,735] INFO [Partition __consumer_offsets-41 broker=5] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,743] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,748] INFO [Broker id=5] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,749] INFO [Partition __consumer_offsets-24 broker=5] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,751] INFO [Broker id=5] Follower __consumer_offsets-24 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,754] INFO [Broker id=5] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,756] INFO [Partition __consumer_offsets-20 broker=5] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,769] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,771] INFO [Broker id=5] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,779] INFO [Partition __consumer_offsets-49 broker=5] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,781] INFO [Broker id=5] Follower __consumer_offsets-49 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,782] INFO [Broker id=5] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,783] INFO [Partition __consumer_offsets-0 broker=5] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,783] INFO [Broker id=5] Follower __consumer_offsets-0 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,784] INFO [Broker id=5] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,785] INFO [Partition __consumer_offsets-29 broker=5] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,789] INFO [Broker id=5] Follower __consumer_offsets-29 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,789] INFO [Broker id=5] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,790] INFO [Partition __consumer_offsets-25 broker=5] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,791] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,793] INFO [Broker id=5] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,797] INFO [Partition __consumer_offsets-8 broker=5] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,798] INFO [Broker id=5] Follower __consumer_offsets-8 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,799] INFO [Broker id=5] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,801] INFO [Partition __consumer_offsets-37 broker=5] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,803] INFO [Broker id=5] Follower __consumer_offsets-37 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,803] INFO [Broker id=5] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,804] INFO [Partition __consumer_offsets-4 broker=5] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,805] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,805] INFO [Broker id=5] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,806] INFO [Partition __consumer_offsets-33 broker=5] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,807] INFO [Broker id=5] Follower __consumer_offsets-33 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,811] INFO [Broker id=5] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,812] INFO [Partition __consumer_offsets-15 broker=5] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,812] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,813] INFO [Broker id=5] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,816] INFO [Partition __consumer_offsets-48 broker=5] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,816] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,817] INFO [Broker id=5] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,817] INFO [Partition __consumer_offsets-11 broker=5] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,818] INFO [Broker id=5] Follower __consumer_offsets-11 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,818] INFO [Broker id=5] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,820] INFO [Partition __consumer_offsets-44 broker=5] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,820] INFO [Broker id=5] Follower __consumer_offsets-44 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,821] INFO [Broker id=5] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,822] INFO [Partition __consumer_offsets-23 broker=5] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,823] INFO [Broker id=5] Follower __consumer_offsets-23 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,823] INFO [Broker id=5] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,826] INFO [Partition __consumer_offsets-19 broker=5] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,826] INFO [Broker id=5] Follower __consumer_offsets-19 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,828] INFO [Broker id=5] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,833] INFO [Partition __consumer_offsets-32 broker=5] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,835] INFO [Broker id=5] Follower __consumer_offsets-32 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,841] INFO [Broker id=5] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,844] INFO [Partition __consumer_offsets-28 broker=5] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,845] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,845] INFO [Broker id=5] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,846] INFO [Partition __consumer_offsets-7 broker=5] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,847] INFO [Broker id=5] Follower __consumer_offsets-7 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,848] INFO [Broker id=5] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,850] INFO [Partition __consumer_offsets-40 broker=5] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,850] INFO [Broker id=5] Follower __consumer_offsets-40 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,851] INFO [Broker id=5] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,852] INFO [Partition __consumer_offsets-3 broker=5] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,852] INFO [Broker id=5] Follower __consumer_offsets-3 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,853] INFO [Broker id=5] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,854] INFO [Partition __consumer_offsets-36 broker=5] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,854] INFO [Broker id=5] Follower __consumer_offsets-36 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,854] INFO [Broker id=5] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,855] INFO [Partition __consumer_offsets-47 broker=5] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,856] INFO [Broker id=5] Follower __consumer_offsets-47 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,856] INFO [Broker id=5] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,858] INFO [Partition __consumer_offsets-14 broker=5] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,859] INFO [Broker id=5] Follower __consumer_offsets-14 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,859] INFO [Broker id=5] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,860] INFO [Partition __consumer_offsets-43 broker=5] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,860] INFO [Broker id=5] Follower __consumer_offsets-43 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,861] INFO [Broker id=5] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,862] INFO [Partition __consumer_offsets-10 broker=5] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,862] INFO [Broker id=5] Follower __consumer_offsets-10 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,863] INFO [Broker id=5] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,864] INFO [Partition __consumer_offsets-22 broker=5] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,865] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,866] INFO [Broker id=5] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,867] INFO [Partition __consumer_offsets-18 broker=5] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,867] INFO [Broker id=5] Follower __consumer_offsets-18 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,867] INFO [Broker id=5] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,868] INFO [Partition __consumer_offsets-31 broker=5] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,869] INFO [Broker id=5] Follower __consumer_offsets-31 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,869] INFO [Broker id=5] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,870] INFO [Partition __consumer_offsets-27 broker=5] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,870] INFO [Broker id=5] Follower __consumer_offsets-27 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,871] INFO [Broker id=5] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,872] INFO [Partition __consumer_offsets-39 broker=5] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,874] INFO [Broker id=5] Follower __consumer_offsets-39 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,876] INFO [Broker id=5] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,879] INFO [Partition __consumer_offsets-6 broker=5] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,881] INFO [Broker id=5] Follower __consumer_offsets-6 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,882] INFO [Broker id=5] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,883] INFO [Partition __consumer_offsets-35 broker=5] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,883] INFO [Broker id=5] Follower __consumer_offsets-35 starts at leader epoch 1 from offset 0 with partition epoch 3 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:01,884] INFO [Broker id=5] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 22:25:01,886] INFO [Partition __consumer_offsets-2 broker=5] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 22:25:01,886] INFO [Broker id=5] Follower __consumer_offsets-2 starts at leader epoch 2 from offset 0 with partition epoch 4 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:01,888] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:01,889] INFO [Broker id=5] Stopped fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 22:25:01,895] INFO [Broker id=5] Started fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 22:25:01,925] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,926] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,927] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,927] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,928] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,928] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,929] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,929] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,929] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,929] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,930] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,930] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,930] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,930] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,930] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,931] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,931] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,931] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,932] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,932] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,932] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,932] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,933] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,933] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,933] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,933] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,934] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,934] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,934] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,935] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,935] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,935] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,936] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,936] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,937] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,937] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,938] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,938] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,939] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,939] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,939] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,940] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,940] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,940] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,941] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,941] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,941] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,941] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,942] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,942] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,942] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,942] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,943] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,943] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,943] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,943] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,944] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,944] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,944] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,944] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,945] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,945] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,945] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,946] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,946] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,946] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,946] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,946] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,947] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,947] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,947] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,948] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,948] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,948] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,949] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,949] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,949] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,949] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,950] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,950] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,950] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,950] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,950] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,951] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,951] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,951] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,951] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,952] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,952] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,952] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,953] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,953] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,953] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,954] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,954] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,954] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,955] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,955] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,956] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,956] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,956] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,957] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,958] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,958] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,958] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,958] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,958] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,959] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,959] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,959] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,959] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,960] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,960] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,960] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,961] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,960] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,961] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,961] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,961] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,962] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,962] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,962] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,962] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,963] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,963] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,963] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,963] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,964] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,964] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,964] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,964] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,965] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,965] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,965] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,966] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,966] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,966] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,967] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,967] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,967] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,967] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,968] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,968] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,968] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,969] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,968] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,969] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:01,969] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,969] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,970] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:01,974] INFO [DynamicConfigPublisher broker id=5] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 22:25:01,980] INFO [DynamicConfigPublisher broker id=5] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 22:25:01,984] INFO [BrokerServer id=5] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 22:25:01,984] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=5) with a snapshot at offset 2226 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 22:25:01,986] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-2:19092,PLAINTEXT_HOST://localhost:39092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 5
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 5
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 22:25:01,989] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 22:25:01,993] INFO [BrokerServer id=5] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 22:25:02,109] INFO [BrokerLifecycleManager id=5] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:25:02,111] INFO [BrokerServer id=5] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 22:25:02,115] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 22:25:02,118] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 22:25:02,119] INFO [SocketServer listenerType=BROKER, nodeId=5] Enabling request processing. (kafka.network.SocketServer)
[2025-05-20 22:25:02,123] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 22:25:02,140] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 22:25:02,149] INFO [BrokerServer id=5] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 22:25:02,151] INFO [BrokerServer id=5] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 22:25:02,152] INFO [BrokerServer id=5] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 22:25:02,153] INFO [BrokerServer id=5] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 22:25:02,155] INFO [BrokerServer id=5] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-20 22:25:02,157] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:25:02,158] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:25:02,158] INFO Kafka startTimeMs: 1747779902157 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:25:02,160] INFO [KafkaRaftServer nodeId=5] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-20 22:25:02,471] INFO [Broker id=5] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:02,474] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,477] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,477] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,478] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,479] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,479] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,481] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,482] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,483] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,483] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,484] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,486] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,487] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,487] INFO [Broker id=5] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,488] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,489] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,489] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,490] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,490] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,492] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,493] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,494] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,496] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,497] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,499] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,501] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,503] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,504] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,507] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,507] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,508] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,508] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,510] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,511] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,513] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,515] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,516] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,517] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,518] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,518] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,519] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,520] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,520] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,521] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,521] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,522] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,523] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,523] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,524] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=4, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,524] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[6], partitionEpoch=4, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,526] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=-1, leaderEpoch=2, isr=[6], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,530] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,530] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,531] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,531] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,532] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,534] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,534] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,535] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,537] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,537] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,537] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,538] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,538] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,538] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,539] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,539] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,539] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,540] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,540] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,540] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,541] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,541] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,541] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,542] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,542] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,542] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,543] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,543] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,543] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,545] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,546] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,545] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,546] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,547] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,547] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,548] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,549] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,548] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,550] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,549] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,550] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,551] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,551] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,551] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,557] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,557] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,559] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,562] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,562] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,562] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,563] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,564] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,564] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,563] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,564] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,565] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,565] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,566] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,566] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,571] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,571] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,577] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,580] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,581] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,583] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,587] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,587] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,587] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,588] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,589] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,588] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,591] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,592] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,592] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,593] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,593] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,596] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,598] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,598] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,599] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,600] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,601] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,601] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,603] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,604] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,603] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,609] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,610] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,610] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,610] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,612] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,612] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,613] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,614] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,615] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,615] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,616] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,614] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,616] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,618] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,617] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,618] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,619] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,619] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,621] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,620] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,623] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,622] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,624] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,624] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,626] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,626] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,626] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,626] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,627] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,627] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,627] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,627] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,631] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,632] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,629] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,636] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,634] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,641] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,643] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,644] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,645] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,640] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,646] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,647] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,648] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,652] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,648] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,653] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,654] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,654] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,655] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,655] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,658] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,656] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,660] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,659] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,660] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,661] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,661] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,662] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,661] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,664] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,665] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,665] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,667] INFO [Broker id=5] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:02,668] INFO [Broker id=5] Follower __consumer_offsets-13 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,669] INFO [Broker id=5] Follower __consumer_offsets-46 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,669] INFO [Broker id=5] Follower __consumer_offsets-9 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,670] INFO [Broker id=5] Follower __consumer_offsets-42 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,671] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,673] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,674] INFO [Broker id=5] Follower __consumer_offsets-30 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,675] INFO [Broker id=5] Follower __consumer_offsets-26 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,675] INFO [Broker id=5] Follower __consumer_offsets-5 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,676] INFO [Broker id=5] Follower __consumer_offsets-38 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,678] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,679] INFO [Broker id=5] Follower __consumer_offsets-34 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,680] INFO [Broker id=5] Follower __consumer_offsets-16 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,681] INFO [Broker id=5] Follower _schemas-0 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,682] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,682] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,684] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,685] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,686] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,687] INFO [Broker id=5] Follower __consumer_offsets-49 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,688] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,689] INFO [Broker id=5] Follower __consumer_offsets-29 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,689] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,690] INFO [Broker id=5] Follower __consumer_offsets-8 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,690] INFO [Broker id=5] Follower __consumer_offsets-37 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,691] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,693] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,693] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,694] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,694] INFO [Broker id=5] Follower __consumer_offsets-11 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,695] INFO [Broker id=5] Follower __consumer_offsets-44 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,695] INFO [Broker id=5] Follower __consumer_offsets-23 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,696] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,696] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,697] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,698] INFO [Broker id=5] Follower __consumer_offsets-7 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,698] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,699] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,699] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,700] INFO [Broker id=5] Follower __consumer_offsets-47 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,701] INFO [Broker id=5] Follower __consumer_offsets-14 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,702] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,703] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,704] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,704] INFO [Broker id=5] Follower __consumer_offsets-18 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,705] INFO [Broker id=5] Follower __consumer_offsets-31 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,705] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,706] INFO [Broker id=5] Follower __consumer_offsets-39 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,707] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=-1, leaderEpoch=1, isr=[5], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 1. (state.change.logger)
[2025-05-20 22:25:02,709] INFO [Broker id=5] Follower __consumer_offsets-35 starts at leader epoch 2 from offset 0 with partition epoch 5 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:25:02,709] INFO [Broker id=5] Follower __consumer_offsets-2 starts at leader epoch 3 from offset 0 with partition epoch 6 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:25:02,710] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, _schemas-0, __consumer_offsets-14, __consumer_offsets-41, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:02,710] INFO [Broker id=5] Stopped fetchers as part of become-follower for 35 partitions (state.change.logger)
[2025-05-20 22:25:02,736] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,740] INFO [ReplicaFetcherManager on broker 5] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-48 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-9 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-42 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-28 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-38 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-1 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-47 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), _schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-14 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-41 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-22 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-20 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-31 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-25 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-8 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0), __consumer_offsets-35 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-4 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,0), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:02,741] INFO [Broker id=5] Started fetchers as part of become-follower for 35 partitions (state.change.logger)
[2025-05-20 22:25:02,742] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,743] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,745] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,745] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,746] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,746] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,746] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,746] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,747] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,747] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,748] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,748] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,748] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,748] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,749] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,749] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,749] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,750] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,750] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,750] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,750] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,751] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,751] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,751] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,752] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,752] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,752] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,752] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,753] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,753] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,754] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,754] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,754] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,755] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,755] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,755] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,756] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,756] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,756] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,757] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,757] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,757] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,758] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,758] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,758] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,758] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,759] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,759] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,759] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,760] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,760] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,760] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,761] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,761] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,762] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,762] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,763] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,763] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,764] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,764] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,765] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,765] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,765] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,766] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,766] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,767] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,767] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,768] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,768] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:25:02,768] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 22:25:02,779] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,779] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,780] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,780] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,780] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,781] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,781] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,781] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,780] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,782] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,783] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,783] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,783] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,784] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,784] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,784] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,785] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,785] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,785] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,785] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,786] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,786] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,786] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,787] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,787] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,787] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,788] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,788] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,788] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,789] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,789] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,789] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,790] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,790] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,790] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,791] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,791] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,792] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,793] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,793] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,793] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,793] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,794] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,794] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,795] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,795] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,796] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,795] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,796] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,797] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,797] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,797] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,798] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,798] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,799] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,799] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,798] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,799] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,800] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,800] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,800] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,801] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,801] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,802] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,802] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,804] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,804] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,804] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,805] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,806] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,806] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,806] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,805] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,807] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,807] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,808] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,808] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,809] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,809] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,809] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,810] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,810] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,811] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,812] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,812] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,812] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,813] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,813] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,813] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,814] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,814] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,815] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,815] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,816] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,816] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,817] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,817] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,818] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,819] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,819] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,819] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,820] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,819] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,820] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,821] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,821] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,821] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,821] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,823] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,823] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,824] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,824] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,824] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,825] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,824] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,825] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,826] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,826] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,826] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,826] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,827] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,827] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,827] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,828] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,828] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,829] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,829] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,829] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,830] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,831] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,831] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,835] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,838] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,837] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,839] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,839] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,841] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,841] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,841] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,842] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,843] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,842] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,844] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,845] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,845] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,845] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,845] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,846] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,846] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,847] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,886] INFO [Broker id=5] Transitioning 2 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:02,887] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,888] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,889] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,889] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,890] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:02,890] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,890] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,891] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:02,994] INFO [Broker id=5] Transitioning 16 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:02,995] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,995] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:02,996] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:02,997] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:02,998] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:02,998] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:02,999] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,000] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,001] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,002] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,003] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,004] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,004] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,006] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,007] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,007] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,008] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,008] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,009] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,009] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,009] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,009] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,010] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,010] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,011] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,011] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,011] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,011] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,012] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,012] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,012] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,013] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,013] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,013] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,014] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,013] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,014] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,014] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,014] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,015] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,015] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,015] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,016] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,016] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,016] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,016] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,017] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,017] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,017] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,018] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,017] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,018] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,018] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,018] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,019] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,019] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,019] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,019] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,020] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,020] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,020] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,021] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,021] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,021] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,389] INFO [Broker id=5] Transitioning 16 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:25:03,394] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-45, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-0, __consumer_offsets-27, __consumer_offsets-40, __consumer_offsets-6, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-33) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:03,405] INFO [Broker id=5] Leader __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 6, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:03,416] INFO [Broker id=5] Leader __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 6, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:03,431] INFO [Broker id=5] Leader __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 6, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:03,435] INFO [Broker id=5] Leader __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 6, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:03,441] INFO [Broker id=5] Leader __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 6, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:03,453] INFO [Broker id=5] Leader __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 6, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:03,460] INFO [Broker id=5] Leader __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 6, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:03,466] INFO [Broker id=5] Leader __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 6, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:03,473] INFO [Broker id=5] Leader __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 6, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:03,481] INFO [Broker id=5] Leader __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 6, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:03,486] INFO [Broker id=5] Leader __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 6, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:03,492] INFO [Broker id=5] Leader __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 6, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:03,499] INFO [Broker id=5] Leader __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 6, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:03,509] INFO [Broker id=5] Leader __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 6, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:03,514] INFO [Broker id=5] Leader __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 6, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:03,518] INFO [Partition __consumer_offsets-45 broker=5] ISR updated to 5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,521] INFO [Broker id=5] Leader __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 2 from offset 0 with partition epoch 6, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 22:25:03,527] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 45 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,528] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,529] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 43 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,530] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,530] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 12 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,530] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,531] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 10 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,531] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,531] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 24 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,531] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,532] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 21 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,532] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,532] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 19 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,532] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,533] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 17 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,533] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,534] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 32 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,534] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,535] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 0 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,535] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,535] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-45 in 5 milliseconds for epoch 2, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,535] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 27 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,536] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,536] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-43 in 6 milliseconds for epoch 2, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,536] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 40 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,537] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,537] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-12 in 6 milliseconds for epoch 2, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,537] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 6 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,538] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-10 in 6 milliseconds for epoch 2, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,538] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,538] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-24 in 6 milliseconds for epoch 2, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,538] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 3 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,539] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,539] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-21 in 7 milliseconds for epoch 2, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,539] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 36 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,540] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,540] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-19 in 7 milliseconds for epoch 2, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,540] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 33 in epoch 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,540] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,540] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-17 in 6 milliseconds for epoch 2, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,541] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-32 in 6 milliseconds for epoch 2, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,541] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-0 in 6 milliseconds for epoch 2, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,542] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-27 in 6 milliseconds for epoch 2, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,542] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-40 in 5 milliseconds for epoch 2, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,543] INFO [Broker id=5] Transitioning 18 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:03,543] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-6 in 5 milliseconds for epoch 2, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,543] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,543] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-3 in 4 milliseconds for epoch 2, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,543] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,544] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-36 in 4 milliseconds for epoch 2, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,544] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,544] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-33 in 3 milliseconds for epoch 2, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,544] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,545] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,545] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,546] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,546] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,546] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,547] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,547] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,547] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,548] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,549] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,549] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,550] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,551] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,553] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,554] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,556] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,557] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,557] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,558] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,558] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,560] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,560] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,560] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,558] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,561] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,561] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,564] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,563] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,564] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,565] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,564] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,565] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,565] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,565] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,566] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,566] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,566] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,567] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,567] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,568] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,568] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,568] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,568] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,569] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,569] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,569] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,570] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,570] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,570] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,570] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,571] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,571] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,571] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,572] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,572] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,572] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,572] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,573] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,573] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,569] INFO [Partition __consumer_offsets-32 broker=5] ISR updated to 5,6  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,574] INFO [Broker id=5] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:25:03,569] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,576] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-45) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:03,575] INFO [Partition __consumer_offsets-0 broker=5] ISR updated to 5,6  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,576] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,576] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,577] INFO [Partition __consumer_offsets-43 broker=5] ISR updated to 5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,577] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,577] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,577] INFO [Partition __consumer_offsets-27 broker=5] ISR updated to 5,6  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,578] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,578] INFO [Partition __consumer_offsets-12 broker=5] ISR updated to 5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,578] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,579] INFO [Partition __consumer_offsets-10 broker=5] ISR updated to 5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,579] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,579] INFO [Partition __consumer_offsets-24 broker=5] ISR updated to 5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,580] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,580] INFO [Partition __consumer_offsets-21 broker=5] ISR updated to 5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,580] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,581] INFO [Partition __consumer_offsets-19 broker=5] ISR updated to 5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,581] INFO [Partition __consumer_offsets-17 broker=5] ISR updated to 5,6  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:03,896] INFO [Broker id=5] Transitioning 10 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:25:03,898] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-32, __consumer_offsets-0, __consumer_offsets-43, __consumer_offsets-27, __consumer_offsets-12, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:03,899] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,900] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,901] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,904] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,906] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,906] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,907] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,908] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,908] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,909] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:03,942] INFO [Broker id=5] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:03,943] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,944] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:03,944] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,945] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:03,995] INFO [Broker id=5] Transitioning 16 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:03,996] INFO [Broker id=5] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:03,997] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,998] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,998] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,999] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:03,999] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,000] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,001] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,002] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,002] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,002] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,003] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,003] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=7, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,004] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,004] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,004] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=7, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,005] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,006] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,007] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,007] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,007] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,007] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,008] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,007] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,009] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,009] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,010] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,010] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,009] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,010] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,011] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,011] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,011] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,012] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,012] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,013] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,013] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,014] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,014] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,015] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,015] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,015] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,017] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,017] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,017] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,018] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,018] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,018] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,018] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,019] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,019] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,031] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,031] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,035] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,040] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,044] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,045] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,046] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,041] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,048] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,049] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,093] INFO [Partition __consumer_offsets-45 broker=5] ISR updated to 5,4,6  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,126] INFO [Broker id=5] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:25:04,127] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-45) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:04,128] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,137] INFO [Partition __consumer_offsets-43 broker=5] ISR updated to 5,4,6  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,138] INFO [Partition __consumer_offsets-12 broker=5] ISR updated to 5,4,6  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,139] INFO [Partition __consumer_offsets-10 broker=5] ISR updated to 5,4,6  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,140] INFO [Partition __consumer_offsets-24 broker=5] ISR updated to 5,4,6  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,140] INFO [Partition __consumer_offsets-21 broker=5] ISR updated to 5,4,6  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,141] INFO [Partition __consumer_offsets-19 broker=5] ISR updated to 5,4,6  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,419] INFO [Broker id=5] Transitioning 6 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:25:04,422] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-24, __consumer_offsets-21, __consumer_offsets-43, __consumer_offsets-19, __consumer_offsets-12, __consumer_offsets-10) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:04,425] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,427] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,430] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,431] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,435] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,437] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,478] INFO [Broker id=5] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:04,478] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,479] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,479] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,480] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,584] INFO [Broker id=5] Transitioning 16 partition(s) to local followers. (state.change.logger)
[2025-05-20 22:25:04,588] INFO [Broker id=5] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,590] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,590] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,594] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,597] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,598] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,598] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,600] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,601] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,601] INFO [Partition __consumer_offsets-32 broker=5] ISR updated to 5,6,4  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,601] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,604] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,607] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,607] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,608] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,608] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=7, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-20 22:25:04,609] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-20 22:25:04,610] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,612] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,614] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,614] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,614] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,615] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,615] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,615] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,615] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,615] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,616] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,616] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,616] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,621] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,621] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,621] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,622] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,622] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,622] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,622] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,623] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,622] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,625] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,623] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,625] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,625] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,626] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,626] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,627] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,627] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,628] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,628] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,628] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,626] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,628] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,629] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,629] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[2] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,629] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,629] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,630] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,630] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:25:04,631] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,630] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,631] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[2]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,632] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:25:04,642] INFO [Broker id=5] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:25:04,644] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-32) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:04,645] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,649] INFO [Partition __consumer_offsets-0 broker=5] ISR updated to 5,6,4  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,649] INFO [Partition __consumer_offsets-27 broker=5] ISR updated to 5,6,4  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,650] INFO [Partition __consumer_offsets-40 broker=5] ISR updated to 5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:04,650] INFO [Partition __consumer_offsets-6 broker=5] ISR updated to 5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:04,650] INFO [Partition __consumer_offsets-3 broker=5] ISR updated to 5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:04,651] INFO [Partition __consumer_offsets-36 broker=5] ISR updated to 5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:04,651] INFO [Partition __consumer_offsets-33 broker=5] ISR updated to 5,4  and version updated to 7 (kafka.cluster.Partition)
[2025-05-20 22:25:04,652] INFO [Partition __consumer_offsets-17 broker=5] ISR updated to 5,6,4  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:04,994] INFO [Broker id=5] Transitioning 8 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:25:04,995] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-0, __consumer_offsets-27, __consumer_offsets-40, __consumer_offsets-6, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-33, __consumer_offsets-17) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:04,996] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,998] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:04,999] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:05,000] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:05,000] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:05,001] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:05,002] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4], partitionEpoch=7, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:05,004] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 6, 4], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:05,104] INFO [Partition __consumer_offsets-40 broker=5] ISR updated to 5,4,6  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:05,138] INFO [Broker id=5] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:25:05,139] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:05,141] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:05,152] INFO [Partition __consumer_offsets-6 broker=5] ISR updated to 5,4,6  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:05,153] INFO [Partition __consumer_offsets-3 broker=5] ISR updated to 5,4,6  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:05,153] INFO [Partition __consumer_offsets-36 broker=5] ISR updated to 5,4,6  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:05,153] INFO [Partition __consumer_offsets-33 broker=5] ISR updated to 5,4,6  and version updated to 8 (kafka.cluster.Partition)
[2025-05-20 22:25:05,493] INFO [Broker id=5] Transitioning 4 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:25:05,495] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-6, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-33) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:25:05,496] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:05,498] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:05,499] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:25:05,502] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5, 4, 6], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:26:02,410] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 22:26:02,417] INFO [BrokerServer id=5] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
[2025-05-20 22:26:02,419] INFO [BrokerServer id=5] shutting down (kafka.server.BrokerServer)
[2025-05-20 22:26:02,421] INFO [BrokerLifecycleManager id=5] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:26:02,466] INFO [Broker id=5] Transitioning 51 partition(s) to local leaders. (state.change.logger)
[2025-05-20 22:26:02,481] INFO [BrokerLifecycleManager id=5] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:26:02,482] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:26:02,483] INFO [Broker id=5] Leader __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 10, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,502] INFO [Broker id=5] Leader __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 10, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,509] INFO [Broker id=5] Leader __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 9, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:26:02,518] INFO [Broker id=5] Leader __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 9, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:26:02,523] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:26:02,524] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:26:02,525] INFO [Broker id=5] Leader __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 10, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,530] INFO [Broker id=5] Leader __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 10, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,537] INFO [Broker id=5] Leader __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 10, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,542] INFO [Broker id=5] Leader __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 9, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:26:02,545] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Node 6 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:26:02,552] INFO [Broker id=5] Leader __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 9, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:26:02,557] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Cancelled in-flight FETCH request with correlation id 122 due to node 6 being disconnected (elapsed time since creation: 260ms, elapsed time since send: 260ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:26:02,569] INFO [Broker id=5] Leader __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 10, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,569] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Client requested connection close from node 6 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 22:26:02,572] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Error sending fetch request (sessionId=221359246, epoch=122) to node 6: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 6 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 22:26:02,577] INFO [BrokerLifecycleManager id=5] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:26:02,579] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=5, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=221359246, epoch=122), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 6 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 22:26:02,577] INFO [Broker id=5] Leader __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 10, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,580] INFO [BrokerLifecycleManager id=5] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:02,582] INFO [BrokerLifecycleManager id=5] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
[2025-05-20 22:26:02,582] INFO [broker-5-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,583] INFO [broker-5-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,583] INFO [broker-5-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,584] INFO [SocketServer listenerType=BROKER, nodeId=5] Stopping socket server request processors (kafka.network.SocketServer)
[2025-05-20 22:26:02,586] INFO [Broker id=5] Leader _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) starts at leader epoch 5 from offset 2 with partition epoch 10, high watermark 2, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,591] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:26:02,591] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:26:02,592] INFO [Broker id=5] Leader __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 9, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:26:02,594] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:26:02,601] INFO [SocketServer listenerType=BROKER, nodeId=5] Stopped socket server request processors (kafka.network.SocketServer)
[2025-05-20 22:26:02,604] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:26:02,605] INFO [Broker id=5] Leader __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 9, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:26:02,612] INFO [Broker id=5] Leader __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 10, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,617] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:26:02,618] INFO [Broker id=5] Leader __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 2 with partition epoch 10, high watermark 2, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,623] INFO [Broker id=5] Leader __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 9, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:26:02,628] INFO [Broker id=5] Leader __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 9, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:26:02,634] INFO [Broker id=5] Leader __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 10, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,639] INFO [Broker id=5] Leader __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 9, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:26:02,646] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:26:02,647] INFO [Broker id=5] Leader __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 9, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:26:02,655] INFO [Broker id=5] Leader __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 9, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:26:02,661] INFO [Broker id=5] Leader __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 10, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,669] INFO [Broker id=5] Leader __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 10, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,675] INFO [Broker id=5] Leader __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 10, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,679] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:26:02,680] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:26:02,680] INFO [Broker id=5] Leader __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 9, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:26:02,685] INFO [Broker id=5] Leader __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 10, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,691] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:26:02,692] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:26:02,692] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:26:02,694] INFO [Broker id=5] Leader __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 9, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:26:02,703] INFO [Broker id=5] Leader __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 9, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:26:02,713] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:26:02,714] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:26:02,715] INFO [Broker id=5] Leader __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 9, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:26:02,750] INFO [Broker id=5] Leader __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 10, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,754] INFO [Broker id=5] Leader __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 9, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:26:02,759] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:26:02,760] INFO [Broker id=5] Leader __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 10, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,764] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=2, isr=[5], partitionEpoch=10, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 2. Current high watermark 0, ISR [5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 22:26:02,765] INFO [Broker id=5] Leader __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 3 from offset 0 with partition epoch 9, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 22:26:02,770] INFO [Broker id=5] Leader __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 10, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 22:26:02,775] INFO [ReplicaFetcherThread-0-6]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:26:02,776] INFO [ReplicaFetcherThread-0-6]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:26:02,776] INFO [ReplicaFetcherThread-0-6]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-20 22:26:02,778] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 15 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,779] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,779] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 48 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,779] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,780] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 13 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,780] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-15 in 1 milliseconds for epoch 4, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,780] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,780] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds for epoch 4, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,781] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 46 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,781] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,781] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-13 in 1 milliseconds for epoch 5, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,781] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 11 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,782] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,782] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 44 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,783] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,783] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-46 in 1 milliseconds for epoch 5, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,783] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 9 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,784] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,784] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-11 in 2 milliseconds for epoch 5, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,784] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 42 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,785] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,785] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-44 in 2 milliseconds for epoch 5, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,785] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 23 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,785] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-9 in 1 milliseconds for epoch 3, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,786] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,786] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-42 in 1 milliseconds for epoch 3, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,786] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 30 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,786] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds for epoch 5, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,787] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,787] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 28 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,787] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,787] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds for epoch 5, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,788] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 26 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,788] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds for epoch 4, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,788] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,789] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 7 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,789] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-26 in 1 milliseconds for epoch 5, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,789] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,789] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 5 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,790] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-7 in 1 milliseconds for epoch 5, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,790] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,790] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 38 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,790] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds for epoch 5, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,791] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,791] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 1 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,791] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds for epoch 3, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,791] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,792] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 34 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,792] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds for epoch 4, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,792] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,793] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 47 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,793] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds for epoch 5, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,793] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,793] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 16 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,794] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,794] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-47 in 1 milliseconds for epoch 3, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,794] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 14 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,795] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,794] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds for epoch 5, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,795] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 41 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,796] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-14 in 1 milliseconds for epoch 3, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,796] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,797] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 22 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,797] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds for epoch 4, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,797] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,798] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 20 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,798] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,798] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-22 in 0 milliseconds for epoch 4, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,798] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 49 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,799] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-20 in 1 milliseconds for epoch 4, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,800] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,801] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 18 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,801] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,801] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds for epoch 5, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,801] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 31 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,802] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,802] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-18 in 1 milliseconds for epoch 5, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,802] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 29 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,803] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,802] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds for epoch 3, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,803] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 25 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,803] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,804] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 39 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,804] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,804] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 8 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,805] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,805] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 37 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,805] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,805] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 35 in epoch 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,806] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,806] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 4 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,806] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,807] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 2 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,807] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,809] INFO [data-plane Kafka Request Handler on Broker 5], shutting down (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 22:26:02,814] INFO [data-plane Kafka Request Handler on Broker 5], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 22:26:02,815] INFO [ExpirationReaper-5-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,816] INFO [ExpirationReaper-5-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,816] INFO [ExpirationReaper-5-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,817] INFO [KafkaApi-5] Shutdown complete. (kafka.server.KafkaApis)
[2025-05-20 22:26:02,820] INFO [TransactionCoordinator id=5] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:26:02,821] INFO [Transaction State Manager 5]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2025-05-20 22:26:02,822] INFO [TxnMarkerSenderThread-5]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:26:02,822] INFO [TxnMarkerSenderThread-5]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:26:02,822] INFO [TxnMarkerSenderThread-5]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 22:26:02,824] INFO [TransactionCoordinator id=5] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 22:26:02,824] INFO [GroupCoordinator 5]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,824] INFO Loaded member MemberMetadata(memberId=sr-1-99122467-ec1a-4fc4-88f0-59fda8f60096, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 1. (kafka.coordinator.group.GroupMetadata$)
[2025-05-20 22:26:02,827] INFO [GroupCoordinator 5]: Loading group metadata for schema-registry with generation 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,828] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-29 in 25 milliseconds for epoch 5, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,828] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-25 in 24 milliseconds for epoch 4, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,829] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-39 in 25 milliseconds for epoch 5, of which 25 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,829] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-8 in 24 milliseconds for epoch 3, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,829] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-37 in 24 milliseconds for epoch 5, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,830] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-35 in 24 milliseconds for epoch 3, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,830] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-4 in 23 milliseconds for epoch 4, of which 23 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,830] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-2 in 23 milliseconds for epoch 5, of which 23 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 22:26:02,831] INFO [ExpirationReaper-5-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,832] INFO [ExpirationReaper-5-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,832] INFO [ExpirationReaper-5-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,833] INFO [ExpirationReaper-5-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,833] INFO [ExpirationReaper-5-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,833] INFO [ExpirationReaper-5-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,834] INFO [GroupCoordinator 5]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 22:26:02,849] INFO [AssignmentsManager id=5]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:02,849] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,850] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,850] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,850] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:26:02,851] INFO [AssignmentsManager id=5]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:02,852] INFO [ReplicaManager broker=5] Shutting down (kafka.server.ReplicaManager)
[2025-05-20 22:26:02,853] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:26:02,854] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:26:02,854] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 22:26:02,854] INFO [ReplicaFetcherManager on broker 5] shutting down (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:26:02,856] INFO [ReplicaFetcherManager on broker 5] shutdown completed (kafka.server.ReplicaFetcherManager)
[2025-05-20 22:26:02,856] INFO [ReplicaAlterLogDirsManager on broker 5] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:26:02,857] INFO [ReplicaAlterLogDirsManager on broker 5] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 22:26:02,857] INFO [ExpirationReaper-5-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,858] INFO [ExpirationReaper-5-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,858] INFO [ExpirationReaper-5-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,858] INFO [ExpirationReaper-5-RemoteFetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,859] INFO [ExpirationReaper-5-RemoteFetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,859] INFO [ExpirationReaper-5-RemoteFetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,860] INFO [ExpirationReaper-5-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,861] INFO [ExpirationReaper-5-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,861] INFO [ExpirationReaper-5-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,862] INFO [ExpirationReaper-5-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,863] INFO [ExpirationReaper-5-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,863] INFO [ExpirationReaper-5-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,863] INFO [ExpirationReaper-5-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,864] INFO [ExpirationReaper-5-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,864] INFO [ExpirationReaper-5-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 22:26:02,871] INFO [AddPartitionsToTxnSenderThread-5]: Shutting down (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:26:02,872] INFO [AddPartitionsToTxnSenderThread-5]: Stopped (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:26:02,872] INFO [AddPartitionsToTxnSenderThread-5]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 22:26:02,873] INFO [ReplicaManager broker=5] Shut down completely (kafka.server.ReplicaManager)
[2025-05-20 22:26:02,874] INFO [broker-5-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,874] INFO [broker-5-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,874] INFO [broker-5-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,875] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:26:02,876] INFO [broker-5-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,876] INFO [broker-5-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,876] INFO [broker-5-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 22:26:02,877] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 22:26:02,878] INFO Shutting down. (kafka.log.LogManager)
[2025-05-20 22:26:02,879] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
[2025-05-20 22:26:02,879] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:26:02,880] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:26:02,880] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 22:26:02,891] INFO [ProducerStateManager partition=__consumer_offsets-29] Wrote producer snapshot at offset 2 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:26:02,989] INFO [ProducerStateManager partition=_schemas-0] Wrote producer snapshot at offset 2 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:26:03,063] INFO Shutdown complete. (kafka.log.LogManager)
[2025-05-20 22:26:03,064] INFO [broker-5-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:03,064] INFO [broker-5-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:03,064] INFO [broker-5-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:03,065] INFO [broker-5-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:03,065] INFO [broker-5-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:03,065] INFO [broker-5-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:03,065] INFO [broker-5-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:03,066] INFO [broker-5-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:03,066] INFO [broker-5-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:03,066] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:03,066] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:03,066] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 22:26:03,068] INFO [SocketServer listenerType=BROKER, nodeId=5] Shutting down socket server (kafka.network.SocketServer)
[2025-05-20 22:26:03,078] INFO [SocketServer listenerType=BROKER, nodeId=5] Shutdown completed (kafka.network.SocketServer)
[2025-05-20 22:26:03,079] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
[2025-05-20 22:26:03,080] INFO [BrokerLifecycleManager id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:03,081] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:26:03,081] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:26:03,081] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 22:26:03,083] INFO [SharedServer id=5] Stopping SharedServer (kafka.server.SharedServer)
[2025-05-20 22:26:03,084] INFO [MetadataLoader id=5] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:03,084] INFO [SnapshotGenerator id=5] close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:03,085] INFO [SnapshotGenerator id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:03,085] INFO [MetadataLoader id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:03,086] INFO [SnapshotGenerator id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 22:26:03,086] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:26:03,267] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:26:03,267] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 22:26:03,268] INFO [kafka-5-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:26:03,268] INFO [RaftManager id=5] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:26:03,269] INFO [RaftManager id=5] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 22:26:03,269] INFO [RaftManager id=5] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:26:03,269] INFO [kafka-5-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:26:03,269] INFO [kafka-5-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 22:26:03,274] INFO [kafka-5-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:26:03,274] INFO [kafka-5-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:26:03,274] INFO [kafka-5-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 22:26:03,278] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2740 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 22:26:03,280] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:26:03,281] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:26:03,281] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 22:26:03,282] INFO App info kafka.server for 5 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 22:26:03,282] INFO [BrokerServer id=5] shut down completed (kafka.server.BrokerServer)
[2025-05-20 22:26:03,282] INFO [BrokerServer id=5] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
[2025-05-20 22:26:03,283] INFO App info kafka.server for 5 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 23:12:46,768] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 23:12:47,259] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-2:19092,PLAINTEXT_HOST://localhost:39092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 5
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 5
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 23:12:47,288] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 23:12:47,305] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:52,890] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-20 23:12:53,050] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-20 23:12:53,062] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:53,355] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:53,373] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:53,407] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 23:12:53,410] INFO [BrokerServer id=5] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-20 23:12:53,413] INFO [SharedServer id=5] Starting SharedServer (kafka.server.SharedServer)
[2025-05-20 23:12:53,420] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:53,582] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
[2025-05-20 23:12:53,597] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:53,600] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:53,605] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000002218.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-20 23:12:53,608] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000002740.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-20 23:12:53,612] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 7ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:53,859] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 2740 with 0 producer ids in 16 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:12:53,868] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 2740 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:53,871] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2740 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:53,871] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=2740, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000002740.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:12:53,884] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 13ms for snapshot load and 0ms for segment recovery from offset 2740 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:53,919] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-20 23:12:53,961] INFO [RaftManager id=5] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 23:12:53,961] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 23:12:54,054] INFO [RaftManager id=5] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 23:12:54,162] INFO [RaftManager id=5] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=15, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from null (org.apache.kafka.raft.QuorumState)
[2025-05-20 23:12:54,175] INFO [kafka-5-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 23:12:54,203] INFO [kafka-5-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 23:12:54,207] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:54,216] INFO [BrokerServer id=5] Starting broker (kafka.server.BrokerServer)
[2025-05-20 23:12:54,236] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:54,268] INFO [broker-5-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:12:54,274] INFO [broker-5-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:12:54,293] INFO [broker-5-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:12:54,323] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:12:54,328] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:54,379] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,394] INFO [RaftManager id=5] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1823229543 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 23:12:54,415] WARN [RaftManager id=5] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,432] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:54,467] INFO [BrokerServer id=5] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 23:12:54,467] INFO [BrokerServer id=5] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-20 23:12:54,473] INFO [broker-5-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:54,475] INFO [broker-5-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:54,493] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 23:12:54,528] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,530] WARN [RaftManager id=5] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,533] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:54,542] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,545] WARN [RaftManager id=5] Connection to node 2 (kafka-controller-2/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,549] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,550] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,622] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,622] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,624] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,626] WARN [RaftManager id=5] Connection to node 2 (kafka-controller-2/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,636] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:54,644] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,649] WARN [RaftManager id=5] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,738] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:54,849] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:54,857] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,858] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,860] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,866] WARN [RaftManager id=5] Connection to node 2 (kafka-controller-2/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,881] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,882] WARN [RaftManager id=5] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:54,955] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,056] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,158] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,260] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,269] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,270] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,272] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,273] WARN [RaftManager id=5] Connection to node 2 (kafka-controller-2/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,289] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,292] WARN [RaftManager id=5] Connection to node 3 (kafka-controller-3/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,373] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,475] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,544] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 23:12:55,590] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,673] INFO [SocketServer listenerType=BROKER, nodeId=5] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-20 23:12:55,770] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-20 23:12:55,791] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,800] WARN [RaftManager id=5] Connection to node 2 (kafka-controller-2/172.19.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,795] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,810] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,819] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:55,807] INFO [SocketServer listenerType=BROKER, nodeId=5] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-20 23:12:55,889] INFO [broker-5-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:55,890] INFO [broker-5-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:55,919] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:55,927] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:55,923] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:55,979] INFO [ExpirationReaper-5-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:55,988] INFO [ExpirationReaper-5-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:55,999] INFO [ExpirationReaper-5-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:56,009] INFO [ExpirationReaper-5-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:56,023] INFO [ExpirationReaper-5-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:56,052] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,074] INFO [ExpirationReaper-5-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:56,074] INFO [ExpirationReaper-5-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:56,162] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,214] INFO [BrokerLifecycleManager id=5] Incarnation L7RjL7pERduUMS1QlwuGOw of broker 5 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:12:56,214] INFO [broker-5-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,218] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,224] INFO [ExpirationReaper-5-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:12:56,259] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,260] INFO [BrokerServer id=5] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 23:12:56,261] INFO [BrokerServer id=5] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-20 23:12:56,262] INFO [BrokerServer id=5] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 23:12:56,270] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:56,286] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,336] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,355] INFO [RaftManager id=5] Completed transition to Unattached(epoch=16, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=15, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-20 23:12:56,365] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,405] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:56,460] INFO [RaftManager id=5] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=16, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=16, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-20 23:12:56,466] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,510] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:56,566] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,667] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,768] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,885] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:56,986] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,049] INFO [RaftManager id=5] High watermark set to Optional[LogOffsetMetadata(offset=2744, metadata=Optional.empty)] for the first time for epoch 16 (org.apache.kafka.raft.FollowerState)
[2025-05-20 23:12:57,057] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 2744 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,069] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:12:57,069] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:57,119] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:12:57,216] INFO [BrokerLifecycleManager id=5] Successfully registered broker 5 with broker epoch 2744 (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:12:57,270] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 2744 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,312] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 2743 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,325] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing MetadataVersionPublisher(id=5) with a snapshot at offset 2743 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,326] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 2743 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:57,331] INFO [BrokerMetadataPublisher id=5] Publishing initial metadata at offset OffsetAndEpoch(offset=2743, epoch=16) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-20 23:12:57,349] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,379] INFO Skipping recovery of 51 logs from /tmp/kafka-logs since clean shutdown file was found (kafka.log.LogManager)
[2025-05-20 23:12:57,480] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,484] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,484] INFO [ProducerStateManager partition=__consumer_offsets-29] Loading producer state from snapshot file 'SnapshotFile(offset=2, file=/tmp/kafka-logs/__consumer_offsets-29/00000000000000000002.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:12:57,485] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,497] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-29, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=29, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=2) with 1 segments, local-log-start-offset 0 and log-end-offset 2 in 67ms (1/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,595] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,648] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-38, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=38, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 146ms (2/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,694] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,719] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-27, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=27, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 63ms (3/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,731] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,744] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-39, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=39, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 25ms (4/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,777] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,780] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-42, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=42, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 35ms (5/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,807] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,809] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-9, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 29ms (6/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,818] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,825] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-22, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=22, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 16ms (7/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,857] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,863] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-26, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=26, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 35ms (8/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,889] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,892] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-40, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=40, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 29ms (9/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,918] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,922] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-47, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=47, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 28ms (10/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,939] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,943] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-15, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 21ms (11/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:57,966] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:57,973] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-10, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 29ms (12/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,008] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,013] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-2, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 34ms (13/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,020] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,034] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-18, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 20ms (14/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,077] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,082] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-46, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=46, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 38ms (15/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,091] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,100] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-12, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 16ms (16/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,110] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,119] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-23, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=23, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 16ms (17/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,138] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,150] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-14, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 31ms (18/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,191] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,206] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-5, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 54ms (19/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,233] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,239] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-41, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=41, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 29ms (20/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,276] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,287] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-1, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 47ms (21/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,311] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,320] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-19, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 30ms (22/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,325] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,327] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-3, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (23/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,332] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,335] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-16, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (24/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,342] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,344] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-25, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=25, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (25/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,358] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,358] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,359] INFO [ProducerStateManager partition=_schemas-0] Loading producer state from snapshot file 'SnapshotFile(offset=2, file=/tmp/kafka-logs/_schemas-0/00000000000000000002.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:12:58,360] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,363] INFO Completed load of Log(dir=/tmp/kafka-logs/_schemas-0, topicId=RrE8eovWRKu4kLR3MRJ0fA, topic=_schemas, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=2) with 1 segments, local-log-start-offset 0 and log-end-offset 2 in 19ms (26/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,368] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,370] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-28, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=28, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (27/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,380] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,383] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-13, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (28/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,398] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,404] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-24, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=24, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 21ms (29/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,420] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,423] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-32, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=32, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (30/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,432] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,438] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-31, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=31, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (31/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,444] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,446] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-21, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=21, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (32/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,470] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,472] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-6, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (33/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,478] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,480] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-11, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (34/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,484] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,489] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-30, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=30, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (35/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,494] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,497] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-43, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=43, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (36/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,500] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,503] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-7, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (37/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,509] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,513] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-33, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=33, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (38/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,520] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,521] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-36, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=36, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (39/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,529] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,535] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-45, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=45, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (40/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,551] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,553] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-0, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 17ms (41/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,626] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,629] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-20, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=20, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 75ms (42/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,659] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,673] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-4, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 30ms (43/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,699] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,704] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-8, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 17ms (44/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,716] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,732] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-49, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=49, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 20ms (45/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,757] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,766] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-34, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=34, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (46/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,774] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,779] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-48, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=48, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (47/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,834] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,836] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-44, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=44, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 56ms (48/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,859] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,865] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-37, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=37, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 28ms (49/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,921] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,924] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-35, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=35, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 58ms (50/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,929] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:12:58,945] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-17, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 19ms (51/51 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-20 23:12:58,953] INFO Loaded 51 logs in 1602ms (kafka.log.LogManager)
[2025-05-20 23:12:58,955] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-20 23:12:58,957] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-20 23:12:58,969] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-20 23:12:59,223] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 23:12:59,229] INFO [GroupCoordinator 5]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,230] INFO [AddPartitionsToTxnSenderThread-5]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 23:12:59,230] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 23:12:59,241] INFO [GroupCoordinator 5]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,251] INFO [TransactionCoordinator id=5] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 23:12:59,265] INFO [TransactionCoordinator id=5] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 23:12:59,275] INFO [TxnMarkerSenderThread-5]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 23:12:59,276] INFO [Broker id=5] Transitioning 51 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:12:59,284] INFO [Broker id=5] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,310] INFO [Partition __consumer_offsets-13 broker=5] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,319] INFO [Broker id=5] Follower __consumer_offsets-13 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,321] INFO [Broker id=5] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,324] INFO [Partition __consumer_offsets-46 broker=5] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,325] INFO [Broker id=5] Follower __consumer_offsets-46 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,327] INFO [Broker id=5] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,331] INFO [Partition __consumer_offsets-9 broker=5] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,332] INFO [Broker id=5] Follower __consumer_offsets-9 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,332] INFO [Broker id=5] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,335] INFO [Partition __consumer_offsets-42 broker=5] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,336] INFO [Broker id=5] Follower __consumer_offsets-42 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,337] INFO [Broker id=5] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,339] INFO [Partition __consumer_offsets-21 broker=5] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,341] INFO [Broker id=5] Follower __consumer_offsets-21 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,342] INFO [Broker id=5] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,343] INFO [Partition __consumer_offsets-17 broker=5] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,343] INFO [Broker id=5] Follower __consumer_offsets-17 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,344] INFO [Broker id=5] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,345] INFO [Partition __consumer_offsets-30 broker=5] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,346] INFO [Broker id=5] Follower __consumer_offsets-30 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,348] INFO [Broker id=5] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,351] INFO [Partition __consumer_offsets-26 broker=5] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,353] INFO [Broker id=5] Follower __consumer_offsets-26 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,353] INFO [Broker id=5] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,358] INFO [Partition __consumer_offsets-5 broker=5] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,358] INFO [Broker id=5] Follower __consumer_offsets-5 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,359] INFO [Broker id=5] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,360] INFO [Partition __consumer_offsets-38 broker=5] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,360] INFO [Broker id=5] Follower __consumer_offsets-38 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,361] INFO [Broker id=5] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,362] INFO [Partition __consumer_offsets-1 broker=5] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,363] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,364] INFO [Broker id=5] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,365] INFO [Partition __consumer_offsets-34 broker=5] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,370] INFO [Broker id=5] Follower __consumer_offsets-34 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,373] INFO [Broker id=5] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,376] INFO [Partition __consumer_offsets-16 broker=5] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,377] INFO [Broker id=5] Follower __consumer_offsets-16 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,378] INFO [Broker id=5] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-20 23:12:59,379] INFO [Partition _schemas-0 broker=5] Log loaded for partition _schemas-0 with initial high watermark 2 (kafka.cluster.Partition)
[2025-05-20 23:12:59,380] INFO [Broker id=5] Follower _schemas-0 starts at leader epoch 6 from offset 2 with partition epoch 11 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,381] INFO [Broker id=5] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,383] INFO [Partition __consumer_offsets-45 broker=5] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,385] INFO [Broker id=5] Follower __consumer_offsets-45 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,391] INFO [Broker id=5] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,394] INFO [Partition __consumer_offsets-12 broker=5] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,396] INFO [Broker id=5] Follower __consumer_offsets-12 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,397] INFO [Broker id=5] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,399] INFO [Partition __consumer_offsets-41 broker=5] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,401] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,402] INFO [Broker id=5] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,407] INFO [Partition __consumer_offsets-24 broker=5] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,408] INFO [Broker id=5] Follower __consumer_offsets-24 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,408] INFO [Broker id=5] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,410] INFO [Partition __consumer_offsets-20 broker=5] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,413] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,415] INFO [Broker id=5] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,416] INFO [Partition __consumer_offsets-49 broker=5] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,417] INFO [Broker id=5] Follower __consumer_offsets-49 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,418] INFO [Broker id=5] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,419] INFO [Partition __consumer_offsets-0 broker=5] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,421] INFO [Broker id=5] Follower __consumer_offsets-0 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,422] INFO [Broker id=5] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,425] INFO [Partition __consumer_offsets-29 broker=5] Log loaded for partition __consumer_offsets-29 with initial high watermark 2 (kafka.cluster.Partition)
[2025-05-20 23:12:59,426] INFO [Broker id=5] Follower __consumer_offsets-29 starts at leader epoch 6 from offset 2 with partition epoch 11 and high watermark 2. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,427] INFO [Broker id=5] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,430] INFO [Partition __consumer_offsets-25 broker=5] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,431] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,433] INFO [Broker id=5] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,451] INFO [Partition __consumer_offsets-8 broker=5] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,451] INFO [Broker id=5] Follower __consumer_offsets-8 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,452] INFO [Broker id=5] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,453] INFO [Partition __consumer_offsets-37 broker=5] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,455] INFO [Broker id=5] Follower __consumer_offsets-37 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,461] INFO [Broker id=5] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,469] INFO [Partition __consumer_offsets-4 broker=5] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,469] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,469] INFO [Broker id=5] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,471] INFO [Partition __consumer_offsets-33 broker=5] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,473] INFO [Broker id=5] Follower __consumer_offsets-33 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,474] INFO [Broker id=5] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,476] INFO [Partition __consumer_offsets-15 broker=5] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,476] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,477] INFO [Broker id=5] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,478] INFO [Partition __consumer_offsets-48 broker=5] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,478] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,479] INFO [Broker id=5] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,480] INFO [Partition __consumer_offsets-11 broker=5] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,481] INFO [Broker id=5] Follower __consumer_offsets-11 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,481] INFO [Broker id=5] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,483] INFO [Partition __consumer_offsets-44 broker=5] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,483] INFO [Broker id=5] Follower __consumer_offsets-44 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,483] INFO [Broker id=5] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,485] INFO [Partition __consumer_offsets-23 broker=5] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,489] INFO [Broker id=5] Follower __consumer_offsets-23 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,490] INFO [Broker id=5] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,492] INFO [Partition __consumer_offsets-19 broker=5] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,495] INFO [Broker id=5] Follower __consumer_offsets-19 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,496] INFO [Broker id=5] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,499] INFO [Partition __consumer_offsets-32 broker=5] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,500] INFO [Broker id=5] Follower __consumer_offsets-32 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,500] INFO [Broker id=5] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,504] INFO [Partition __consumer_offsets-28 broker=5] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,505] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,511] INFO [Broker id=5] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,512] INFO [Partition __consumer_offsets-7 broker=5] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,513] INFO [Broker id=5] Follower __consumer_offsets-7 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,513] INFO [Broker id=5] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,515] INFO [Partition __consumer_offsets-40 broker=5] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,518] INFO [Broker id=5] Follower __consumer_offsets-40 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,518] INFO [Broker id=5] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,520] INFO [Partition __consumer_offsets-3 broker=5] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,521] INFO [Broker id=5] Follower __consumer_offsets-3 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,521] INFO [Broker id=5] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,522] INFO [Partition __consumer_offsets-36 broker=5] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,523] INFO [Broker id=5] Follower __consumer_offsets-36 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,526] INFO [Broker id=5] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,530] INFO [Partition __consumer_offsets-47 broker=5] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,531] INFO [Broker id=5] Follower __consumer_offsets-47 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,532] INFO [Broker id=5] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,534] INFO [Partition __consumer_offsets-14 broker=5] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,534] INFO [Broker id=5] Follower __consumer_offsets-14 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,551] INFO [Broker id=5] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,557] INFO [Partition __consumer_offsets-43 broker=5] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,559] INFO [Broker id=5] Follower __consumer_offsets-43 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,560] INFO [Broker id=5] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,570] INFO [Partition __consumer_offsets-10 broker=5] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,573] INFO [Broker id=5] Follower __consumer_offsets-10 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,574] INFO [Broker id=5] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,575] INFO [Partition __consumer_offsets-22 broker=5] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,575] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 5 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:12:59,576] INFO [Broker id=5] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,577] INFO [Partition __consumer_offsets-18 broker=5] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,577] INFO [Broker id=5] Follower __consumer_offsets-18 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,578] INFO [Broker id=5] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,580] INFO [Partition __consumer_offsets-31 broker=5] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,581] INFO [Broker id=5] Follower __consumer_offsets-31 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,584] INFO [Broker id=5] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,587] INFO [Partition __consumer_offsets-27 broker=5] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,587] INFO [Broker id=5] Follower __consumer_offsets-27 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,589] INFO [Broker id=5] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,591] INFO [Partition __consumer_offsets-39 broker=5] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,592] INFO [Broker id=5] Follower __consumer_offsets-39 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,592] INFO [Broker id=5] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,594] INFO [Partition __consumer_offsets-6 broker=5] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,594] INFO [Broker id=5] Follower __consumer_offsets-6 starts at leader epoch 3 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:12:59,595] INFO [Broker id=5] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,597] INFO [Partition __consumer_offsets-35 broker=5] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,598] INFO [Broker id=5] Follower __consumer_offsets-35 starts at leader epoch 4 from offset 0 with partition epoch 10 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:12:59,602] INFO [Broker id=5] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-20 23:12:59,605] INFO [Partition __consumer_offsets-2 broker=5] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:12:59,607] INFO [Broker id=5] Follower __consumer_offsets-2 starts at leader epoch 6 from offset 0 with partition epoch 11 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:12:59,610] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:12:59,611] INFO [Broker id=5] Stopped fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 23:12:59,635] INFO [Broker id=5] Started fetchers as part of become-follower for 51 partitions (state.change.logger)
[2025-05-20 23:12:59,669] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,672] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,673] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,676] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,677] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,677] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,678] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,678] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,679] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,679] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,680] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,680] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,682] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,683] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,683] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,684] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,684] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,685] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,685] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,685] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,686] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,686] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,687] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,687] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,687] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,688] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,688] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,688] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,689] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,689] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,689] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,691] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,693] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,694] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,696] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,698] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,698] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,699] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,700] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,701] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,702] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,702] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,702] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,703] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,703] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,704] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,704] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,704] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,676] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,705] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,706] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,706] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,706] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,711] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,712] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,712] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,713] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,713] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,708] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,714] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,715] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,717] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,717] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,717] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,718] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,718] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,713] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,719] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,719] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,719] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,720] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,720] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,720] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,721] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,721] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,721] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,722] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,722] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,723] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,726] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,727] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,719] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,727] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,728] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,729] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,734] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,735] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,736] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,736] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,737] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,737] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,738] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,739] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,728] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,741] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,742] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,742] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,743] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,743] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,743] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,744] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,744] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,747] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,747] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,748] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,741] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,748] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,750] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,752] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,750] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,753] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,753] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,757] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,758] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,764] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,765] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,765] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,766] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,767] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,767] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,768] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,768] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[5] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,768] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,768] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,768] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,769] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,769] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,769] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,769] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,770] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,770] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,764] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,770] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[3] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,771] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,773] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[4] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,773] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,774] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:12:59,775] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,771] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,779] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,780] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,780] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,782] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[5]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,782] INFO [DynamicConfigPublisher broker id=5] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 23:12:59,782] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,787] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,787] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,787] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,788] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[3]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,788] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[4]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,789] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:12:59,802] INFO [DynamicConfigPublisher broker id=5] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-20 23:12:59,817] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=5) with a snapshot at offset 2743 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-20 23:12:59,876] INFO [BrokerLifecycleManager id=5] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:12:59,878] INFO [BrokerServer id=5] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-20 23:12:59,879] INFO [BrokerServer id=5] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 23:12:59,880] INFO [BrokerServer id=5] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-20 23:12:59,883] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-2:19092,PLAINTEXT_HOST://localhost:39092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 5
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 5
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-20 23:12:59,885] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-20 23:12:59,890] INFO [BrokerServer id=5] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 23:12:59,894] INFO [BrokerLifecycleManager id=5] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:13:00,025] INFO [BrokerLifecycleManager id=5] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:13:00,026] INFO [BrokerServer id=5] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-20 23:13:00,029] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 23:13:00,029] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-20 23:13:00,030] INFO [SocketServer listenerType=BROKER, nodeId=5] Enabling request processing. (kafka.network.SocketServer)
[2025-05-20 23:13:00,033] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 23:13:00,062] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-20 23:13:00,087] INFO [BrokerServer id=5] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 23:13:00,087] INFO [BrokerServer id=5] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-20 23:13:00,088] INFO [BrokerServer id=5] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 23:13:00,088] INFO [BrokerServer id=5] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-20 23:13:00,088] INFO [BrokerServer id=5] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-20 23:13:00,089] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 23:13:00,089] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 23:13:00,089] INFO Kafka startTimeMs: 1747782780089 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 23:13:00,093] INFO [KafkaRaftServer nodeId=5] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-20 23:13:00,120] INFO [Broker id=5] Transitioning 51 partition(s) to local leaders. (state.change.logger)
[2025-05-20 23:13:00,134] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:13:00,157] INFO [Broker id=5] Leader __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,166] INFO [Broker id=5] Leader __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,179] INFO [Broker id=5] Leader __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 11, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,189] INFO [Broker id=5] Leader __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 11, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,201] INFO [Broker id=5] Leader __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:13:00,219] INFO [Broker id=5] Leader __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:13:00,227] INFO [Broker id=5] Leader __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,248] INFO [Broker id=5] Leader __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,262] INFO [Broker id=5] Leader __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,273] INFO [Broker id=5] Leader __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 11, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,294] INFO [Broker id=5] Leader __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 6 from offset 0 with partition epoch 11, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,313] INFO [Broker id=5] Leader __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,328] INFO [Broker id=5] Leader __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,333] INFO [Broker id=5] Leader _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) starts at leader epoch 7 from offset 2 with partition epoch 12, high watermark 2, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,346] INFO [Broker id=5] Leader __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:13:00,361] INFO [Broker id=5] Leader __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:13:00,403] INFO [Broker id=5] Leader __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 6 from offset 0 with partition epoch 11, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,411] INFO [Broker id=5] Leader __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:13:00,422] INFO [Broker id=5] Leader __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 6 from offset 0 with partition epoch 11, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,437] INFO [Broker id=5] Leader __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,445] INFO [Broker id=5] Leader __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:13:00,450] INFO [Broker id=5] Leader __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 2 with partition epoch 12, high watermark 2, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,455] INFO [Broker id=5] Leader __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 6 from offset 0 with partition epoch 11, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,461] INFO [Broker id=5] Leader __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 11, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,469] INFO [Broker id=5] Leader __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,482] INFO [Broker id=5] Leader __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 6 from offset 0 with partition epoch 11, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,496] INFO [Broker id=5] Leader __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:13:00,506] INFO [Broker id=5] Leader __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 6 from offset 0 with partition epoch 11, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,512] INFO [Broker id=5] Leader __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 6 from offset 0 with partition epoch 11, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,518] INFO [Broker id=5] Leader __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,524] INFO [Broker id=5] Leader __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,540] INFO [Broker id=5] Leader __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,546] INFO [Broker id=5] Leader __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:13:00,556] INFO [NodeToControllerChannelManager id=5 name=alter-partition] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:13:00,557] INFO [broker-5-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:13:00,579] INFO [Broker id=5] Leader __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:13:00,596] INFO [Broker id=5] Leader __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 6 from offset 0 with partition epoch 11, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,625] INFO [Partition __consumer_offsets-13 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,639] INFO [Broker id=5] Leader __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,649] INFO [Broker id=5] Leader __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:13:00,656] INFO [Broker id=5] Leader __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:13:00,662] INFO [Broker id=5] Leader __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:13:00,669] INFO [Broker id=5] Leader __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 11, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,697] INFO [Partition __consumer_offsets-15 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-20 23:13:00,699] INFO [Partition __consumer_offsets-48 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-20 23:13:00,701] INFO [Partition __consumer_offsets-46 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,703] INFO [Partition __consumer_offsets-11 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,703] INFO [Partition __consumer_offsets-44 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,704] INFO [Partition __consumer_offsets-9 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-20 23:13:00,704] INFO [Partition __consumer_offsets-42 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-20 23:13:00,705] INFO [Partition __consumer_offsets-23 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,705] INFO [Partition __consumer_offsets-21 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,706] INFO [Partition __consumer_offsets-17 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,698] INFO [Broker id=5] Leader __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 11, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,706] INFO [Partition __consumer_offsets-30 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,708] INFO [Partition __consumer_offsets-26 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,708] INFO [Partition __consumer_offsets-5 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,709] INFO [Partition __consumer_offsets-38 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-20 23:13:00,709] INFO [Partition __consumer_offsets-1 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-20 23:13:00,710] INFO [Partition __consumer_offsets-34 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,711] INFO [Partition __consumer_offsets-16 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,712] INFO [Partition _schemas-0 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,712] INFO [Partition __consumer_offsets-45 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,712] INFO [Partition __consumer_offsets-12 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,713] INFO [Partition __consumer_offsets-41 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-20 23:13:00,714] INFO [Partition __consumer_offsets-24 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,715] INFO [Partition __consumer_offsets-20 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-20 23:13:00,715] INFO [Broker id=5] Leader __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:13:00,720] INFO [Partition __consumer_offsets-49 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,721] INFO [Partition __consumer_offsets-0 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,721] INFO [Partition __consumer_offsets-29 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,722] INFO [Partition __consumer_offsets-25 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-20 23:13:00,722] INFO [Partition __consumer_offsets-8 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-20 23:13:00,722] INFO [Partition __consumer_offsets-37 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,723] INFO [Partition __consumer_offsets-4 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-20 23:13:00,723] INFO [Partition __consumer_offsets-33 broker=5] ISR updated to 5,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:00,725] INFO [Broker id=5] Leader __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:13:00,734] INFO [Broker id=5] Leader __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 6 from offset 0 with partition epoch 11, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-20 23:13:00,740] INFO [Broker id=5] Leader __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,749] INFO [Broker id=5] Leader __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 11, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,764] INFO [Broker id=5] Leader __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:13:00,769] INFO [Broker id=5] Leader __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,776] INFO [Broker id=5] Leader __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 4 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:13:00,789] INFO [Broker id=5] Leader __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 5 from offset 0 with partition epoch 11, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-20 23:13:00,797] INFO [Broker id=5] Leader __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 7 from offset 0 with partition epoch 12, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:13:00,810] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 13 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,811] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,812] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 46 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,813] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,814] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 9 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,814] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,814] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 42 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,818] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,818] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 21 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,819] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,819] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 17 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,819] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,820] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 30 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,820] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,821] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 26 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,821] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,821] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 5 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,821] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,822] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 38 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,822] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,822] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 1 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,822] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,822] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-13 in 10 milliseconds for epoch 7, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,823] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 34 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,823] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,824] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 16 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,824] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,824] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 45 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,823] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-46 in 10 milliseconds for epoch 7, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,825] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,826] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-9 in 12 milliseconds for epoch 5, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,826] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 12 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,827] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,827] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 41 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,827] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-42 in 9 milliseconds for epoch 5, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,827] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,829] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 24 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,829] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-21 in 10 milliseconds for epoch 4, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,829] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,830] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-17 in 10 milliseconds for epoch 4, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,831] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 20 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,832] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,832] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 49 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,833] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,834] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 0 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,834] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,838] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 29 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,840] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,840] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 25 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,841] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,841] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 8 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,842] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,845] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 37 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,846] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,847] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 4 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,849] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,851] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 33 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,852] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,854] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 15 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,857] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,857] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 48 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,857] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,858] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 11 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,859] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,859] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 44 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,859] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,860] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 23 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,860] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,861] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 19 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,861] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,862] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 32 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,862] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,863] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 28 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,866] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,867] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 7 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,867] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,868] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 40 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,869] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,870] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 3 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,873] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,874] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 36 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,875] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,832] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-30 in 11 milliseconds for epoch 7, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,875] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 47 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,876] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,876] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-26 in 55 milliseconds for epoch 7, of which 55 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,876] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 14 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,877] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,879] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 43 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,879] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,877] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-5 in 55 milliseconds for epoch 7, of which 55 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,880] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 10 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,881] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,881] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 22 in epoch 6 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,881] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-38 in 59 milliseconds for epoch 5, of which 58 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,882] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,882] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-1 in 59 milliseconds for epoch 6, of which 59 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,883] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 18 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,883] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,884] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 31 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,885] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,883] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-34 in 59 milliseconds for epoch 7, of which 59 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,886] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-16 in 62 milliseconds for epoch 7, of which 62 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,886] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 27 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,891] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,892] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 39 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,892] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,893] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 6 in epoch 4 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,893] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,891] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-45 in 65 milliseconds for epoch 4, of which 65 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,893] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 35 in epoch 5 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,894] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,895] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 2 in epoch 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,895] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,895] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-12 in 68 milliseconds for epoch 4, of which 67 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,899] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-41 in 70 milliseconds for epoch 6, of which 70 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,900] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-24 in 69 milliseconds for epoch 4, of which 69 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,901] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-20 in 69 milliseconds for epoch 6, of which 68 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,901] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-49 in 67 milliseconds for epoch 7, of which 67 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,902] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-0 in 64 milliseconds for epoch 4, of which 64 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,897] INFO [Broker id=5] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-20 23:13:00,906] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-13) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:13:00,912] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:00,930] INFO Loaded member MemberMetadata(memberId=sr-1-99122467-ec1a-4fc4-88f0-59fda8f60096, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 1. (kafka.coordinator.group.GroupMetadata$)
[2025-05-20 23:13:00,932] INFO [GroupCoordinator 5]: Loading group metadata for schema-registry with generation 2 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:00,934] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-29 in 94 milliseconds for epoch 7, of which 62 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,935] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-25 in 94 milliseconds for epoch 6, of which 93 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,935] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-8 in 90 milliseconds for epoch 5, of which 90 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,935] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-37 in 88 milliseconds for epoch 7, of which 88 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,936] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-4 in 85 milliseconds for epoch 6, of which 85 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,940] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-33 in 85 milliseconds for epoch 4, of which 85 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,942] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-15 in 85 milliseconds for epoch 6, of which 85 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,942] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-48 in 84 milliseconds for epoch 6, of which 84 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,943] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-11 in 84 milliseconds for epoch 7, of which 84 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,944] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-44 in 84 milliseconds for epoch 7, of which 83 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,944] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-23 in 83 milliseconds for epoch 7, of which 83 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,951] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-19 in 89 milliseconds for epoch 4, of which 89 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,952] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-32 in 89 milliseconds for epoch 4, of which 89 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,953] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-28 in 86 milliseconds for epoch 6, of which 85 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,953] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-7 in 85 milliseconds for epoch 7, of which 85 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,954] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-40 in 84 milliseconds for epoch 4, of which 84 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,954] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-3 in 80 milliseconds for epoch 4, of which 80 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,955] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-36 in 80 milliseconds for epoch 4, of which 80 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,956] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-47 in 80 milliseconds for epoch 5, of which 80 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,956] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-14 in 77 milliseconds for epoch 5, of which 77 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,957] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-43 in 77 milliseconds for epoch 4, of which 77 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,958] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-10 in 77 milliseconds for epoch 4, of which 76 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,958] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-22 in 75 milliseconds for epoch 6, of which 75 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,959] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-18 in 75 milliseconds for epoch 7, of which 74 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,959] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-31 in 73 milliseconds for epoch 5, of which 73 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,962] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-27 in 70 milliseconds for epoch 4, of which 70 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,967] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-39 in 75 milliseconds for epoch 7, of which 74 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,979] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-6 in 74 milliseconds for epoch 4, of which 74 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,980] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-35 in 84 milliseconds for epoch 5, of which 84 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:00,980] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-2 in 85 milliseconds for epoch 7, of which 85 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:13:01,171] INFO [Broker id=5] Transitioning 31 partition(s) to local leaders. (state.change.logger)
[2025-05-20 23:13:01,172] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:13:01,172] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,173] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,174] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,174] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,175] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,175] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,176] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,177] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,178] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,179] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,179] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,180] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,181] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,181] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,182] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,182] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,183] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,186] INFO [Broker id=5] Skipped the become-leader state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 2, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,188] INFO [Partition __consumer_offsets-13 broker=5] ISR updated to 5,6,4  and version updated to 14 (kafka.cluster.Partition)
[2025-05-20 23:13:01,190] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,192] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,194] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,195] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,196] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,196] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,197] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,198] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 2, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,199] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,200] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,203] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,204] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,208] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,233] INFO [Broker id=5] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-20 23:13:01,234] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-13) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:13:01,237] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,657] INFO [Broker id=5] Transitioning 25 partition(s) to local leaders. (state.change.logger)
[2025-05-20 23:13:01,658] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:13:01,660] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,667] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,672] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,673] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,673] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,674] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,675] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,676] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,676] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,676] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,677] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,678] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,678] INFO [Broker id=5] Skipped the become-leader state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 2, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,679] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,679] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,707] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,710] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,715] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,723] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,725] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,735] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 2, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,736] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,738] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,753] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,754] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:01,850] INFO [Partition __consumer_offsets-33 broker=5] ISR updated to 5,6,4  and version updated to 14 (kafka.cluster.Partition)
[2025-05-20 23:13:01,851] INFO [Partition __consumer_offsets-15 broker=5] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:01,853] INFO [Partition __consumer_offsets-48 broker=5] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:01,854] INFO [Partition __consumer_offsets-11 broker=5] ISR updated to 5,6,4  and version updated to 14 (kafka.cluster.Partition)
[2025-05-20 23:13:01,855] INFO [Partition __consumer_offsets-44 broker=5] ISR updated to 5,6,4  and version updated to 14 (kafka.cluster.Partition)
[2025-05-20 23:13:01,855] INFO [Partition __consumer_offsets-23 broker=5] ISR updated to 5,6,4  and version updated to 14 (kafka.cluster.Partition)
[2025-05-20 23:13:01,856] INFO [Partition __consumer_offsets-19 broker=5] ISR updated to 5,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:01,857] INFO [Partition __consumer_offsets-32 broker=5] ISR updated to 5,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:01,859] INFO [Partition __consumer_offsets-28 broker=5] ISR updated to 5,4  and version updated to 12 (kafka.cluster.Partition)
[2025-05-20 23:13:01,860] INFO [Partition __consumer_offsets-7 broker=5] ISR updated to 5,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:01,860] INFO [Partition __consumer_offsets-40 broker=5] ISR updated to 5,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:01,861] INFO [Partition __consumer_offsets-3 broker=5] ISR updated to 5,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:01,861] INFO [Partition __consumer_offsets-36 broker=5] ISR updated to 5,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:01,861] INFO [Partition __consumer_offsets-47 broker=5] ISR updated to 5,4  and version updated to 12 (kafka.cluster.Partition)
[2025-05-20 23:13:01,862] INFO [Partition __consumer_offsets-14 broker=5] ISR updated to 5,4  and version updated to 12 (kafka.cluster.Partition)
[2025-05-20 23:13:01,862] INFO [Partition __consumer_offsets-43 broker=5] ISR updated to 5,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:01,862] INFO [Partition __consumer_offsets-10 broker=5] ISR updated to 5,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:01,863] INFO [Partition __consumer_offsets-22 broker=5] ISR updated to 5,4  and version updated to 12 (kafka.cluster.Partition)
[2025-05-20 23:13:01,863] INFO [Partition __consumer_offsets-18 broker=5] ISR updated to 5,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:01,864] INFO [Partition __consumer_offsets-31 broker=5] ISR updated to 5,4  and version updated to 12 (kafka.cluster.Partition)
[2025-05-20 23:13:01,864] INFO [Partition __consumer_offsets-27 broker=5] ISR updated to 5,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:01,868] INFO [Partition __consumer_offsets-39 broker=5] ISR updated to 5,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:01,870] INFO [Partition __consumer_offsets-6 broker=5] ISR updated to 5,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:01,870] INFO [Partition __consumer_offsets-35 broker=5] ISR updated to 5,4  and version updated to 12 (kafka.cluster.Partition)
[2025-05-20 23:13:01,871] INFO [Partition __consumer_offsets-2 broker=5] ISR updated to 5,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:02,153] INFO [Broker id=5] Transitioning 25 partition(s) to local leaders. (state.change.logger)
[2025-05-20 23:13:02,155] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:13:02,157] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,159] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,161] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,161] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,162] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,179] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,183] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,184] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,185] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,186] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,186] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,187] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,187] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,188] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,188] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,189] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,190] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,191] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,192] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,192] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,193] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,194] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,194] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,195] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 6, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,196] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,314] INFO [Partition __consumer_offsets-47 broker=5] ISR updated to 5,4,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:02,350] INFO [Broker id=5] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-20 23:13:02,351] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-47) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:13:02,353] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,368] INFO [Partition __consumer_offsets-19 broker=5] ISR updated to 5,4,6  and version updated to 14 (kafka.cluster.Partition)
[2025-05-20 23:13:02,369] INFO [Partition __consumer_offsets-32 broker=5] ISR updated to 5,4,6  and version updated to 14 (kafka.cluster.Partition)
[2025-05-20 23:13:02,369] INFO [Partition __consumer_offsets-28 broker=5] ISR updated to 5,4,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:02,370] INFO [Partition __consumer_offsets-7 broker=5] ISR updated to 5,4,6  and version updated to 14 (kafka.cluster.Partition)
[2025-05-20 23:13:02,371] INFO [Partition __consumer_offsets-40 broker=5] ISR updated to 5,4,6  and version updated to 14 (kafka.cluster.Partition)
[2025-05-20 23:13:02,371] INFO [Partition __consumer_offsets-3 broker=5] ISR updated to 5,4,6  and version updated to 14 (kafka.cluster.Partition)
[2025-05-20 23:13:02,372] INFO [Partition __consumer_offsets-36 broker=5] ISR updated to 5,4,6  and version updated to 14 (kafka.cluster.Partition)
[2025-05-20 23:13:02,372] INFO [Partition __consumer_offsets-14 broker=5] ISR updated to 5,4,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:02,372] INFO [Partition __consumer_offsets-43 broker=5] ISR updated to 5,4,6  and version updated to 14 (kafka.cluster.Partition)
[2025-05-20 23:13:02,373] INFO [Partition __consumer_offsets-10 broker=5] ISR updated to 5,4,6  and version updated to 14 (kafka.cluster.Partition)
[2025-05-20 23:13:02,373] INFO [Partition __consumer_offsets-22 broker=5] ISR updated to 5,4,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:02,373] INFO [Partition __consumer_offsets-18 broker=5] ISR updated to 5,4,6  and version updated to 14 (kafka.cluster.Partition)
[2025-05-20 23:13:02,374] INFO [Partition __consumer_offsets-31 broker=5] ISR updated to 5,4,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:02,374] INFO [Partition __consumer_offsets-27 broker=5] ISR updated to 5,4,6  and version updated to 14 (kafka.cluster.Partition)
[2025-05-20 23:13:02,375] INFO [Partition __consumer_offsets-39 broker=5] ISR updated to 5,4,6  and version updated to 14 (kafka.cluster.Partition)
[2025-05-20 23:13:02,375] INFO [Partition __consumer_offsets-6 broker=5] ISR updated to 5,4,6  and version updated to 14 (kafka.cluster.Partition)
[2025-05-20 23:13:02,375] INFO [Partition __consumer_offsets-35 broker=5] ISR updated to 5,4,6  and version updated to 13 (kafka.cluster.Partition)
[2025-05-20 23:13:02,375] INFO [Partition __consumer_offsets-2 broker=5] ISR updated to 5,4,6  and version updated to 14 (kafka.cluster.Partition)
[2025-05-20 23:13:02,652] INFO [Broker id=5] Transitioning 18 partition(s) to local leaders. (state.change.logger)
[2025-05-20 23:13:02,653] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-19, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-32, __consumer_offsets-27, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-39, __consumer_offsets-40, __consumer_offsets-6, __consumer_offsets-3, __consumer_offsets-35, __consumer_offsets-36, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:13:02,655] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,656] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,657] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,657] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4, 6], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,660] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,663] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4, 6], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,664] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,665] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,666] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,666] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4, 6], partitionEpoch=13, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,667] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4, 6], partitionEpoch=14, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,667] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4, 6], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,668] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,669] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,670] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,670] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 4, 6], partitionEpoch=13, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,671] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4, 6], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:02,671] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 4, 6], partitionEpoch=14, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-20 23:13:05,994] INFO [GroupCoordinator 5]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-3027da35-fa63-4fb3-9cba-2acfe293f9a1 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:06,005] INFO [GroupCoordinator 5]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 2 (__consumer_offsets-29) (reason: Adding new member sr-1-3027da35-fa63-4fb3-9cba-2acfe293f9a1 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:06,010] INFO [GroupCoordinator 5]: Stabilized group schema-registry generation 3 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:13:06,042] INFO [GroupCoordinator 5]: Assignment received from leader sr-1-3027da35-fa63-4fb3-9cba-2acfe293f9a1 for group schema-registry for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,238] INFO [Broker id=5] Transitioning 35 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:17:57,239] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,239] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,240] INFO [Broker id=5] Follower __consumer_offsets-13 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,240] INFO [Broker id=5] Follower __consumer_offsets-46 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,240] INFO [Broker id=5] Follower __consumer_offsets-11 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,240] INFO [Broker id=5] Follower __consumer_offsets-44 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,241] INFO [Broker id=5] Follower __consumer_offsets-9 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,241] INFO [Broker id=5] Follower __consumer_offsets-42 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,241] INFO [Broker id=5] Follower __consumer_offsets-23 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,241] INFO [Broker id=5] Follower __consumer_offsets-30 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,242] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,242] INFO [Broker id=5] Follower __consumer_offsets-26 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,242] INFO [Broker id=5] Follower __consumer_offsets-7 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,242] INFO [Broker id=5] Follower __consumer_offsets-5 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,243] INFO [Broker id=5] Follower __consumer_offsets-38 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,243] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,243] INFO [Broker id=5] Follower __consumer_offsets-34 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,243] INFO [Broker id=5] Follower __consumer_offsets-47 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,244] INFO [Broker id=5] Follower __consumer_offsets-16 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,244] INFO [Broker id=5] Follower _schemas-0 starts at leader epoch 8 from offset 4 with partition epoch 15 and high watermark 4. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,244] INFO [Broker id=5] Follower __consumer_offsets-14 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,244] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,245] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,245] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,245] INFO [Broker id=5] Follower __consumer_offsets-49 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,246] INFO [Broker id=5] Follower __consumer_offsets-18 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,246] INFO [Broker id=5] Follower __consumer_offsets-31 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,246] INFO [Broker id=5] Follower __consumer_offsets-29 starts at leader epoch 8 from offset 3 with partition epoch 15 and high watermark 3. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,246] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,247] INFO [Broker id=5] Follower __consumer_offsets-39 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,247] INFO [Broker id=5] Follower __consumer_offsets-8 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,247] INFO [Broker id=5] Follower __consumer_offsets-37 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,247] INFO [Broker id=5] Follower __consumer_offsets-35 starts at leader epoch 6 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:17:57,248] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 7 from offset 0 with partition epoch 14 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:17:57,248] INFO [Broker id=5] Follower __consumer_offsets-2 starts at leader epoch 8 from offset 0 with partition epoch 15 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:17:57,248] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-47, __consumer_offsets-16, _schemas-0, __consumer_offsets-14, __consumer_offsets-41, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-35, __consumer_offsets-4, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:17:57,249] INFO [Broker id=5] Stopped fetchers as part of become-follower for 35 partitions (state.change.logger)
[2025-05-20 23:17:57,263] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,265] INFO [ReplicaFetcherManager on broker 5] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), _schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,4), __consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,3), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:17:57,268] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:17:57,268] INFO [ReplicaFetcherManager on broker 5] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-47 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-48 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-14 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-41 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-9 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-42 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-22 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-20 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-31 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-28 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-25 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-8 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-38 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-35 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,0), __consumer_offsets-4 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-1 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:17:57,268] INFO [Broker id=5] Started fetchers as part of become-follower for 35 partitions (state.change.logger)
[2025-05-20 23:17:57,286] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,286] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,287] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,287] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,287] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,288] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,289] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,289] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,289] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,289] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,290] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,291] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,291] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,291] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,292] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,293] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,293] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,294] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,294] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,294] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,295] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,295] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,295] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,296] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,297] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,297] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,298] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,298] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,298] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,298] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,299] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,299] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,300] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,300] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,300] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,300] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,301] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,300] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,305] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,305] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,305] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,305] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,306] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,306] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,307] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,308] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,309] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,307] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,309] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,309] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,310] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,310] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,310] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,311] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,311] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,312] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,312] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,312] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,313] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,313] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,313] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,314] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,314] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,314] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,314] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,315] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,315] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,315] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,315] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,316] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,316] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,317] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,317] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,317] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,319] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,319] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,320] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,320] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,320] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,320] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,321] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,321] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,321] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,322] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,322] INFO [GroupCoordinator 5]: Unloading group metadata for schema-registry with generation 3 (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,322] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,322] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,323] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,323] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,323] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,323] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,324] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,324] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,324] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:17:57,324] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,324] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,325] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,325] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,326] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,326] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,327] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,327] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:17:57,327] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:22:56,583] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:22:56,583] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:23:02,428] INFO [NodeToControllerChannelManager id=5 name=alter-partition] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:31:58,482] INFO [Broker id=5] Transitioning 7 partition(s) to local leaders. (state.change.logger)
[2025-05-20 23:31:58,484] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-14, financial_transactions-15, financial_transactions-2, financial_transactions-19, financial_transactions-4, financial_transactions-6, financial_transactions-11) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:31:58,485] INFO [Broker id=5] Creating new partition financial_transactions-14 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,500] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,503] INFO Created log for partition financial_transactions-14 in /tmp/kafka-logs/financial_transactions-14 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,505] INFO [Partition financial_transactions-14 broker=5] No checkpointed highwatermark is found for partition financial_transactions-14 (kafka.cluster.Partition)
[2025-05-20 23:31:58,509] INFO [Partition financial_transactions-14 broker=5] Log loaded for partition financial_transactions-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,510] INFO [Broker id=5] Leader financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,516] INFO [Broker id=5] Creating new partition financial_transactions-15 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,521] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,522] INFO Created log for partition financial_transactions-15 in /tmp/kafka-logs/financial_transactions-15 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,522] INFO [Partition financial_transactions-15 broker=5] No checkpointed highwatermark is found for partition financial_transactions-15 (kafka.cluster.Partition)
[2025-05-20 23:31:58,523] INFO [Partition financial_transactions-15 broker=5] Log loaded for partition financial_transactions-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,524] INFO [Broker id=5] Leader financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,531] INFO [Broker id=5] Creating new partition financial_transactions-2 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,536] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,537] INFO Created log for partition financial_transactions-2 in /tmp/kafka-logs/financial_transactions-2 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,537] INFO [Partition financial_transactions-2 broker=5] No checkpointed highwatermark is found for partition financial_transactions-2 (kafka.cluster.Partition)
[2025-05-20 23:31:58,537] INFO [Partition financial_transactions-2 broker=5] Log loaded for partition financial_transactions-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,538] INFO [Broker id=5] Leader financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,543] INFO [Broker id=5] Creating new partition financial_transactions-19 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,546] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,547] INFO Created log for partition financial_transactions-19 in /tmp/kafka-logs/financial_transactions-19 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,548] INFO [Partition financial_transactions-19 broker=5] No checkpointed highwatermark is found for partition financial_transactions-19 (kafka.cluster.Partition)
[2025-05-20 23:31:58,548] INFO [Partition financial_transactions-19 broker=5] Log loaded for partition financial_transactions-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,548] INFO [Broker id=5] Leader financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,559] INFO [Broker id=5] Creating new partition financial_transactions-4 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,564] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,565] INFO Created log for partition financial_transactions-4 in /tmp/kafka-logs/financial_transactions-4 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,565] INFO [Partition financial_transactions-4 broker=5] No checkpointed highwatermark is found for partition financial_transactions-4 (kafka.cluster.Partition)
[2025-05-20 23:31:58,565] INFO [Partition financial_transactions-4 broker=5] Log loaded for partition financial_transactions-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,566] INFO [Broker id=5] Leader financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,572] INFO [Broker id=5] Creating new partition financial_transactions-6 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,577] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,578] INFO Created log for partition financial_transactions-6 in /tmp/kafka-logs/financial_transactions-6 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,578] INFO [Partition financial_transactions-6 broker=5] No checkpointed highwatermark is found for partition financial_transactions-6 (kafka.cluster.Partition)
[2025-05-20 23:31:58,579] INFO [Partition financial_transactions-6 broker=5] Log loaded for partition financial_transactions-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,580] INFO [Broker id=5] Leader financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,590] INFO [Broker id=5] Creating new partition financial_transactions-11 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,594] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,595] INFO Created log for partition financial_transactions-11 in /tmp/kafka-logs/financial_transactions-11 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,596] INFO [Partition financial_transactions-11 broker=5] No checkpointed highwatermark is found for partition financial_transactions-11 (kafka.cluster.Partition)
[2025-05-20 23:31:58,597] INFO [Partition financial_transactions-11 broker=5] Log loaded for partition financial_transactions-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,598] INFO [Broker id=5] Leader financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:31:58,604] INFO [Broker id=5] Transitioning 13 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:31:58,605] INFO [Broker id=5] Creating new partition financial_transactions-13 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,613] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,614] INFO Created log for partition financial_transactions-13 in /tmp/kafka-logs/financial_transactions-13 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,614] INFO [Partition financial_transactions-13 broker=5] No checkpointed highwatermark is found for partition financial_transactions-13 (kafka.cluster.Partition)
[2025-05-20 23:31:58,615] INFO [Partition financial_transactions-13 broker=5] Log loaded for partition financial_transactions-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,615] INFO [Broker id=5] Follower financial_transactions-13 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,615] INFO [Broker id=5] Creating new partition financial_transactions-16 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,624] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,630] INFO Created log for partition financial_transactions-16 in /tmp/kafka-logs/financial_transactions-16 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,631] INFO [Partition financial_transactions-16 broker=5] No checkpointed highwatermark is found for partition financial_transactions-16 (kafka.cluster.Partition)
[2025-05-20 23:31:58,631] INFO [Partition financial_transactions-16 broker=5] Log loaded for partition financial_transactions-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,631] INFO [Broker id=5] Follower financial_transactions-16 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,632] INFO [Broker id=5] Creating new partition financial_transactions-17 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,635] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,636] INFO Created log for partition financial_transactions-17 in /tmp/kafka-logs/financial_transactions-17 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,637] INFO [Partition financial_transactions-17 broker=5] No checkpointed highwatermark is found for partition financial_transactions-17 (kafka.cluster.Partition)
[2025-05-20 23:31:58,637] INFO [Partition financial_transactions-17 broker=5] Log loaded for partition financial_transactions-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,639] INFO [Broker id=5] Follower financial_transactions-17 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,639] INFO [Broker id=5] Creating new partition financial_transactions-18 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,646] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,648] INFO Created log for partition financial_transactions-18 in /tmp/kafka-logs/financial_transactions-18 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,649] INFO [Partition financial_transactions-18 broker=5] No checkpointed highwatermark is found for partition financial_transactions-18 (kafka.cluster.Partition)
[2025-05-20 23:31:58,649] INFO [Partition financial_transactions-18 broker=5] Log loaded for partition financial_transactions-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,650] INFO [Broker id=5] Follower financial_transactions-18 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,651] INFO [Broker id=5] Creating new partition financial_transactions-0 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,655] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,656] INFO Created log for partition financial_transactions-0 in /tmp/kafka-logs/financial_transactions-0 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,657] INFO [Partition financial_transactions-0 broker=5] No checkpointed highwatermark is found for partition financial_transactions-0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,657] INFO [Partition financial_transactions-0 broker=5] Log loaded for partition financial_transactions-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,657] INFO [Broker id=5] Follower financial_transactions-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,658] INFO [Broker id=5] Creating new partition financial_transactions-1 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,662] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,663] INFO Created log for partition financial_transactions-1 in /tmp/kafka-logs/financial_transactions-1 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,664] INFO [Partition financial_transactions-1 broker=5] No checkpointed highwatermark is found for partition financial_transactions-1 (kafka.cluster.Partition)
[2025-05-20 23:31:58,664] INFO [Partition financial_transactions-1 broker=5] Log loaded for partition financial_transactions-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,664] INFO [Broker id=5] Follower financial_transactions-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,665] INFO [Broker id=5] Creating new partition financial_transactions-3 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,669] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,671] INFO Created log for partition financial_transactions-3 in /tmp/kafka-logs/financial_transactions-3 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,672] INFO [Partition financial_transactions-3 broker=5] No checkpointed highwatermark is found for partition financial_transactions-3 (kafka.cluster.Partition)
[2025-05-20 23:31:58,673] INFO [Partition financial_transactions-3 broker=5] Log loaded for partition financial_transactions-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,675] INFO [Broker id=5] Follower financial_transactions-3 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,676] INFO [Broker id=5] Creating new partition financial_transactions-5 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,679] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,681] INFO Created log for partition financial_transactions-5 in /tmp/kafka-logs/financial_transactions-5 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,683] INFO [Partition financial_transactions-5 broker=5] No checkpointed highwatermark is found for partition financial_transactions-5 (kafka.cluster.Partition)
[2025-05-20 23:31:58,684] INFO [Partition financial_transactions-5 broker=5] Log loaded for partition financial_transactions-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,684] INFO [Broker id=5] Follower financial_transactions-5 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,685] INFO [Broker id=5] Creating new partition financial_transactions-7 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,695] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,697] INFO Created log for partition financial_transactions-7 in /tmp/kafka-logs/financial_transactions-7 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,698] INFO [Partition financial_transactions-7 broker=5] No checkpointed highwatermark is found for partition financial_transactions-7 (kafka.cluster.Partition)
[2025-05-20 23:31:58,698] INFO [Partition financial_transactions-7 broker=5] Log loaded for partition financial_transactions-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,698] INFO [Broker id=5] Follower financial_transactions-7 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,699] INFO [Broker id=5] Creating new partition financial_transactions-8 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,702] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,703] INFO Created log for partition financial_transactions-8 in /tmp/kafka-logs/financial_transactions-8 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,704] INFO [Partition financial_transactions-8 broker=5] No checkpointed highwatermark is found for partition financial_transactions-8 (kafka.cluster.Partition)
[2025-05-20 23:31:58,704] INFO [Partition financial_transactions-8 broker=5] Log loaded for partition financial_transactions-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,705] INFO [Broker id=5] Follower financial_transactions-8 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,705] INFO [Broker id=5] Creating new partition financial_transactions-9 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,713] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,715] INFO Created log for partition financial_transactions-9 in /tmp/kafka-logs/financial_transactions-9 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,716] INFO [Partition financial_transactions-9 broker=5] No checkpointed highwatermark is found for partition financial_transactions-9 (kafka.cluster.Partition)
[2025-05-20 23:31:58,716] INFO [Partition financial_transactions-9 broker=5] Log loaded for partition financial_transactions-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,716] INFO [Broker id=5] Follower financial_transactions-9 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,717] INFO [Broker id=5] Creating new partition financial_transactions-10 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,720] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,722] INFO Created log for partition financial_transactions-10 in /tmp/kafka-logs/financial_transactions-10 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,723] INFO [Partition financial_transactions-10 broker=5] No checkpointed highwatermark is found for partition financial_transactions-10 (kafka.cluster.Partition)
[2025-05-20 23:31:58,726] INFO [Partition financial_transactions-10 broker=5] Log loaded for partition financial_transactions-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,729] INFO [Broker id=5] Follower financial_transactions-10 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,730] INFO [Broker id=5] Creating new partition financial_transactions-12 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-20 23:31:58,742] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:31:58,743] INFO Created log for partition financial_transactions-12 in /tmp/kafka-logs/financial_transactions-12 with properties {} (kafka.log.LogManager)
[2025-05-20 23:31:58,744] INFO [Partition financial_transactions-12 broker=5] No checkpointed highwatermark is found for partition financial_transactions-12 (kafka.cluster.Partition)
[2025-05-20 23:31:58,744] INFO [Partition financial_transactions-12 broker=5] Log loaded for partition financial_transactions-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:31:58,744] INFO [Broker id=5] Follower financial_transactions-12 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 0. (state.change.logger)
[2025-05-20 23:31:58,745] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-13, financial_transactions-16, financial_transactions-17, financial_transactions-18, financial_transactions-0, financial_transactions-1, financial_transactions-3, financial_transactions-5, financial_transactions-7, financial_transactions-8, financial_transactions-9, financial_transactions-10, financial_transactions-12) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:31:58,746] INFO [Broker id=5] Stopped fetchers as part of become-follower for 13 partitions (state.change.logger)
[2025-05-20 23:31:58,747] INFO [ReplicaFetcherManager on broker 5] Added fetcher to broker 4 for partitions HashMap(financial_transactions-13 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), financial_transactions-16 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), financial_transactions-1 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), financial_transactions-5 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), financial_transactions-8 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0), financial_transactions-9 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:31:58,748] INFO [ReplicaFetcherManager on broker 5] Added fetcher to broker 6 for partitions HashMap(financial_transactions-0 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), financial_transactions-17 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), financial_transactions-18 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), financial_transactions-3 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), financial_transactions-7 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), financial_transactions-10 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0), financial_transactions-12 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),0,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:31:58,748] INFO [Broker id=5] Started fetchers as part of become-follower for 13 partitions (state.change.logger)
[2025-05-20 23:31:58,754] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition financial_transactions-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:58,756] INFO [UnifiedLog partition=financial_transactions-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:58,759] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition financial_transactions-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:58,759] INFO [UnifiedLog partition=financial_transactions-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:58,760] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition financial_transactions-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:58,761] INFO [UnifiedLog partition=financial_transactions-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:58,761] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition financial_transactions-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:58,761] INFO [UnifiedLog partition=financial_transactions-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:58,762] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition financial_transactions-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:58,762] INFO [UnifiedLog partition=financial_transactions-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:58,763] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition financial_transactions-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:58,763] INFO [UnifiedLog partition=financial_transactions-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,220] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition financial_transactions-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,221] INFO [UnifiedLog partition=financial_transactions-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,221] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition financial_transactions-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,222] INFO [UnifiedLog partition=financial_transactions-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,222] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition financial_transactions-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,223] INFO [UnifiedLog partition=financial_transactions-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,223] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition financial_transactions-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,224] INFO [UnifiedLog partition=financial_transactions-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,224] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition financial_transactions-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,224] INFO [UnifiedLog partition=financial_transactions-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,225] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition financial_transactions-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,225] INFO [UnifiedLog partition=financial_transactions-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:31:59,225] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition financial_transactions-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:31:59,226] INFO [UnifiedLog partition=financial_transactions-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-20 23:37:27,862] INFO Sent auto-creation request for Set(aggregated_transactions) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-05-20 23:37:27,891] INFO [NodeToControllerChannelManager id=5 name=forwarding] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:37:27,893] INFO [broker-5-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:37:27,985] INFO Sent auto-creation request for Set(aggregated_transactions) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2025-05-20 23:37:28,038] INFO [Broker id=5] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-20 23:37:28,040] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(aggregated_transactions-0) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:37:28,041] INFO [Broker id=5] Creating new partition aggregated_transactions-0 with topic id tZKMqbfwSlSmyms8wDFH7g. (state.change.logger)
[2025-05-20 23:37:28,050] INFO [LogLoader partition=aggregated_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-20 23:37:28,055] INFO Created log for partition aggregated_transactions-0 in /tmp/kafka-logs/aggregated_transactions-0 with properties {} (kafka.log.LogManager)
[2025-05-20 23:37:28,056] INFO [Partition aggregated_transactions-0 broker=5] No checkpointed highwatermark is found for partition aggregated_transactions-0 (kafka.cluster.Partition)
[2025-05-20 23:37:28,056] INFO [Partition aggregated_transactions-0 broker=5] Log loaded for partition aggregated_transactions-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-20 23:37:28,058] INFO [Broker id=5] Leader aggregated_transactions-0 with topic id Some(tZKMqbfwSlSmyms8wDFH7g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
[2025-05-20 23:47:28,232] INFO [NodeToControllerChannelManager id=5 name=forwarding] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,103] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-20 23:53:45,110] INFO [BrokerServer id=5] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
[2025-05-20 23:53:45,110] INFO [BrokerServer id=5] shutting down (kafka.server.BrokerServer)
[2025-05-20 23:53:45,111] INFO [BrokerLifecycleManager id=5] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:53:45,152] INFO [BrokerLifecycleManager id=5] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:53:45,234] INFO [Broker id=5] Transitioning 72 partition(s) to local followers. (state.change.logger)
[2025-05-20 23:53:45,234] INFO [Broker id=5] Follower financial_transactions-13 starts at leader epoch 3 from offset 53728 with partition epoch 3 and high watermark 53728. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:53:45,235] INFO [Broker id=5] Follower __consumer_offsets-13 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,235] INFO [Broker id=5] Follower __consumer_offsets-46 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,235] INFO [Broker id=5] Follower financial_transactions-17 starts at leader epoch 1 from offset 53580 with partition epoch 3 and high watermark 53580. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,236] INFO [Broker id=5] Follower __consumer_offsets-9 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,236] INFO [Broker id=5] Follower __consumer_offsets-42 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,236] INFO [Broker id=5] Follower __consumer_offsets-21 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,236] INFO [Broker id=5] Follower __consumer_offsets-17 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,237] INFO [Broker id=5] Follower financial_transactions-0 starts at leader epoch 1 from offset 53808 with partition epoch 3 and high watermark 53808. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,237] INFO [Broker id=5] Follower __consumer_offsets-30 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,237] INFO [Broker id=5] Follower financial_transactions-4 starts at leader epoch 2 from offset 53258 with partition epoch 3 and high watermark 53258. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,237] INFO [Broker id=5] Follower __consumer_offsets-26 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,238] INFO [Broker id=5] Follower __consumer_offsets-5 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,238] INFO [Broker id=5] Follower financial_transactions-8 starts at leader epoch 3 from offset 53130 with partition epoch 3 and high watermark 53130. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:53:45,238] INFO [Broker id=5] Follower __consumer_offsets-38 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,239] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,239] INFO [Broker id=5] Follower financial_transactions-12 starts at leader epoch 1 from offset 53441 with partition epoch 3 and high watermark 53441. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,239] INFO [Broker id=5] Follower __consumer_offsets-34 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,240] INFO [Broker id=5] Follower financial_transactions-14 starts at leader epoch 2 from offset 53468 with partition epoch 3 and high watermark 53468. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,240] INFO [Broker id=5] Follower __consumer_offsets-16 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,241] INFO [Broker id=5] Follower _schemas-0 starts at leader epoch 11 from offset 4 with partition epoch 18 and high watermark 4. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,241] INFO [Broker id=5] Follower __consumer_offsets-45 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,241] INFO [BrokerLifecycleManager id=5] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:53:45,242] INFO [Broker id=5] Follower financial_transactions-18 starts at leader epoch 1 from offset 53605 with partition epoch 3 and high watermark 53605. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,242] INFO [BrokerLifecycleManager id=5] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:45,242] INFO [Broker id=5] Follower __consumer_offsets-12 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,243] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,243] INFO [BrokerLifecycleManager id=5] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
[2025-05-20 23:53:45,243] INFO [Broker id=5] Follower __consumer_offsets-24 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,244] INFO [broker-5-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,244] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,245] INFO [broker-5-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,245] INFO [broker-5-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,247] INFO [Broker id=5] Follower __consumer_offsets-49 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,249] INFO [SocketServer listenerType=BROKER, nodeId=5] Stopping socket server request processors (kafka.network.SocketServer)
[2025-05-20 23:53:45,251] INFO [Broker id=5] Follower __consumer_offsets-0 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,255] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 23:53:45,257] INFO [Broker id=5] Follower __consumer_offsets-29 starts at leader epoch 11 from offset 4 with partition epoch 18 and high watermark 4. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,258] INFO [Broker id=5] Follower financial_transactions-1 starts at leader epoch 3 from offset 53591 with partition epoch 3 and high watermark 53591. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-20 23:53:45,258] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,258] INFO [Broker id=5] Follower financial_transactions-5 starts at leader epoch 2 from offset 53395 with partition epoch 3 and high watermark 53395. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,259] INFO [Broker id=5] Follower __consumer_offsets-8 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,259] INFO [Broker id=5] Follower __consumer_offsets-37 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,260] INFO [Broker id=5] Follower financial_transactions-9 starts at leader epoch 2 from offset 53169 with partition epoch 3 and high watermark 53169. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,260] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,260] INFO [Broker id=5] Follower __consumer_offsets-33 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,261] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,261] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,261] INFO [Broker id=5] Follower financial_transactions-15 starts at leader epoch 2 from offset 53502 with partition epoch 3 and high watermark 53502. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,262] INFO [Broker id=5] Follower __consumer_offsets-11 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,262] INFO [Broker id=5] Follower __consumer_offsets-44 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,262] INFO [Broker id=5] Follower financial_transactions-19 starts at leader epoch 2 from offset 53309 with partition epoch 3 and high watermark 53309. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,262] INFO [Broker id=5] Follower __consumer_offsets-23 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,263] INFO [Broker id=5] Follower __consumer_offsets-19 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,263] INFO [Broker id=5] Follower __consumer_offsets-32 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,263] INFO [Broker id=5] Follower financial_transactions-2 starts at leader epoch 2 from offset 53173 with partition epoch 3 and high watermark 53173. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,264] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,264] INFO [Broker id=5] Follower __consumer_offsets-7 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-20 23:53:45,264] INFO [Broker id=5] Follower financial_transactions-6 starts at leader epoch 2 from offset 53206 with partition epoch 3 and high watermark 53206. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,264] INFO [Broker id=5] Follower __consumer_offsets-40 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,265] INFO [Broker id=5] Follower __consumer_offsets-3 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,265] INFO [Broker id=5] Follower financial_transactions-10 starts at leader epoch 1 from offset 53252 with partition epoch 3 and high watermark 53252. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,265] INFO [Broker id=5] Follower __consumer_offsets-36 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,265] INFO [Broker id=5] Follower __consumer_offsets-47 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,266] INFO [Broker id=5] Follower financial_transactions-16 starts at leader epoch 2 from offset 53208 with partition epoch 3 and high watermark 53208. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,266] INFO [Broker id=5] Follower __consumer_offsets-14 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,270] INFO [Broker id=5] Follower __consumer_offsets-43 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,271] INFO [Broker id=5] Follower __consumer_offsets-10 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,271] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-20 23:53:45,272] INFO [Broker id=5] Follower __consumer_offsets-18 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,272] INFO [Broker id=5] Follower __consumer_offsets-31 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,273] INFO [Broker id=5] Follower aggregated_transactions-0 starts at leader epoch 1 from offset 352 with partition epoch 1 and high watermark 352. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,273] INFO [Broker id=5] Follower __consumer_offsets-27 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,274] INFO [Broker id=5] Follower financial_transactions-3 starts at leader epoch 1 from offset 53622 with partition epoch 3 and high watermark 53622. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,274] INFO [Broker id=5] Follower __consumer_offsets-39 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,276] INFO [Broker id=5] Follower financial_transactions-7 starts at leader epoch 1 from offset 53438 with partition epoch 3 and high watermark 53438. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-20 23:53:45,277] INFO [Broker id=5] Follower __consumer_offsets-6 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-20 23:53:45,278] INFO [Broker id=5] Follower __consumer_offsets-35 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-20 23:53:45,278] INFO [Broker id=5] Follower financial_transactions-11 starts at leader epoch 2 from offset 53486 with partition epoch 3 and high watermark 53486. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-20 23:53:45,279] INFO [Broker id=5] Follower __consumer_offsets-2 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-20 23:53:45,279] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Node 4 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,279] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Cancelled in-flight FETCH request with correlation id 24127 due to node 4 being disconnected (elapsed time since creation: 267ms, elapsed time since send: 267ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,279] INFO [SocketServer listenerType=BROKER, nodeId=5] Stopped socket server request processors (kafka.network.SocketServer)
[2025-05-20 23:53:45,280] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Client requested connection close from node 4 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,282] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Error sending fetch request (sessionId=132234361, epoch=24127) to node 4: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 4 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 23:53:45,287] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Node 6 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,292] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Cancelled in-flight FETCH request with correlation id 27491 due to node 6 being disconnected (elapsed time since creation: 134ms, elapsed time since send: 134ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,293] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Client requested connection close from node 6 (org.apache.kafka.clients.NetworkClient)
[2025-05-20 23:53:45,293] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Error sending fetch request (sessionId=41420854, epoch=27491) to node 6: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 6 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 23:53:45,294] WARN [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=5, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=132234361, epoch=24127), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 4 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 23:53:45,294] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=5, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=41420854, epoch=27491), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 6 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-20 23:53:45,297] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, financial_transactions-18, financial_transactions-12, financial_transactions-11, __consumer_offsets-8, __consumer_offsets-21, financial_transactions-15, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, financial_transactions-4, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, financial_transactions-0, financial_transactions-6, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, financial_transactions-16, __consumer_offsets-15, financial_transactions-10, __consumer_offsets-24, financial_transactions-19, financial_transactions-7, __consumer_offsets-17, financial_transactions-17, financial_transactions-1, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, financial_transactions-14, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, financial_transactions-8, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, financial_transactions-2, aggregated_transactions-0, financial_transactions-13, __consumer_offsets-32, financial_transactions-5, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:53:45,297] INFO [ReplicaAlterLogDirsManager on broker 5] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, financial_transactions-18, financial_transactions-12, financial_transactions-11, __consumer_offsets-8, __consumer_offsets-21, financial_transactions-15, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, financial_transactions-4, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, financial_transactions-0, financial_transactions-6, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, financial_transactions-16, __consumer_offsets-15, financial_transactions-10, __consumer_offsets-24, financial_transactions-19, financial_transactions-7, __consumer_offsets-17, financial_transactions-17, financial_transactions-1, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, financial_transactions-14, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, financial_transactions-8, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, financial_transactions-2, aggregated_transactions-0, financial_transactions-13, __consumer_offsets-32, financial_transactions-5, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 23:53:45,302] INFO [Broker id=5] Stopped fetchers as part of controlled shutdown for 72 partitions (state.change.logger)
[2025-05-20 23:53:45,303] INFO [ReplicaFetcherThread-0-4]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:53:45,304] INFO [ReplicaFetcherThread-0-4]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:53:45,304] INFO [ReplicaFetcherThread-0-4]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:53:45,306] INFO [ReplicaFetcherThread-0-6]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:53:45,306] INFO [ReplicaFetcherThread-0-6]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:53:45,306] INFO [ReplicaFetcherThread-0-6]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-20 23:53:45,309] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,309] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,309] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,309] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,309] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,310] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,310] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,311] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,311] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,311] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,311] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,312] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,312] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,312] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,313] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,313] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,313] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,314] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,314] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,314] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,315] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,315] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,315] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,316] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,316] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,317] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,317] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,317] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,318] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,318] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,318] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,319] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,318] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,319] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,319] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,319] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,320] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,320] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,320] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,321] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,321] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,321] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,322] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,321] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,322] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,322] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,323] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,323] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,323] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,323] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,324] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,324] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,324] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,324] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,325] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,325] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,326] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,326] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,326] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,326] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,327] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,326] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,327] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,327] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,327] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,328] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,328] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,328] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,329] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,329] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,329] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,329] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,330] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,329] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,330] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,330] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,331] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,331] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,332] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,331] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,332] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,332] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,333] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,333] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,333] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,332] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,334] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,334] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,334] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,335] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,335] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,335] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,336] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,336] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,336] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,337] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,337] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,338] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,339] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,339] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,339] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,340] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,340] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,340] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,340] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,341] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,340] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,341] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,342] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,341] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,342] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,342] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,342] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,343] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,343] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,343] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,344] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,343] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,344] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,345] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,345] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,345] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,345] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,345] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,346] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,346] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,346] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,346] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,347] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,347] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,347] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,348] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,348] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,348] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,349] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,349] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,350] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,351] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,351] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,351] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,351] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,352] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,352] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,352] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,353] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,352] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,353] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,353] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,353] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,354] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-20 23:53:45,355] INFO [data-plane Kafka Request Handler on Broker 5], shutting down (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 23:53:45,361] INFO [data-plane Kafka Request Handler on Broker 5], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2025-05-20 23:53:45,361] INFO [ExpirationReaper-5-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,362] INFO [ExpirationReaper-5-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,362] INFO [ExpirationReaper-5-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,364] INFO [KafkaApi-5] Shutdown complete. (kafka.server.KafkaApis)
[2025-05-20 23:53:45,369] INFO [TransactionCoordinator id=5] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 23:53:45,370] INFO [Transaction State Manager 5]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2025-05-20 23:53:45,370] INFO [TxnMarkerSenderThread-5]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 23:53:45,371] INFO [TxnMarkerSenderThread-5]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 23:53:45,371] INFO [TxnMarkerSenderThread-5]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-20 23:53:45,374] INFO [TransactionCoordinator id=5] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-20 23:53:45,375] INFO [GroupCoordinator 5]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,376] INFO [ExpirationReaper-5-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,377] INFO [ExpirationReaper-5-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,377] INFO [ExpirationReaper-5-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,378] INFO [ExpirationReaper-5-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,379] INFO [ExpirationReaper-5-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,379] INFO [ExpirationReaper-5-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,380] INFO [GroupCoordinator 5]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-20 23:53:45,381] INFO [AssignmentsManager id=5]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:45,381] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,382] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,382] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,383] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 23:53:45,383] INFO [AssignmentsManager id=5]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:45,385] INFO [ReplicaManager broker=5] Shutting down (kafka.server.ReplicaManager)
[2025-05-20 23:53:45,385] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 23:53:45,386] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 23:53:45,386] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-20 23:53:45,387] INFO [ReplicaFetcherManager on broker 5] shutting down (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:53:45,388] INFO [ReplicaFetcherManager on broker 5] shutdown completed (kafka.server.ReplicaFetcherManager)
[2025-05-20 23:53:45,389] INFO [ReplicaAlterLogDirsManager on broker 5] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 23:53:45,389] INFO [ReplicaAlterLogDirsManager on broker 5] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-20 23:53:45,389] INFO [ExpirationReaper-5-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,390] INFO [ExpirationReaper-5-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,390] INFO [ExpirationReaper-5-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,391] INFO [ExpirationReaper-5-RemoteFetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,392] INFO [ExpirationReaper-5-RemoteFetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,392] INFO [ExpirationReaper-5-RemoteFetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,392] INFO [ExpirationReaper-5-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,393] INFO [ExpirationReaper-5-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,393] INFO [ExpirationReaper-5-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,394] INFO [ExpirationReaper-5-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,394] INFO [ExpirationReaper-5-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,394] INFO [ExpirationReaper-5-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,395] INFO [ExpirationReaper-5-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,396] INFO [ExpirationReaper-5-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,396] INFO [ExpirationReaper-5-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-20 23:53:45,406] INFO [AddPartitionsToTxnSenderThread-5]: Shutting down (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 23:53:45,407] INFO [AddPartitionsToTxnSenderThread-5]: Stopped (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 23:53:45,407] INFO [AddPartitionsToTxnSenderThread-5]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
[2025-05-20 23:53:45,408] INFO [ReplicaManager broker=5] Shut down completely (kafka.server.ReplicaManager)
[2025-05-20 23:53:45,409] INFO [broker-5-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,409] INFO [broker-5-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,409] INFO [broker-5-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,410] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 23:53:45,411] INFO [broker-5-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,411] INFO [broker-5-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,411] INFO [broker-5-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-20 23:53:45,412] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-20 23:53:45,413] INFO Shutting down. (kafka.log.LogManager)
[2025-05-20 23:53:45,414] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
[2025-05-20 23:53:45,414] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 23:53:45,414] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 23:53:45,415] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
[2025-05-20 23:53:45,438] INFO [ProducerStateManager partition=financial_transactions-15] Wrote producer snapshot at offset 53502 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,478] INFO [ProducerStateManager partition=financial_transactions-0] Wrote producer snapshot at offset 53808 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,540] INFO [ProducerStateManager partition=financial_transactions-3] Wrote producer snapshot at offset 53622 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,566] INFO [ProducerStateManager partition=financial_transactions-10] Wrote producer snapshot at offset 53252 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,579] INFO [ProducerStateManager partition=financial_transactions-12] Wrote producer snapshot at offset 53441 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,594] INFO [ProducerStateManager partition=financial_transactions-13] Wrote producer snapshot at offset 53728 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,631] INFO [ProducerStateManager partition=financial_transactions-19] Wrote producer snapshot at offset 53309 with 0 producer ids in 7 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,643] INFO [ProducerStateManager partition=financial_transactions-5] Wrote producer snapshot at offset 53395 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,655] INFO [ProducerStateManager partition=financial_transactions-6] Wrote producer snapshot at offset 53206 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,692] INFO [ProducerStateManager partition=financial_transactions-17] Wrote producer snapshot at offset 53580 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,705] INFO [ProducerStateManager partition=financial_transactions-7] Wrote producer snapshot at offset 53438 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,740] INFO [ProducerStateManager partition=__consumer_offsets-29] Wrote producer snapshot at offset 4 with 0 producer ids in 8 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,804] INFO [ProducerStateManager partition=financial_transactions-4] Wrote producer snapshot at offset 53258 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,842] INFO [ProducerStateManager partition=financial_transactions-18] Wrote producer snapshot at offset 53605 with 0 producer ids in 6 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,864] INFO [ProducerStateManager partition=financial_transactions-9] Wrote producer snapshot at offset 53169 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,877] INFO [ProducerStateManager partition=financial_transactions-1] Wrote producer snapshot at offset 53591 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,888] INFO [ProducerStateManager partition=financial_transactions-14] Wrote producer snapshot at offset 53468 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,912] INFO [ProducerStateManager partition=financial_transactions-11] Wrote producer snapshot at offset 53486 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,941] INFO [ProducerStateManager partition=aggregated_transactions-0] Wrote producer snapshot at offset 352 with 3 producer ids in 6 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,958] INFO [ProducerStateManager partition=_schemas-0] Wrote producer snapshot at offset 4 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:45,980] INFO [ProducerStateManager partition=financial_transactions-8] Wrote producer snapshot at offset 53130 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:46,003] INFO [ProducerStateManager partition=financial_transactions-2] Wrote producer snapshot at offset 53173 with 0 producer ids in 7 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:46,022] INFO [ProducerStateManager partition=financial_transactions-16] Wrote producer snapshot at offset 53208 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:46,100] INFO Shutdown complete. (kafka.log.LogManager)
[2025-05-20 23:53:46,101] INFO [broker-5-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,103] INFO [broker-5-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,103] INFO [broker-5-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,103] INFO [broker-5-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,104] INFO [broker-5-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,104] INFO [broker-5-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,106] INFO [broker-5-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,107] INFO [broker-5-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,107] INFO [broker-5-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,109] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,109] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,109] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-20 23:53:46,110] INFO [SocketServer listenerType=BROKER, nodeId=5] Shutting down socket server (kafka.network.SocketServer)
[2025-05-20 23:53:46,118] INFO [SocketServer listenerType=BROKER, nodeId=5] Shutdown completed (kafka.network.SocketServer)
[2025-05-20 23:53:46,120] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
[2025-05-20 23:53:46,121] INFO [BrokerLifecycleManager id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:46,122] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 23:53:46,123] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 23:53:46,123] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-20 23:53:46,124] INFO [SharedServer id=5] Stopping SharedServer (kafka.server.SharedServer)
[2025-05-20 23:53:46,124] INFO [MetadataLoader id=5] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:46,124] INFO [SnapshotGenerator id=5] close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:46,125] INFO [SnapshotGenerator id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:46,125] INFO [MetadataLoader id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:46,126] INFO [SnapshotGenerator id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-20 23:53:46,127] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 23:53:46,213] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 23:53:46,213] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-20 23:53:46,214] INFO [kafka-5-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 23:53:46,214] INFO [RaftManager id=5] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 23:53:46,215] INFO [RaftManager id=5] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-20 23:53:46,215] INFO [RaftManager id=5] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 23:53:46,215] INFO [kafka-5-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 23:53:46,215] INFO [kafka-5-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-20 23:53:46,220] INFO [kafka-5-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 23:53:46,221] INFO [kafka-5-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 23:53:46,221] INFO [kafka-5-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-20 23:53:46,225] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 8082 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-20 23:53:46,228] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 23:53:46,228] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 23:53:46,228] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-20 23:53:46,229] INFO App info kafka.server for 5 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-20 23:53:46,230] INFO [BrokerServer id=5] shut down completed (kafka.server.BrokerServer)
[2025-05-20 23:53:46,230] INFO [BrokerServer id=5] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
[2025-05-20 23:53:46,230] INFO App info kafka.server for 5 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 12:15:19,109] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-21 12:15:19,426] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-2:19092,PLAINTEXT_HOST://localhost:39092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 5
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 5
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-21 12:15:19,453] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-21 12:15:19,455] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:24,374] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-21 12:15:24,509] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-21 12:15:24,518] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:24,718] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:24,750] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:24,765] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-21 12:15:24,771] INFO [BrokerServer id=5] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-21 12:15:24,773] INFO [SharedServer id=5] Starting SharedServer (kafka.server.SharedServer)
[2025-05-21 12:15:24,776] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:24,908] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
[2025-05-21 12:15:24,918] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:24,920] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:24,921] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000002740.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 12:15:24,933] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000008082.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 12:15:24,934] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 13ms for snapshot load and 1ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:25,251] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 8082 with 0 producer ids in 11 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:25,349] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 8082 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:25,351] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 8082 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:25,359] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=8082, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000008082.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:25,363] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 5ms for snapshot load and 0ms for segment recovery from offset 8082 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:25,436] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-21 12:15:25,518] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 12:15:25,535] INFO [RaftManager id=5] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 12:15:25,723] INFO [RaftManager id=5] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 12:15:25,916] INFO [RaftManager id=5] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=16, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from null (org.apache.kafka.raft.QuorumState)
[2025-05-21 12:15:25,922] INFO [kafka-5-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 12:15:25,930] INFO [kafka-5-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 12:15:25,993] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:25,994] INFO [BrokerServer id=5] Starting broker (kafka.server.BrokerServer)
[2025-05-21 12:15:26,001] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:26,067] INFO [broker-5-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:15:26,071] INFO [broker-5-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:15:26,091] INFO [broker-5-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:15:26,094] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:26,129] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:15:26,199] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:26,242] INFO [RaftManager id=5] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1526044992 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 12:15:26,305] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:26,341] INFO [BrokerServer id=5] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-21 12:15:26,344] INFO [BrokerServer id=5] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-21 12:15:26,343] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,385] WARN [RaftManager id=5] Connection to node 2 (kafka-controller-2/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,387] INFO [broker-5-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:26,402] INFO [broker-5-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:26,412] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:26,479] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 12:15:26,521] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:26,622] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:26,751] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:26,763] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,763] WARN [RaftManager id=5] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,777] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,780] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,782] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,784] WARN [RaftManager id=5] Connection to node 2 (kafka-controller-2/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,855] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:26,912] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,914] WARN [RaftManager id=5] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,915] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,918] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,951] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,952] WARN [RaftManager id=5] Connection to node 2 (kafka-controller-2/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:26,959] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,249] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,252] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,271] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,315] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,321] WARN [RaftManager id=5] Connection to node 2 (kafka-controller-2/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,364] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,483] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,597] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,701] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,780] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,796] WARN [RaftManager id=5] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,811] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,791] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-21 12:15:27,811] WARN [RaftManager id=5] Connection to node 2 (kafka-controller-2/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,803] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,845] INFO [SocketServer listenerType=BROKER, nodeId=5] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-21 12:15:27,862] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-21 12:15:27,937] INFO [SocketServer listenerType=BROKER, nodeId=5] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-21 12:15:27,971] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:27,976] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,985] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:27,981] INFO [broker-5-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,011] INFO [broker-5-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,055] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,119] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,097] INFO [ExpirationReaper-5-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,073] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,207] INFO [ExpirationReaper-5-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,226] INFO [ExpirationReaper-5-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,226] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,267] INFO [ExpirationReaper-5-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,282] INFO [ExpirationReaper-5-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,299] INFO [ExpirationReaper-5-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,301] INFO [ExpirationReaper-5-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,319] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,320] WARN [RaftManager id=5] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,341] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,341] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,344] WARN [RaftManager id=5] Connection to node 2 (kafka-controller-2/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,444] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,554] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,678] INFO [broker-5-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,679] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,680] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,679] INFO [BrokerLifecycleManager id=5] Incarnation dZ6JbvMITBCNIRQKgye98Q of broker 5 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:15:28,713] INFO [ExpirationReaper-5-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:15:28,772] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,782] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.5:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,786] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,799] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,801] WARN [RaftManager id=5] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,826] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,830] INFO [BrokerServer id=5] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-21 12:15:28,841] INFO [BrokerServer id=5] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-21 12:15:28,842] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,843] INFO [BrokerServer id=5] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-21 12:15:28,852] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,901] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,930] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:28,933] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:28,934] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,984] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:28,998] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,006] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,032] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,054] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,097] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,099] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,136] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,153] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,180] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,191] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,235] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,242] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,278] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,279] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,304] INFO [RaftManager id=5] Completed transition to Unattached(epoch=17, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) from FollowerState(fetchTimeoutMs=2000, epoch=16, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-21 12:15:29,354] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,457] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,518] INFO [RaftManager id=5] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=17, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from Unattached(epoch=17, voters=[1, 2, 3], electionTimeoutMs=9223372036854775807) (org.apache.kafka.raft.QuorumState)
[2025-05-21 12:15:29,545] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,560] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,561] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:29,562] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,619] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:29,661] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,767] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,871] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:29,991] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,098] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,200] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,211] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:15:30,213] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:30,263] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:15:30,305] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,332] INFO [RaftManager id=5] High watermark set to Optional[LogOffsetMetadata(offset=8087, metadata=Optional.empty)] for the first time for epoch 17 (org.apache.kafka.raft.FollowerState)
[2025-05-21 12:15:30,378] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 8087 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,424] INFO [BrokerLifecycleManager id=5] Successfully registered broker 5 with broker epoch 8090 (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:15:30,663] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 8086, but the high water mark is 8090 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,667] INFO [MetadataLoader id=5] initializeNewPublishers: The loader is still catching up because we have loaded up to offset 8086, but the high water mark is 8090 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,673] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 8090 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,681] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 8089 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,683] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing MetadataVersionPublisher(id=5) with a snapshot at offset 8089 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,684] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 8089 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:30,686] INFO [BrokerMetadataPublisher id=5] Publishing initial metadata at offset OffsetAndEpoch(offset=8089, epoch=17) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-21 12:15:30,697] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:30,711] INFO Skipping recovery of 72 logs from /tmp/kafka-logs since clean shutdown file was found (kafka.log.LogManager)
[2025-05-21 12:15:30,861] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Loading producer state till offset 53728 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,878] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53728 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,879] INFO [ProducerStateManager partition=financial_transactions-13] Loading producer state from snapshot file 'SnapshotFile(offset=53728, file=/tmp/kafka-logs/financial_transactions-13/00000000000000053728.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:30,883] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Producer state recovery took 4ms for snapshot load and 0ms for segment recovery from offset 53728 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,922] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-13, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53728) with 1 segments, local-log-start-offset 0 and log-end-offset 53728 in 146ms (1/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:30,942] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Loading producer state till offset 53605 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,943] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53605 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,948] INFO [ProducerStateManager partition=financial_transactions-18] Loading producer state from snapshot file 'SnapshotFile(offset=53605, file=/tmp/kafka-logs/financial_transactions-18/00000000000000053605.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:30,949] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53605 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,955] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-18, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53605) with 1 segments, local-log-start-offset 0 and log-end-offset 53605 in 32ms (2/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:30,972] INFO Deleted producer state snapshot /tmp/kafka-logs/__consumer_offsets-29/00000000000000000002.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 12:15:30,975] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 4 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,976] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 4 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,976] INFO [ProducerStateManager partition=__consumer_offsets-29] Loading producer state from snapshot file 'SnapshotFile(offset=4, file=/tmp/kafka-logs/__consumer_offsets-29/00000000000000000004.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:30,977] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 4 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,979] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-29, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=29, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=4) with 1 segments, local-log-start-offset 0 and log-end-offset 4 in 20ms (3/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:30,991] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Loading producer state till offset 53441 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,991] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53441 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:30,994] INFO [ProducerStateManager partition=financial_transactions-12] Loading producer state from snapshot file 'SnapshotFile(offset=53441, file=/tmp/kafka-logs/financial_transactions-12/00000000000000053441.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:30,995] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53441 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,002] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-12, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53441) with 1 segments, local-log-start-offset 0 and log-end-offset 53441 in 21ms (4/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,027] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,041] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-38, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=38, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 27ms (5/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,065] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Loading producer state till offset 53208 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,070] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53208 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,071] INFO [ProducerStateManager partition=financial_transactions-16] Loading producer state from snapshot file 'SnapshotFile(offset=53208, file=/tmp/kafka-logs/financial_transactions-16/00000000000000053208.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,073] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 53208 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,075] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-16, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53208) with 1 segments, local-log-start-offset 0 and log-end-offset 53208 in 34ms (6/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,084] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,086] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-27, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=27, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (7/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,096] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,100] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-39, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=39, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (8/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,110] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,115] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-42, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=42, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (9/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,129] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,136] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-9, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 19ms (10/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,145] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,151] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-22, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=22, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (11/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,178] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,179] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-26, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=26, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 27ms (12/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,193] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,202] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-40, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=40, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 21ms (13/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,219] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,227] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-47, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=47, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 23ms (14/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,283] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,286] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-15, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 52ms (15/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,298] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,304] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-10, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 17ms (16/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,324] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Loading producer state till offset 53486 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,329] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53486 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,330] INFO [ProducerStateManager partition=financial_transactions-11] Loading producer state from snapshot file 'SnapshotFile(offset=53486, file=/tmp/kafka-logs/financial_transactions-11/00000000000000053486.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,331] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53486 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,333] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-11, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53486) with 1 segments, local-log-start-offset 0 and log-end-offset 53486 in 28ms (17/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,376] INFO [LogLoader partition=aggregated_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 352 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,377] INFO [LogLoader partition=aggregated_transactions-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 352 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,393] INFO [ProducerStateManager partition=aggregated_transactions-0] Loading producer state from snapshot file 'SnapshotFile(offset=352, file=/tmp/kafka-logs/aggregated_transactions-0/00000000000000000352.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,400] INFO [LogLoader partition=aggregated_transactions-0, dir=/tmp/kafka-logs] Producer state recovery took 6ms for snapshot load and 0ms for segment recovery from offset 352 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,412] INFO Completed load of Log(dir=/tmp/kafka-logs/aggregated_transactions-0, topicId=tZKMqbfwSlSmyms8wDFH7g, topic=aggregated_transactions, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=352) with 1 segments, local-log-start-offset 0 and log-end-offset 352 in 72ms (18/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,439] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,448] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-2, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 26ms (19/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,466] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,469] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-18, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 20ms (20/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,486] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,488] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-46, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=46, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (21/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,495] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Loading producer state till offset 53591 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,495] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53591 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,496] INFO [ProducerStateManager partition=financial_transactions-1] Loading producer state from snapshot file 'SnapshotFile(offset=53591, file=/tmp/kafka-logs/financial_transactions-1/00000000000000053591.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,497] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53591 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,500] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-1, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53591) with 1 segments, local-log-start-offset 0 and log-end-offset 53591 in 11ms (22/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,508] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,514] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-12, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (23/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,519] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,522] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-23, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=23, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (24/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,527] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,530] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-14, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (25/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,537] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,540] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-5, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (26/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,545] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,552] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-41, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=41, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (27/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,571] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Loading producer state till offset 53580 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,571] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53580 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,572] INFO [ProducerStateManager partition=financial_transactions-17] Loading producer state from snapshot file 'SnapshotFile(offset=53580, file=/tmp/kafka-logs/financial_transactions-17/00000000000000053580.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,573] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53580 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,579] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-17, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53580) with 1 segments, local-log-start-offset 0 and log-end-offset 53580 in 24ms (28/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,583] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,586] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-1, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (29/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,598] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,604] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-19, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 16ms (30/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,623] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,624] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-3, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 20ms (31/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,639] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,642] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-16, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (32/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,651] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,654] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-25, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=25, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (33/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,660] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Loading producer state till offset 53502 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,661] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53502 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,662] INFO [ProducerStateManager partition=financial_transactions-15] Loading producer state from snapshot file 'SnapshotFile(offset=53502, file=/tmp/kafka-logs/financial_transactions-15/00000000000000053502.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,663] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53502 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,666] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-15, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53502) with 1 segments, local-log-start-offset 0 and log-end-offset 53502 in 12ms (34/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,673] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Loading producer state till offset 53622 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,675] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53622 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,678] INFO [ProducerStateManager partition=financial_transactions-3] Loading producer state from snapshot file 'SnapshotFile(offset=53622, file=/tmp/kafka-logs/financial_transactions-3/00000000000000053622.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,680] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 53622 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,686] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-3, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53622) with 1 segments, local-log-start-offset 0 and log-end-offset 53622 in 20ms (35/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,695] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Loading producer state till offset 53252 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,696] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53252 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,696] INFO [ProducerStateManager partition=financial_transactions-10] Loading producer state from snapshot file 'SnapshotFile(offset=53252, file=/tmp/kafka-logs/financial_transactions-10/00000000000000053252.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,697] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53252 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,698] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-10, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53252) with 1 segments, local-log-start-offset 0 and log-end-offset 53252 in 11ms (36/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,708] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Loading producer state till offset 53309 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,709] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53309 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,709] INFO [ProducerStateManager partition=financial_transactions-19] Loading producer state from snapshot file 'SnapshotFile(offset=53309, file=/tmp/kafka-logs/financial_transactions-19/00000000000000053309.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,711] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 53309 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,718] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-19, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53309) with 1 segments, local-log-start-offset 0 and log-end-offset 53309 in 19ms (37/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,731] INFO Deleted producer state snapshot /tmp/kafka-logs/_schemas-0/00000000000000000002.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 12:15:31,733] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 4 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,735] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 4 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,736] INFO [ProducerStateManager partition=_schemas-0] Loading producer state from snapshot file 'SnapshotFile(offset=4, file=/tmp/kafka-logs/_schemas-0/00000000000000000004.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,738] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 1ms for segment recovery from offset 4 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,754] INFO Completed load of Log(dir=/tmp/kafka-logs/_schemas-0, topicId=RrE8eovWRKu4kLR3MRJ0fA, topic=_schemas, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=4) with 1 segments, local-log-start-offset 0 and log-end-offset 4 in 30ms (38/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,775] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,777] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-28, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=28, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 21ms (39/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,782] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,804] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-13, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 27ms (40/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,830] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,844] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-24, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=24, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 39ms (41/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,863] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,871] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-32, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=32, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 25ms (42/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,884] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Loading producer state till offset 53468 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,885] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53468 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,886] INFO [ProducerStateManager partition=financial_transactions-14] Loading producer state from snapshot file 'SnapshotFile(offset=53468, file=/tmp/kafka-logs/financial_transactions-14/00000000000000053468.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,886] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 53468 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,889] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-14, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53468) with 1 segments, local-log-start-offset 0 and log-end-offset 53468 in 16ms (43/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,899] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Loading producer state till offset 53173 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,899] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53173 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,900] INFO [ProducerStateManager partition=financial_transactions-2] Loading producer state from snapshot file 'SnapshotFile(offset=53173, file=/tmp/kafka-logs/financial_transactions-2/00000000000000053173.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,902] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 53173 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,911] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-2, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53173) with 1 segments, local-log-start-offset 0 and log-end-offset 53173 in 21ms (44/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,918] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,921] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-31, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=31, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (45/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,926] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,929] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-21, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=21, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (46/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,944] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 53808 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,946] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53808 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,949] INFO [ProducerStateManager partition=financial_transactions-0] Loading producer state from snapshot file 'SnapshotFile(offset=53808, file=/tmp/kafka-logs/financial_transactions-0/00000000000000053808.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,953] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Producer state recovery took 4ms for snapshot load and 0ms for segment recovery from offset 53808 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,955] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-0, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53808) with 1 segments, local-log-start-offset 0 and log-end-offset 53808 in 26ms (47/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,971] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Loading producer state till offset 53169 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,971] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53169 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,972] INFO [ProducerStateManager partition=financial_transactions-9] Loading producer state from snapshot file 'SnapshotFile(offset=53169, file=/tmp/kafka-logs/financial_transactions-9/00000000000000053169.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:31,974] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 0ms for segment recovery from offset 53169 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,977] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-9, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53169) with 1 segments, local-log-start-offset 0 and log-end-offset 53169 in 22ms (48/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:31,987] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:31,989] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-6, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (49/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,004] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,007] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-11, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 17ms (50/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,014] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Loading producer state till offset 53395 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,014] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53395 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,015] INFO [ProducerStateManager partition=financial_transactions-5] Loading producer state from snapshot file 'SnapshotFile(offset=53395, file=/tmp/kafka-logs/financial_transactions-5/00000000000000053395.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:32,015] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 53395 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,017] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-5, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53395) with 1 segments, local-log-start-offset 0 and log-end-offset 53395 in 10ms (51/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,023] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,030] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-30, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=30, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (52/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,052] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Loading producer state till offset 53206 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,052] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53206 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,053] INFO [ProducerStateManager partition=financial_transactions-6] Loading producer state from snapshot file 'SnapshotFile(offset=53206, file=/tmp/kafka-logs/financial_transactions-6/00000000000000053206.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:32,054] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53206 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,060] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-6, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53206) with 1 segments, local-log-start-offset 0 and log-end-offset 53206 in 30ms (53/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,070] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,073] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-43, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=43, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (54/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,088] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,092] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-7, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 18ms (55/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,096] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,101] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-33, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=33, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (56/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,112] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,115] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-36, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=36, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (57/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,121] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,124] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-45, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=45, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (58/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,131] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,134] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-0, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (59/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,144] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,148] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-20, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=20, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 13ms (60/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,152] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,160] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-4, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (61/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,172] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,174] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-8, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (62/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,184] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,186] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-49, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=49, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (63/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,190] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,192] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-34, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=34, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (64/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,198] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,201] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-48, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=48, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (65/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,210] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Loading producer state till offset 53258 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,210] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53258 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,211] INFO [ProducerStateManager partition=financial_transactions-4] Loading producer state from snapshot file 'SnapshotFile(offset=53258, file=/tmp/kafka-logs/financial_transactions-4/00000000000000053258.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:32,212] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53258 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,215] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-4, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53258) with 1 segments, local-log-start-offset 0 and log-end-offset 53258 in 11ms (66/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,224] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,226] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-44, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=44, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (67/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,234] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,235] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-37, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=37, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (68/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,240] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Loading producer state till offset 53438 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,240] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53438 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,242] INFO [ProducerStateManager partition=financial_transactions-7] Loading producer state from snapshot file 'SnapshotFile(offset=53438, file=/tmp/kafka-logs/financial_transactions-7/00000000000000053438.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:32,248] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Producer state recovery took 6ms for snapshot load and 0ms for segment recovery from offset 53438 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,251] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-7, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53438) with 1 segments, local-log-start-offset 0 and log-end-offset 53438 in 15ms (69/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,255] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,258] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-35, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=35, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (70/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,262] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,264] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-17, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (71/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,274] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Loading producer state till offset 53130 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,274] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 53130 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,275] INFO [ProducerStateManager partition=financial_transactions-8] Loading producer state from snapshot file 'SnapshotFile(offset=53130, file=/tmp/kafka-logs/financial_transactions-8/00000000000000053130.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:15:32,276] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 53130 (kafka.log.UnifiedLog$)
[2025-05-21 12:15:32,279] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-8, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=53130) with 1 segments, local-log-start-offset 0 and log-end-offset 53130 in 12ms (72/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 12:15:32,285] INFO Loaded 72 logs in 1585ms (kafka.log.LogManager)
[2025-05-21 12:15:32,286] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-21 12:15:32,288] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-21 12:15:32,297] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-21 12:15:32,427] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 12:15:32,428] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 12:15:32,429] INFO [AddPartitionsToTxnSenderThread-5]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 12:15:32,430] INFO [GroupCoordinator 5]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,433] INFO [GroupCoordinator 5]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,434] INFO [TransactionCoordinator id=5] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 12:15:32,454] INFO [TxnMarkerSenderThread-5]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 12:15:32,455] INFO [TransactionCoordinator id=5] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 12:15:32,476] INFO [Broker id=5] Transitioning 72 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:32,479] INFO [Broker id=5] Creating new partition financial_transactions-13 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,505] INFO [Partition financial_transactions-13 broker=5] Log loaded for partition financial_transactions-13 with initial high watermark 53728 (kafka.cluster.Partition)
[2025-05-21 12:15:32,506] INFO [Broker id=5] Follower financial_transactions-13 starts at leader epoch 3 from offset 53728 with partition epoch 3 and high watermark 53728. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:32,512] INFO [Broker id=5] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,516] INFO [Partition __consumer_offsets-13 broker=5] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,517] INFO [Broker id=5] Follower __consumer_offsets-13 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,517] INFO [Broker id=5] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,518] INFO [Partition __consumer_offsets-46 broker=5] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,519] INFO [Broker id=5] Follower __consumer_offsets-46 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,519] INFO [Broker id=5] Creating new partition financial_transactions-17 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,521] INFO [Partition financial_transactions-17 broker=5] Log loaded for partition financial_transactions-17 with initial high watermark 53580 (kafka.cluster.Partition)
[2025-05-21 12:15:32,521] INFO [Broker id=5] Follower financial_transactions-17 starts at leader epoch 1 from offset 53580 with partition epoch 3 and high watermark 53580. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,522] INFO [Broker id=5] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,524] INFO [Partition __consumer_offsets-9 broker=5] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,524] INFO [Broker id=5] Follower __consumer_offsets-9 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,524] INFO [Broker id=5] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,527] INFO [Partition __consumer_offsets-42 broker=5] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,529] INFO [Broker id=5] Follower __consumer_offsets-42 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,530] INFO [Broker id=5] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,531] INFO [Partition __consumer_offsets-21 broker=5] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,532] INFO [Broker id=5] Follower __consumer_offsets-21 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,532] INFO [Broker id=5] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,536] INFO [Partition __consumer_offsets-17 broker=5] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,536] INFO [Broker id=5] Follower __consumer_offsets-17 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,537] INFO [Broker id=5] Creating new partition financial_transactions-0 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,538] INFO [Partition financial_transactions-0 broker=5] Log loaded for partition financial_transactions-0 with initial high watermark 53808 (kafka.cluster.Partition)
[2025-05-21 12:15:32,538] INFO [Broker id=5] Follower financial_transactions-0 starts at leader epoch 1 from offset 53808 with partition epoch 3 and high watermark 53808. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,539] INFO [Broker id=5] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,540] INFO [Partition __consumer_offsets-30 broker=5] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,541] INFO [Broker id=5] Follower __consumer_offsets-30 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,541] INFO [Broker id=5] Creating new partition financial_transactions-4 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,542] INFO [Partition financial_transactions-4 broker=5] Log loaded for partition financial_transactions-4 with initial high watermark 53258 (kafka.cluster.Partition)
[2025-05-21 12:15:32,543] INFO [Broker id=5] Follower financial_transactions-4 starts at leader epoch 2 from offset 53258 with partition epoch 3 and high watermark 53258. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,543] INFO [Broker id=5] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,545] INFO [Partition __consumer_offsets-26 broker=5] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,546] INFO [Broker id=5] Follower __consumer_offsets-26 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,546] INFO [Broker id=5] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,549] INFO [Partition __consumer_offsets-5 broker=5] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,549] INFO [Broker id=5] Follower __consumer_offsets-5 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,550] INFO [Broker id=5] Creating new partition financial_transactions-8 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,553] INFO [Partition financial_transactions-8 broker=5] Log loaded for partition financial_transactions-8 with initial high watermark 53130 (kafka.cluster.Partition)
[2025-05-21 12:15:32,554] INFO [Broker id=5] Follower financial_transactions-8 starts at leader epoch 3 from offset 53130 with partition epoch 3 and high watermark 53130. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:32,554] INFO [Broker id=5] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,555] INFO [Partition __consumer_offsets-38 broker=5] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,556] INFO [Broker id=5] Follower __consumer_offsets-38 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,556] INFO [Broker id=5] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,558] INFO [Partition __consumer_offsets-1 broker=5] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,558] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,559] INFO [Broker id=5] Creating new partition financial_transactions-12 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,560] INFO [Partition financial_transactions-12 broker=5] Log loaded for partition financial_transactions-12 with initial high watermark 53441 (kafka.cluster.Partition)
[2025-05-21 12:15:32,560] INFO [Broker id=5] Follower financial_transactions-12 starts at leader epoch 1 from offset 53441 with partition epoch 3 and high watermark 53441. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,560] INFO [Broker id=5] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,564] INFO [Partition __consumer_offsets-34 broker=5] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,565] INFO [Broker id=5] Follower __consumer_offsets-34 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,565] INFO [Broker id=5] Creating new partition financial_transactions-14 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,566] INFO [Partition financial_transactions-14 broker=5] Log loaded for partition financial_transactions-14 with initial high watermark 53468 (kafka.cluster.Partition)
[2025-05-21 12:15:32,566] INFO [Broker id=5] Follower financial_transactions-14 starts at leader epoch 2 from offset 53468 with partition epoch 3 and high watermark 53468. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,567] INFO [Broker id=5] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,568] INFO [Partition __consumer_offsets-16 broker=5] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,568] INFO [Broker id=5] Follower __consumer_offsets-16 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,569] INFO [Broker id=5] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-21 12:15:32,570] INFO [Partition _schemas-0 broker=5] Log loaded for partition _schemas-0 with initial high watermark 4 (kafka.cluster.Partition)
[2025-05-21 12:15:32,570] INFO [Broker id=5] Follower _schemas-0 starts at leader epoch 11 from offset 4 with partition epoch 18 and high watermark 4. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,571] INFO [Broker id=5] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,572] INFO [Partition __consumer_offsets-45 broker=5] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,573] INFO [Broker id=5] Follower __consumer_offsets-45 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,573] INFO [Broker id=5] Creating new partition financial_transactions-18 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,574] INFO [Partition financial_transactions-18 broker=5] Log loaded for partition financial_transactions-18 with initial high watermark 53605 (kafka.cluster.Partition)
[2025-05-21 12:15:32,574] INFO [Broker id=5] Follower financial_transactions-18 starts at leader epoch 1 from offset 53605 with partition epoch 3 and high watermark 53605. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,575] INFO [Broker id=5] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,576] INFO [Partition __consumer_offsets-12 broker=5] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,576] INFO [Broker id=5] Follower __consumer_offsets-12 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,577] INFO [Broker id=5] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,580] INFO [Partition __consumer_offsets-41 broker=5] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,580] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,581] INFO [Broker id=5] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,582] INFO [Partition __consumer_offsets-24 broker=5] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,582] INFO [Broker id=5] Follower __consumer_offsets-24 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,583] INFO [Broker id=5] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,584] INFO [Partition __consumer_offsets-20 broker=5] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,584] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,586] INFO [Broker id=5] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,587] INFO [Partition __consumer_offsets-49 broker=5] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,587] INFO [Broker id=5] Follower __consumer_offsets-49 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,588] INFO [Broker id=5] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,592] INFO [Partition __consumer_offsets-0 broker=5] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,593] INFO [Broker id=5] Follower __consumer_offsets-0 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,593] INFO [Broker id=5] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,594] INFO [Partition __consumer_offsets-29 broker=5] Log loaded for partition __consumer_offsets-29 with initial high watermark 4 (kafka.cluster.Partition)
[2025-05-21 12:15:32,594] INFO [Broker id=5] Follower __consumer_offsets-29 starts at leader epoch 11 from offset 4 with partition epoch 18 and high watermark 4. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,595] INFO [Broker id=5] Creating new partition financial_transactions-1 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,595] INFO [Partition financial_transactions-1 broker=5] Log loaded for partition financial_transactions-1 with initial high watermark 53591 (kafka.cluster.Partition)
[2025-05-21 12:15:32,596] INFO [Broker id=5] Follower financial_transactions-1 starts at leader epoch 3 from offset 53591 with partition epoch 3 and high watermark 53591. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:32,597] INFO [Broker id=5] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,598] INFO [Partition __consumer_offsets-25 broker=5] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,598] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,598] INFO [Broker id=5] Creating new partition financial_transactions-5 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,599] INFO [Partition financial_transactions-5 broker=5] Log loaded for partition financial_transactions-5 with initial high watermark 53395 (kafka.cluster.Partition)
[2025-05-21 12:15:32,600] INFO [Broker id=5] Follower financial_transactions-5 starts at leader epoch 2 from offset 53395 with partition epoch 3 and high watermark 53395. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,600] INFO [Broker id=5] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,601] INFO [Partition __consumer_offsets-8 broker=5] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,602] INFO [Broker id=5] Follower __consumer_offsets-8 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,602] INFO [Broker id=5] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,604] INFO [Partition __consumer_offsets-37 broker=5] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,604] INFO [Broker id=5] Follower __consumer_offsets-37 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,605] INFO [Broker id=5] Creating new partition financial_transactions-9 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,606] INFO [Partition financial_transactions-9 broker=5] Log loaded for partition financial_transactions-9 with initial high watermark 53169 (kafka.cluster.Partition)
[2025-05-21 12:15:32,607] INFO [Broker id=5] Follower financial_transactions-9 starts at leader epoch 2 from offset 53169 with partition epoch 3 and high watermark 53169. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,607] INFO [Broker id=5] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,609] INFO [Partition __consumer_offsets-4 broker=5] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,609] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,610] INFO [Broker id=5] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,611] INFO [Partition __consumer_offsets-33 broker=5] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,611] INFO [Broker id=5] Follower __consumer_offsets-33 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,611] INFO [Broker id=5] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,612] INFO [Partition __consumer_offsets-15 broker=5] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,613] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,613] INFO [Broker id=5] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,615] INFO [Partition __consumer_offsets-48 broker=5] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,616] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,616] INFO [Broker id=5] Creating new partition financial_transactions-15 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,617] INFO [Partition financial_transactions-15 broker=5] Log loaded for partition financial_transactions-15 with initial high watermark 53502 (kafka.cluster.Partition)
[2025-05-21 12:15:32,618] INFO [Broker id=5] Follower financial_transactions-15 starts at leader epoch 2 from offset 53502 with partition epoch 3 and high watermark 53502. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,619] INFO [Broker id=5] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,620] INFO [Partition __consumer_offsets-11 broker=5] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,620] INFO [Broker id=5] Follower __consumer_offsets-11 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,621] INFO [Broker id=5] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,622] INFO [Partition __consumer_offsets-44 broker=5] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,622] INFO [Broker id=5] Follower __consumer_offsets-44 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,623] INFO [Broker id=5] Creating new partition financial_transactions-19 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,625] INFO [Partition financial_transactions-19 broker=5] Log loaded for partition financial_transactions-19 with initial high watermark 53309 (kafka.cluster.Partition)
[2025-05-21 12:15:32,625] INFO [Broker id=5] Follower financial_transactions-19 starts at leader epoch 2 from offset 53309 with partition epoch 3 and high watermark 53309. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,627] INFO [Broker id=5] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,627] INFO [Partition __consumer_offsets-23 broker=5] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,628] INFO [Broker id=5] Follower __consumer_offsets-23 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,629] INFO [Broker id=5] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,630] INFO [Partition __consumer_offsets-19 broker=5] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,631] INFO [Broker id=5] Follower __consumer_offsets-19 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,631] INFO [Broker id=5] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,632] INFO [Partition __consumer_offsets-32 broker=5] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,632] INFO [Broker id=5] Follower __consumer_offsets-32 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,633] INFO [Broker id=5] Creating new partition financial_transactions-2 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,634] INFO [Partition financial_transactions-2 broker=5] Log loaded for partition financial_transactions-2 with initial high watermark 53173 (kafka.cluster.Partition)
[2025-05-21 12:15:32,635] INFO [Broker id=5] Follower financial_transactions-2 starts at leader epoch 2 from offset 53173 with partition epoch 3 and high watermark 53173. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,636] INFO [Broker id=5] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,638] INFO [Partition __consumer_offsets-28 broker=5] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,638] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,639] INFO [Broker id=5] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,640] INFO [Partition __consumer_offsets-7 broker=5] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,640] INFO [Broker id=5] Follower __consumer_offsets-7 starts at leader epoch 10 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:15:32,642] INFO [Broker id=5] Creating new partition financial_transactions-6 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,643] INFO [Partition financial_transactions-6 broker=5] Log loaded for partition financial_transactions-6 with initial high watermark 53206 (kafka.cluster.Partition)
[2025-05-21 12:15:32,644] INFO [Broker id=5] Follower financial_transactions-6 starts at leader epoch 2 from offset 53206 with partition epoch 3 and high watermark 53206. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,645] INFO [Broker id=5] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,646] INFO [Partition __consumer_offsets-40 broker=5] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,649] INFO [Broker id=5] Follower __consumer_offsets-40 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,650] INFO [Broker id=5] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,651] INFO [Partition __consumer_offsets-3 broker=5] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,652] INFO [Broker id=5] Follower __consumer_offsets-3 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,653] INFO [Broker id=5] Creating new partition financial_transactions-10 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,655] INFO [Partition financial_transactions-10 broker=5] Log loaded for partition financial_transactions-10 with initial high watermark 53252 (kafka.cluster.Partition)
[2025-05-21 12:15:32,655] INFO [Broker id=5] Follower financial_transactions-10 starts at leader epoch 1 from offset 53252 with partition epoch 3 and high watermark 53252. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,656] INFO [Broker id=5] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,657] INFO [Partition __consumer_offsets-36 broker=5] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,657] INFO [Broker id=5] Follower __consumer_offsets-36 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,658] INFO [Broker id=5] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,660] INFO [Partition __consumer_offsets-47 broker=5] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,660] INFO [Broker id=5] Follower __consumer_offsets-47 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,661] INFO [Broker id=5] Creating new partition financial_transactions-16 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,662] INFO [Partition financial_transactions-16 broker=5] Log loaded for partition financial_transactions-16 with initial high watermark 53208 (kafka.cluster.Partition)
[2025-05-21 12:15:32,662] INFO [Broker id=5] Follower financial_transactions-16 starts at leader epoch 2 from offset 53208 with partition epoch 3 and high watermark 53208. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,663] INFO [Broker id=5] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,664] INFO [Partition __consumer_offsets-14 broker=5] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,664] INFO [Broker id=5] Follower __consumer_offsets-14 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,665] INFO [Broker id=5] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,668] INFO [Partition __consumer_offsets-43 broker=5] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,668] INFO [Broker id=5] Follower __consumer_offsets-43 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,668] INFO [Broker id=5] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,670] INFO [Partition __consumer_offsets-10 broker=5] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,670] INFO [Broker id=5] Follower __consumer_offsets-10 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,671] INFO [Broker id=5] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,672] INFO [Partition __consumer_offsets-22 broker=5] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,673] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 8 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:32,674] INFO [Broker id=5] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,675] INFO [Partition __consumer_offsets-18 broker=5] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,675] INFO [Broker id=5] Follower __consumer_offsets-18 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,676] INFO [Broker id=5] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,677] INFO [Partition __consumer_offsets-31 broker=5] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,678] INFO [Broker id=5] Follower __consumer_offsets-31 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,678] INFO [Broker id=5] Creating new partition aggregated_transactions-0 with topic id tZKMqbfwSlSmyms8wDFH7g. (state.change.logger)
[2025-05-21 12:15:32,679] INFO [Partition aggregated_transactions-0 broker=5] Log loaded for partition aggregated_transactions-0 with initial high watermark 352 (kafka.cluster.Partition)
[2025-05-21 12:15:32,680] INFO [Broker id=5] Follower aggregated_transactions-0 starts at leader epoch 1 from offset 352 with partition epoch 1 and high watermark 352. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,680] INFO [Broker id=5] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,684] INFO [Partition __consumer_offsets-27 broker=5] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,684] INFO [Broker id=5] Follower __consumer_offsets-27 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,685] INFO [Broker id=5] Creating new partition financial_transactions-3 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,686] INFO [Partition financial_transactions-3 broker=5] Log loaded for partition financial_transactions-3 with initial high watermark 53622 (kafka.cluster.Partition)
[2025-05-21 12:15:32,687] INFO [Broker id=5] Follower financial_transactions-3 starts at leader epoch 1 from offset 53622 with partition epoch 3 and high watermark 53622. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,687] INFO [Broker id=5] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,689] INFO [Partition __consumer_offsets-39 broker=5] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,690] INFO [Broker id=5] Follower __consumer_offsets-39 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,691] INFO [Broker id=5] Creating new partition financial_transactions-7 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,692] INFO [Partition financial_transactions-7 broker=5] Log loaded for partition financial_transactions-7 with initial high watermark 53438 (kafka.cluster.Partition)
[2025-05-21 12:15:32,692] INFO [Broker id=5] Follower financial_transactions-7 starts at leader epoch 1 from offset 53438 with partition epoch 3 and high watermark 53438. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:32,693] INFO [Broker id=5] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,695] INFO [Partition __consumer_offsets-6 broker=5] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,695] INFO [Broker id=5] Follower __consumer_offsets-6 starts at leader epoch 6 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:15:32,696] INFO [Broker id=5] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,697] INFO [Partition __consumer_offsets-35 broker=5] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,698] INFO [Broker id=5] Follower __consumer_offsets-35 starts at leader epoch 7 from offset 0 with partition epoch 17 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:32,698] INFO [Broker id=5] Creating new partition financial_transactions-11 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 12:15:32,699] INFO [Partition financial_transactions-11 broker=5] Log loaded for partition financial_transactions-11 with initial high watermark 53486 (kafka.cluster.Partition)
[2025-05-21 12:15:32,700] INFO [Broker id=5] Follower financial_transactions-11 starts at leader epoch 2 from offset 53486 with partition epoch 3 and high watermark 53486. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:32,701] INFO [Broker id=5] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 12:15:32,702] INFO [Partition __consumer_offsets-2 broker=5] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 12:15:32,702] INFO [Broker id=5] Follower __consumer_offsets-2 starts at leader epoch 11 from offset 0 with partition epoch 18 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:32,705] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, aggregated_transactions-0, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-6, __consumer_offsets-35, financial_transactions-11, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:15:32,706] INFO [Broker id=5] Stopped fetchers as part of become-follower for 72 partitions (state.change.logger)
[2025-05-21 12:15:32,712] INFO [Broker id=5] Started fetchers as part of become-follower for 72 partitions (state.change.logger)
[2025-05-21 12:15:32,727] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,729] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,730] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,731] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,731] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,731] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,732] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,732] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,732] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,733] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,734] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,735] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,733] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,736] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,737] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,738] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,749] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,749] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,746] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,750] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,750] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,751] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,751] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,752] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,752] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,753] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,753] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,754] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,755] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,755] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,755] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,756] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,756] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,752] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,756] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,756] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,757] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,758] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,758] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,757] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,762] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,762] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,763] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,763] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,763] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,761] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,764] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,764] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,765] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,765] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,766] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,765] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,766] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,767] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,767] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,775] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,775] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,775] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,775] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,777] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,777] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,778] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,779] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,779] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,779] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,780] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,780] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,780] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,780] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,781] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,781] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,781] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,781] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,781] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,782] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,782] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,782] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,782] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,783] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,783] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,783] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,783] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,783] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,779] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,784] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,784] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,785] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,785] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,785] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,785] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,786] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,786] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,786] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,786] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,786] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,784] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,788] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,787] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,788] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,789] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,789] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,789] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,790] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,790] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,790] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,791] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,790] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,791] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,791] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,792] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,791] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,792] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,793] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,793] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,794] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,794] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,795] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,796] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,796] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,797] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,797] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,797] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,797] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,798] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,798] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,798] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,799] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,799] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,798] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,799] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,800] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,801] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,801] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[6] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,801] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,801] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,802] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,802] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,803] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:32,803] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,803] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,804] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,805] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,805] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,807] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,810] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,810] INFO [DynamicConfigPublisher broker id=5] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-21 12:15:32,811] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,813] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,816] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[6]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,816] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,816] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:32,819] INFO [DynamicConfigPublisher broker id=5] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-21 12:15:32,825] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=5) with a snapshot at offset 8089 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 12:15:32,864] INFO [BrokerLifecycleManager id=5] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:15:32,864] INFO [BrokerServer id=5] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-21 12:15:32,865] INFO [BrokerServer id=5] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-21 12:15:32,866] INFO [BrokerServer id=5] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-21 12:15:32,868] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-2:19092,PLAINTEXT_HOST://localhost:39092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 5
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 5
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-21 12:15:32,869] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 12:15:32,875] INFO [BrokerServer id=5] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-21 12:15:32,877] INFO [BrokerLifecycleManager id=5] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:15:32,922] INFO [BrokerLifecycleManager id=5] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:15:32,923] INFO [BrokerServer id=5] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-21 12:15:32,924] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-21 12:15:32,925] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-21 12:15:32,925] INFO [SocketServer listenerType=BROKER, nodeId=5] Enabling request processing. (kafka.network.SocketServer)
[2025-05-21 12:15:32,928] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-21 12:15:32,930] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-21 12:15:32,934] INFO [BrokerServer id=5] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-21 12:15:32,934] INFO [BrokerServer id=5] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-21 12:15:32,934] INFO [BrokerServer id=5] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-21 12:15:32,935] INFO [BrokerServer id=5] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-21 12:15:32,935] INFO [BrokerServer id=5] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-21 12:15:32,936] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 12:15:32,936] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 12:15:32,936] INFO Kafka startTimeMs: 1747829732935 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 12:15:32,937] INFO [KafkaRaftServer nodeId=5] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-21 12:15:32,973] INFO [Broker id=5] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-21 12:15:32,975] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(aggregated_transactions-0) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:15:32,985] INFO [Broker id=5] Leader aggregated_transactions-0 with topic id Some(tZKMqbfwSlSmyms8wDFH7g) starts at leader epoch 2 from offset 352 with partition epoch 2, high watermark 352, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 1. (state.change.logger)
[2025-05-21 12:15:33,334] INFO [Broker id=5] Transitioning 71 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:33,336] INFO [Broker id=5] Follower financial_transactions-13 starts at leader epoch 4 from offset 53728 with partition epoch 4 and high watermark 53728. Current leader is 6. Previous leader Some(6) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:15:33,338] INFO [Broker id=5] Follower __consumer_offsets-13 starts at leader epoch 11 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,338] INFO [Broker id=5] Follower __consumer_offsets-46 starts at leader epoch 11 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,339] INFO [Broker id=5] Follower financial_transactions-17 starts at leader epoch 2 from offset 53580 with partition epoch 4 and high watermark 53580. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,340] INFO [Broker id=5] Follower __consumer_offsets-9 starts at leader epoch 8 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,346] INFO [Broker id=5] Follower __consumer_offsets-42 starts at leader epoch 8 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,348] INFO [Broker id=5] Follower __consumer_offsets-21 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,349] INFO [Broker id=5] Follower __consumer_offsets-17 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,349] INFO [Broker id=5] Follower financial_transactions-0 starts at leader epoch 2 from offset 53808 with partition epoch 4 and high watermark 53808. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,350] INFO [Broker id=5] Follower __consumer_offsets-30 starts at leader epoch 11 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,351] INFO [Broker id=5] Follower financial_transactions-4 starts at leader epoch 3 from offset 53258 with partition epoch 4 and high watermark 53258. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,352] INFO [Broker id=5] Follower __consumer_offsets-26 starts at leader epoch 12 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,353] INFO [Broker id=5] Follower __consumer_offsets-5 starts at leader epoch 12 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,353] INFO [Broker id=5] Follower financial_transactions-8 starts at leader epoch 4 from offset 53130 with partition epoch 4 and high watermark 53130. Current leader is 6. Previous leader Some(6) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:15:33,354] INFO [Broker id=5] Follower __consumer_offsets-38 starts at leader epoch 8 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,354] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 9 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:15:33,355] INFO [Broker id=5] Follower financial_transactions-12 starts at leader epoch 2 from offset 53441 with partition epoch 4 and high watermark 53441. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,357] INFO [Broker id=5] Follower __consumer_offsets-34 starts at leader epoch 11 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,357] INFO [Broker id=5] Follower financial_transactions-14 starts at leader epoch 3 from offset 53468 with partition epoch 4 and high watermark 53468. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,359] INFO [Broker id=5] Follower __consumer_offsets-16 starts at leader epoch 12 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,362] INFO [Broker id=5] Follower _schemas-0 starts at leader epoch 12 from offset 4 with partition epoch 19 and high watermark 4. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,365] INFO [Broker id=5] Follower __consumer_offsets-45 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,366] INFO [Broker id=5] Follower financial_transactions-18 starts at leader epoch 2 from offset 53605 with partition epoch 4 and high watermark 53605. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,367] INFO [Broker id=5] Follower __consumer_offsets-12 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,368] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 9 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:15:33,369] INFO [Broker id=5] Follower __consumer_offsets-24 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,370] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 9 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:15:33,370] INFO [Broker id=5] Follower __consumer_offsets-49 starts at leader epoch 12 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,371] INFO [Broker id=5] Follower __consumer_offsets-0 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,371] INFO [Broker id=5] Follower __consumer_offsets-29 starts at leader epoch 12 from offset 4 with partition epoch 19 and high watermark 4. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,371] INFO [Broker id=5] Follower financial_transactions-1 starts at leader epoch 4 from offset 53591 with partition epoch 4 and high watermark 53591. Current leader is 6. Previous leader Some(6) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:15:33,373] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 9 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:15:33,373] INFO [Broker id=5] Follower financial_transactions-5 starts at leader epoch 3 from offset 53395 with partition epoch 4 and high watermark 53395. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,375] INFO [Broker id=5] Follower __consumer_offsets-8 starts at leader epoch 8 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,377] INFO [Broker id=5] Follower __consumer_offsets-37 starts at leader epoch 11 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,378] INFO [Broker id=5] Follower financial_transactions-9 starts at leader epoch 3 from offset 53169 with partition epoch 4 and high watermark 53169. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,379] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 9 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:15:33,379] INFO [Broker id=5] Follower __consumer_offsets-33 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,380] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 9 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:15:33,381] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 9 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:15:33,383] INFO [Broker id=5] Follower financial_transactions-15 starts at leader epoch 3 from offset 53502 with partition epoch 4 and high watermark 53502. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,384] INFO [Broker id=5] Follower __consumer_offsets-11 starts at leader epoch 11 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,384] INFO [Broker id=5] Follower __consumer_offsets-44 starts at leader epoch 11 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,385] INFO [Broker id=5] Follower financial_transactions-19 starts at leader epoch 3 from offset 53309 with partition epoch 4 and high watermark 53309. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,386] INFO [Broker id=5] Follower __consumer_offsets-23 starts at leader epoch 12 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,386] INFO [Broker id=5] Follower __consumer_offsets-19 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,387] INFO [Broker id=5] Follower __consumer_offsets-32 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,387] INFO [Broker id=5] Follower financial_transactions-2 starts at leader epoch 3 from offset 53173 with partition epoch 4 and high watermark 53173. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,388] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 9 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:15:33,389] INFO [Broker id=5] Follower __consumer_offsets-7 starts at leader epoch 11 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 12:15:33,390] INFO [Broker id=5] Follower financial_transactions-6 starts at leader epoch 3 from offset 53206 with partition epoch 4 and high watermark 53206. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,393] INFO [Broker id=5] Follower __consumer_offsets-40 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,394] INFO [Broker id=5] Follower __consumer_offsets-3 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,397] INFO [Broker id=5] Follower financial_transactions-10 starts at leader epoch 2 from offset 53252 with partition epoch 4 and high watermark 53252. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,398] INFO [Broker id=5] Follower __consumer_offsets-36 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,399] INFO [Broker id=5] Follower __consumer_offsets-47 starts at leader epoch 8 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,400] INFO [Broker id=5] Follower financial_transactions-16 starts at leader epoch 3 from offset 53208 with partition epoch 4 and high watermark 53208. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,400] INFO [Broker id=5] Follower __consumer_offsets-14 starts at leader epoch 8 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,401] INFO [Broker id=5] Follower __consumer_offsets-43 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,402] INFO [Broker id=5] Follower __consumer_offsets-10 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,403] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 9 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:15:33,403] INFO [Broker id=5] Follower __consumer_offsets-18 starts at leader epoch 12 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,404] INFO [Broker id=5] Follower __consumer_offsets-31 starts at leader epoch 8 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,404] INFO [Broker id=5] Follower __consumer_offsets-27 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,405] INFO [Broker id=5] Follower financial_transactions-3 starts at leader epoch 2 from offset 53622 with partition epoch 4 and high watermark 53622. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,407] INFO [Broker id=5] Follower __consumer_offsets-39 starts at leader epoch 12 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,407] INFO [Broker id=5] Follower financial_transactions-7 starts at leader epoch 2 from offset 53438 with partition epoch 4 and high watermark 53438. Current leader is 6. Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:15:33,408] INFO [Broker id=5] Follower __consumer_offsets-6 starts at leader epoch 7 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:15:33,409] INFO [Broker id=5] Follower __consumer_offsets-35 starts at leader epoch 8 from offset 0 with partition epoch 18 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:15:33,410] INFO [Broker id=5] Follower financial_transactions-11 starts at leader epoch 3 from offset 53486 with partition epoch 4 and high watermark 53486. Current leader is 6. Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:15:33,412] INFO [Broker id=5] Follower __consumer_offsets-2 starts at leader epoch 12 from offset 0 with partition epoch 19 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:15:33,414] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-6, __consumer_offsets-35, financial_transactions-11, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:15:33,414] INFO [Broker id=5] Stopped fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-21 12:15:33,447] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,454] INFO [ReplicaFetcherManager on broker 5] Added fetcher to broker 6 for partitions HashMap(financial_transactions-13 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),4,53728), __consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),11,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),11,0), financial_transactions-17 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,53580), __consumer_offsets-9 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),8,0), __consumer_offsets-42 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),8,0), __consumer_offsets-21 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-17 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), financial_transactions-0 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,53808), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),11,0), financial_transactions-4 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53258), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), financial_transactions-8 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),4,53130), __consumer_offsets-38 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),8,0), __consumer_offsets-1 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),9,0), financial_transactions-12 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,53441), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),11,0), financial_transactions-14 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53468), __consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), _schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,4), __consumer_offsets-45 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), financial_transactions-18 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,53605), __consumer_offsets-12 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-41 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),9,0), __consumer_offsets-24 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-20 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),9,0), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), __consumer_offsets-0 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,4), financial_transactions-1 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),4,53591), __consumer_offsets-25 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),9,0), financial_transactions-5 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53395), __consumer_offsets-8 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),8,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),11,0), financial_transactions-9 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53169), __consumer_offsets-4 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),9,0), __consumer_offsets-33 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-15 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),9,0), __consumer_offsets-48 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),9,0), financial_transactions-15 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53502), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),11,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),11,0), financial_transactions-19 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53309), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), __consumer_offsets-19 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-32 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), financial_transactions-2 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53173), __consumer_offsets-28 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),9,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),11,0), financial_transactions-6 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53206), __consumer_offsets-40 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-3 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), financial_transactions-10 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,53252), __consumer_offsets-36 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-47 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),8,0), financial_transactions-16 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53208), __consumer_offsets-14 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),8,0), __consumer_offsets-43 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-10 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-22 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),9,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), __consumer_offsets-31 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),8,0), __consumer_offsets-27 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), financial_transactions-3 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,53622), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), financial_transactions-7 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),2,53438), __consumer_offsets-6 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,0), __consumer_offsets-35 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),8,0), financial_transactions-11 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),3,53486), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:15:33,455] INFO [Broker id=5] Started fetchers as part of become-follower for 71 partitions (state.change.logger)
[2025-05-21 12:15:33,457] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,458] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,460] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,460] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,460] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,461] INFO [UnifiedLog partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,461] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,461] INFO [UnifiedLog partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,462] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,462] INFO [UnifiedLog partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,462] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,463] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,463] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,464] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,464] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,464] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,465] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,465] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,465] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,465] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,466] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,466] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,466] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,467] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,467] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,467] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,468] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,468] INFO [UnifiedLog partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,468] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,468] INFO [UnifiedLog partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,469] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,469] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,469] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,469] INFO [UnifiedLog partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,470] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,470] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,470] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,471] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,471] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,471] INFO [UnifiedLog partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,472] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,472] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,472] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,472] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,473] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,473] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,473] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,473] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,474] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,474] INFO [UnifiedLog partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,474] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,475] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,475] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,475] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,476] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,476] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,476] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,477] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,477] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,477] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,478] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,478] INFO [UnifiedLog partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,478] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,478] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,479] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,479] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,479] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,480] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,480] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,480] INFO [UnifiedLog partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,481] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,481] INFO [UnifiedLog partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,482] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,482] INFO [UnifiedLog partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,482] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,482] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,483] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,483] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,483] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,484] INFO [UnifiedLog partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,484] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,484] INFO [UnifiedLog partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,485] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,485] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,485] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,485] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,486] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,486] INFO [UnifiedLog partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,486] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,486] INFO [UnifiedLog partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,487] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,487] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,487] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,487] INFO [UnifiedLog partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,488] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,488] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,488] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:15:33,488] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:15:33,497] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,497] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,498] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,498] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,498] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,498] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,498] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,499] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,499] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,500] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,500] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,500] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,501] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,501] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,502] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,502] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,502] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,502] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,503] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,503] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,503] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,503] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,504] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,505] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,505] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,505] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,505] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,506] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,506] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,506] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,507] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,506] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,507] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,507] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,507] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,508] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,508] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,509] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,509] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,509] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,509] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,510] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,510] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,510] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,510] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,511] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,511] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,512] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,512] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,512] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,513] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,513] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,514] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,515] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,515] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,515] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,515] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,516] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,516] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,517] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,517] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,517] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,517] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,517] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,518] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,519] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,519] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,519] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,519] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,520] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,520] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,520] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,521] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,521] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,522] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,521] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,522] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,523] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,522] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,523] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,524] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,523] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,524] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,525] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,524] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,525] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,526] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,525] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,526] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,526] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,526] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,527] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,528] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,527] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,528] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,529] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,529] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,529] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,530] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,530] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,531] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,533] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,533] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,534] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,534] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,535] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,535] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,535] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,536] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,536] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,536] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,537] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,537] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,538] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,539] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,540] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,540] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,540] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,541] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,539] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,541] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,542] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,542] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,543] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,541] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,543] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,543] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,544] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,543] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,547] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,544] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,555] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,551] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,556] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,555] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,558] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,559] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,557] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,560] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,560] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,560] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,561] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,560] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,561] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,561] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,563] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,563] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,565] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,565] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,567] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,644] INFO [Broker id=5] Transitioning 2 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:33,646] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[6, 5], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:15:33,646] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=17, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:33,837] INFO [Broker id=5] Transitioning 27 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:33,838] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 5], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:33,840] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 5], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:33,843] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 4], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:33,843] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:33,844] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:33,845] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:33,847] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=5, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:33,848] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:33,850] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:33,853] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:33,855] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:33,856] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[6, 4], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:15:33,856] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 4], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:33,857] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:33,859] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=12, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=5, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:33,861] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:33,862] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:33,863] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:33,863] INFO [Broker id=5] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:33,864] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:33,865] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=18, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:33,867] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:33,868] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:33,869] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:33,870] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:33,870] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:33,870] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:33,871] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,872] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,872] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,874] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,872] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,874] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,875] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,875] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,876] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,876] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,877] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,877] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,877] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,878] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,879] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,877] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,880] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,881] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,879] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,882] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,883] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,883] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,883] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,884] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,885] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,885] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,885] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,885] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,885] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,886] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,887] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,887] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,888] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,887] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,888] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,888] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,889] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,889] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,890] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,890] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,890] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,891] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,891] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,891] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,891] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,892] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,892] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,892] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,893] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,893] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,893] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,892] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,894] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,894] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,895] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,895] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,896] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:33,896] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,896] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:33,898] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,188] INFO [Broker id=5] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:34,190] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=17, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:34,334] INFO [Broker id=5] Transitioning 27 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:34,336] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[6, 5, 4], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:15:34,337] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:34,338] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:34,338] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:34,339] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:34,340] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,342] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,344] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:34,351] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:34,352] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,353] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,354] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,354] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[6, 4, 5], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:15:34,355] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:34,355] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:34,355] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=12, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:34,356] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:34,356] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,357] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,357] INFO [Broker id=5] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,358] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,359] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=18, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:34,360] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,363] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:34,367] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,371] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:34,372] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,374] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,377] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,377] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,377] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,378] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,379] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,379] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,380] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,380] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,379] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,384] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,384] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,384] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,385] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,386] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,386] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,387] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,387] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,387] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,395] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,398] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,400] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,400] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,401] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,399] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,403] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,404] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,402] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,406] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,406] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,406] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,407] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,408] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,406] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,411] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,413] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,408] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,414] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,420] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,420] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,423] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,424] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,424] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,425] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,425] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,425] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,426] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,427] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,427] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,427] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,428] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,428] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,429] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,429] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,430] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,430] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,431] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,693] INFO [Broker id=5] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:34,693] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:34,696] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,698] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,698] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,834] INFO [Broker id=5] Transitioning 42 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:34,835] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:34,837] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,838] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:34,838] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:34,839] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,839] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,840] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,842] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,844] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,845] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 5], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:34,846] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 5], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:34,847] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,847] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,848] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,849] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=10, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:34,849] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,850] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:34,850] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,850] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:34,851] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,851] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,852] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:34,852] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,853] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:34,853] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5, 4], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,853] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,854] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[6, 5], partitionEpoch=5, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:15:34,854] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,854] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=3, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5], partitionEpoch=5, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:34,855] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 5], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:34,855] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,856] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,856] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:34,857] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4], partitionEpoch=5, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:34,857] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:34,857] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,858] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,858] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:34,858] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:34,859] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4], partitionEpoch=5, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:34,860] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:34,860] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:34,861] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,861] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,862] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,862] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,862] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,863] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,863] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,863] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,864] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,864] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,864] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,864] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,865] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,864] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,865] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,866] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,865] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,866] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,866] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,866] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,867] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,867] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,867] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,868] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,868] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,868] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,868] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,869] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,869] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,869] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,870] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,870] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,870] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,870] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,871] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,871] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,872] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,871] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,872] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,873] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,873] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,873] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,874] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,873] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,874] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,874] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,874] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,875] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,875] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,875] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,876] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,876] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,876] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,877] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,877] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,877] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,877] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,878] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,878] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,878] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,879] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,878] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,879] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,879] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,880] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,880] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,879] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,880] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,881] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,880] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,881] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,881] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,882] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,882] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,882] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,882] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,883] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,884] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,884] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,884] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,885] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,885] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,885] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,885] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,886] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,885] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,886] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,886] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:34,887] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:34,887] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,239] INFO [Broker id=5] Transitioning 1 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:35,239] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:35,240] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,240] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,240] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,334] INFO [Broker id=5] Transitioning 41 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:15:35,334] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:35,335] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:35,336] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:35,336] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:35,337] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:35,337] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:35,338] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,340] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5, 4], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,341] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:35,343] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:35,344] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:35,344] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:35,345] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5, 4], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,345] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,346] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=10, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:35,352] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5, 4], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,353] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:35,354] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=6, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:35,355] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:35,355] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,356] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,356] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:35,356] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:35,357] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:35,357] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:35,357] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=6, leaderEpoch=4, isr=[6, 5, 4], partitionEpoch=6, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:15:35,358] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5, 4], partitionEpoch=20, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,358] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=3, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 5, 4], partitionEpoch=6, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:35,358] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:35,358] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 5, 4], partitionEpoch=6, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:35,359] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:35,359] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:35,360] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=7, controllerEpoch=-1, leader=6, leaderEpoch=2, isr=[6, 4, 5], partitionEpoch=6, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 2. (state.change.logger)
[2025-05-21 12:15:35,360] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=6, leaderEpoch=11, isr=[6, 4, 5], partitionEpoch=21, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 11. (state.change.logger)
[2025-05-21 12:15:35,360] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 5, 4], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,361] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:35,361] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=6, leaderEpoch=8, isr=[6, 5, 4], partitionEpoch=20, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 12:15:35,361] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=6, leaderEpoch=9, isr=[6, 4, 5], partitionEpoch=20, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 12:15:35,362] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=6, leaderEpoch=3, isr=[6, 4, 5], partitionEpoch=6, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 3. (state.change.logger)
[2025-05-21 12:15:35,362] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=6, leaderEpoch=7, isr=[6, 4, 5], partitionEpoch=20, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 7. (state.change.logger)
[2025-05-21 12:15:35,362] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=6, leaderEpoch=12, isr=[6, 5, 4], partitionEpoch=21, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:15:35,363] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,363] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,363] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,364] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,364] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,364] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,365] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,364] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,365] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,366] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,366] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,367] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,367] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,367] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,367] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,368] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,368] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,368] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,368] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,370] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,371] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,371] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,371] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,372] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,373] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,373] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,374] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,373] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,375] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,375] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,375] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,376] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,376] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,376] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,377] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,378] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,378] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,378] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,379] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,379] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,379] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,380] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,380] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,381] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,381] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,380] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,382] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,382] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,382] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,383] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,383] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,383] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,384] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,387] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,387] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,388] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,388] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,388] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,388] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,390] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,390] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,390] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,391] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,391] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,391] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,391] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,391] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,391] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,391] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,392] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,392] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[8] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,392] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,392] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,392] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,393] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[7] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,391] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,393] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,393] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,393] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,394] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,394] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,394] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:15:35,395] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,394] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[8]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,395] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,396] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[7]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:15:35,396] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,420] INFO [Broker id=5] Transitioning 23 partition(s) to local leaders. (state.change.logger)
[2025-05-21 12:20:30,425] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-14, financial_transactions-15, __consumer_offsets-45, __consumer_offsets-43, __consumer_offsets-12, financial_transactions-19, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-0, __consumer_offsets-32, __consumer_offsets-27, financial_transactions-2, financial_transactions-4, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-6, __consumer_offsets-3, __consumer_offsets-36, financial_transactions-11, __consumer_offsets-33) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:20:30,427] INFO [Broker id=5] Leader financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 4 from offset 53468 with partition epoch 7, high watermark 53468, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:20:30,433] INFO [Broker id=5] Leader financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 4 from offset 53502 with partition epoch 7, high watermark 53502, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:20:30,437] INFO [Broker id=5] Leader __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 21, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:20:30,441] INFO [Broker id=5] Leader __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 21, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:20:30,446] INFO [Broker id=5] Leader __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 21, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:20:30,449] INFO [Broker id=5] Leader financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 4 from offset 53309 with partition epoch 7, high watermark 53309, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:20:30,454] INFO [Broker id=5] Leader __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 21, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:20:30,458] INFO [Broker id=5] Leader __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 21, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:20:30,460] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-13 has an older epoch (4) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,461] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-13 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,462] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-8 has an older epoch (4) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,462] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-8 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,462] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-13 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,463] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-13 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,463] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-46 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,463] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-46 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,464] INFO [Broker id=5] Leader __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 21, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:20:30,464] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-30 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,464] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-30 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,465] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-26 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,465] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-26 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,466] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-5 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,466] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-5 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,466] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-34 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,467] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-34 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,467] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-16 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,467] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-16 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,468] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-49 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,468] INFO [Broker id=5] Leader __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 21, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:20:30,468] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-49 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,469] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-11 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,469] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-11 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,469] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-44 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,470] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-44 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,470] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-23 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,470] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-23 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,471] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-7 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,471] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-7 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,471] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-16 has an older epoch (3) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,472] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-16 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,472] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-18 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,473] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-18 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,473] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-1 has an older epoch (4) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,473] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-1 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,473] INFO [Broker id=5] Leader __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 21, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:20:30,473] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-5 has an older epoch (3) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,474] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-5 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,474] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-39 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,475] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-39 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,475] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-37 has an older epoch (11) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,475] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-37 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,476] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-9 has an older epoch (3) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,476] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-9 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,476] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-2 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,477] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-2 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,477] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-29 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,477] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-29 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,477] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition _schemas-0 has an older epoch (12) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,478] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition _schemas-0 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,478] INFO [Broker id=5] Leader __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 21, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:20:30,484] INFO [Broker id=5] Leader __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 21, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:20:30,488] INFO [Broker id=5] Leader __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 21, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:20:30,492] INFO [Broker id=5] Leader financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 4 from offset 53173 with partition epoch 7, high watermark 53173, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:20:30,497] INFO [Broker id=5] Leader financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 4 from offset 53258 with partition epoch 7, high watermark 53258, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:20:30,501] INFO [Broker id=5] Leader financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 4 from offset 53206 with partition epoch 7, high watermark 53206, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:20:30,505] INFO [Broker id=5] Leader __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 21, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:20:30,510] INFO [Broker id=5] Leader __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 21, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:20:30,514] INFO [Broker id=5] Leader __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 21, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:20:30,518] INFO [Broker id=5] Leader __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 21, high watermark 0, ISR [6,5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:20:30,522] INFO [Broker id=5] Leader financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 4 from offset 53486 with partition epoch 7, high watermark 53486, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:20:30,526] INFO [Broker id=5] Leader __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 8 from offset 0 with partition epoch 21, high watermark 0, ISR [6,4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:20:30,529] INFO [Broker id=5] Transitioning 24 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:20:30,530] INFO [Broker id=5] Follower financial_transactions-13 starts at leader epoch 5 from offset 53728 with partition epoch 7 and high watermark 53728. Current leader is 4. Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 12:20:30,530] INFO [Broker id=5] Follower __consumer_offsets-16 starts at leader epoch 13 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,530] INFO [Broker id=5] Follower _schemas-0 starts at leader epoch 13 from offset 6 with partition epoch 22 and high watermark 6. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,531] INFO [Broker id=5] Follower __consumer_offsets-13 starts at leader epoch 12 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,531] INFO [Broker id=5] Follower financial_transactions-16 starts at leader epoch 4 from offset 53208 with partition epoch 7 and high watermark 53208. Current leader is 4. Previous leader Some(4) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,531] INFO [Broker id=5] Follower __consumer_offsets-46 starts at leader epoch 12 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,531] INFO [Broker id=5] Follower __consumer_offsets-11 starts at leader epoch 12 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,532] INFO [Broker id=5] Follower __consumer_offsets-44 starts at leader epoch 12 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,532] INFO [Broker id=5] Follower __consumer_offsets-23 starts at leader epoch 13 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,532] INFO [Broker id=5] Follower __consumer_offsets-49 starts at leader epoch 13 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,533] INFO [Broker id=5] Follower __consumer_offsets-18 starts at leader epoch 13 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,533] INFO [Broker id=5] Follower __consumer_offsets-29 starts at leader epoch 13 from offset 5 with partition epoch 22 and high watermark 5. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,533] INFO [Broker id=5] Follower __consumer_offsets-30 starts at leader epoch 12 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,533] INFO [Broker id=5] Follower financial_transactions-1 starts at leader epoch 5 from offset 53591 with partition epoch 7 and high watermark 53591. Current leader is 4. Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 12:20:30,534] INFO [Broker id=5] Follower __consumer_offsets-26 starts at leader epoch 13 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,534] INFO [Broker id=5] Follower financial_transactions-5 starts at leader epoch 4 from offset 53395 with partition epoch 7 and high watermark 53395. Current leader is 4. Previous leader Some(4) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,534] INFO [Broker id=5] Follower __consumer_offsets-7 starts at leader epoch 12 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,535] INFO [Broker id=5] Follower __consumer_offsets-39 starts at leader epoch 13 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,536] INFO [Broker id=5] Follower __consumer_offsets-5 starts at leader epoch 13 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,536] INFO [Broker id=5] Follower __consumer_offsets-37 starts at leader epoch 12 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,537] INFO [Broker id=5] Follower financial_transactions-8 starts at leader epoch 5 from offset 53130 with partition epoch 7 and high watermark 53130. Current leader is 4. Previous leader Some(4) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 12:20:30,537] INFO [Broker id=5] Follower financial_transactions-9 starts at leader epoch 4 from offset 53169 with partition epoch 7 and high watermark 53169. Current leader is 4. Previous leader Some(4) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:20:30,537] INFO [Broker id=5] Follower __consumer_offsets-2 starts at leader epoch 13 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-21 12:20:30,538] INFO [Broker id=5] Follower __consumer_offsets-34 starts at leader epoch 12 from offset 0 with partition epoch 22 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:20:30,539] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-7, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, financial_transactions-16, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-29, financial_transactions-1, financial_transactions-5, __consumer_offsets-39, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:20:30,539] INFO [Broker id=5] Stopped fetchers as part of become-follower for 24 partitions (state.change.logger)
[2025-05-21 12:20:30,544] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,544] INFO [ReplicaFetcherManager on broker 5] Added fetcher to broker 4 for partitions HashMap(financial_transactions-13 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),5,53728), __consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,0), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,0), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,0), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,0), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,0), financial_transactions-8 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),5,53130), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,0), __consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,0), _schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,6), financial_transactions-16 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),4,53208), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,5), financial_transactions-1 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),5,53591), financial_transactions-5 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),4,53395), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,0), financial_transactions-9 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),4,53169), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:20:30,545] INFO [Broker id=5] Started fetchers as part of become-follower for 24 partitions (state.change.logger)
[2025-05-21 12:20:30,545] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,545] INFO [UnifiedLog partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,546] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,546] INFO [UnifiedLog partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,547] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,547] INFO [UnifiedLog partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,547] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,548] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,548] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,548] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,549] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,549] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,549] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,550] INFO [UnifiedLog partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,550] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,550] INFO [UnifiedLog partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,551] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,551] INFO [UnifiedLog partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,551] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,552] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,552] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,553] INFO [UnifiedLog partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,553] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,553] INFO [UnifiedLog partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,554] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,554] INFO [UnifiedLog partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,554] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,555] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,555] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,555] INFO [UnifiedLog partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,556] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:20:30,556] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:20:30,558] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 45 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,558] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,564] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-45 in 4 milliseconds for epoch 8, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,564] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 43 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,564] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,564] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 12 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,565] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,565] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-43 in 1 milliseconds for epoch 8, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,565] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 10 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,565] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds for epoch 8, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,566] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,566] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 24 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,566] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds for epoch 8, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,566] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,567] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 21 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,567] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds for epoch 8, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,567] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,568] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds for epoch 8, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,568] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 19 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,568] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,569] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 17 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,569] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,569] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 0 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,570] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,570] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 32 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,569] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds for epoch 8, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,570] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,571] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-17 in 2 milliseconds for epoch 8, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,571] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 27 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,572] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,572] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 40 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,572] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,572] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 6 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,572] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,573] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 3 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,573] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,573] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 36 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,571] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds for epoch 8, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,574] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,574] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-32 in 3 milliseconds for epoch 8, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,574] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 33 in epoch 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,575] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-27 in 3 milliseconds for epoch 8, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,575] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,575] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-40 in 3 milliseconds for epoch 8, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,575] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,576] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,576] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-6 in 3 milliseconds for epoch 8, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,576] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,577] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-3 in 4 milliseconds for epoch 8, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,577] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,577] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-36 in 3 milliseconds for epoch 8, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,577] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,578] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,578] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-33 in 3 milliseconds for epoch 8, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,578] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,579] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,579] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,579] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,578] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,580] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,581] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,581] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,582] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,582] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,583] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,582] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,583] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,583] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,584] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,584] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,584] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,585] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,585] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,585] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,586] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,585] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,586] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,586] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,587] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,587] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,587] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,587] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,588] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,588] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,588] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,588] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,589] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,589] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,589] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,590] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,590] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,591] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,591] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,591] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,591] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,591] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:20:30,592] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,592] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:20:30,592] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:25:29,492] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:57,815] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-21 12:47:57,834] INFO [BrokerServer id=5] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
[2025-05-21 12:47:57,839] INFO [BrokerServer id=5] shutting down (kafka.server.BrokerServer)
[2025-05-21 12:47:57,853] INFO [BrokerLifecycleManager id=5] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:47:57,893] INFO [Broker id=5] Transitioning 35 partition(s) to local leaders. (state.change.logger)
[2025-05-21 12:47:57,911] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-15, financial_transactions-17, financial_transactions-19, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-32, financial_transactions-2, financial_transactions-4, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-38, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-14, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-43, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-31, __consumer_offsets-0, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-8, __consumer_offsets-6, __consumer_offsets-35, financial_transactions-11, __consumer_offsets-33) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:47:57,913] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-0 has an older epoch (2) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:57,913] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 256221, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,914] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-0 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:57,915] INFO [BrokerLifecycleManager id=5] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:47:57,915] INFO [Broker id=5] Leader financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 3 from offset 255866 with partition epoch 7, high watermark 255866, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:47:57,926] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 255146, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,929] INFO [Broker id=5] Leader __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 9 from offset 0 with partition epoch 21, high watermark 0, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:47:57,941] INFO [Broker id=5] Leader __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 9 from offset 0 with partition epoch 21, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:47:57,949] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,949] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,950] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,952] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,953] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[5, 4], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 255527, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,954] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 255314, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,955] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 255168, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,956] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,957] INFO [Broker id=5] Leader __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 9 from offset 0 with partition epoch 21, high watermark 0, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:47:57,966] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:57,969] INFO [Broker id=5] Leader financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 3 from offset 256290 with partition epoch 7, high watermark 256277, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:47:58,020] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,022] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Node 6 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,026] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Cancelled in-flight FETCH request with correlation id 68561 due to node 6 being disconnected (elapsed time since creation: 107ms, elapsed time since send: 107ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,026] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Client requested connection close from node 6 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,031] INFO [BrokerLifecycleManager id=5] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:47:58,037] INFO [BrokerLifecycleManager id=5] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,038] INFO [BrokerLifecycleManager id=5] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
[2025-05-21 12:47:58,029] INFO [Broker id=5] Leader __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 9 from offset 0 with partition epoch 21, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:47:58,043] INFO [SocketServer listenerType=BROKER, nodeId=5] Stopping socket server request processors (kafka.network.SocketServer)
[2025-05-21 12:47:58,041] INFO [broker-5-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,039] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Error sending fetch request (sessionId=378302946, epoch=68561) to node 6: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 6 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 12:47:58,097] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=5, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=read_uncommitted, removed=0e8v3fGFR_uwy9DAR-lNZA:financial_transactions-0, replaced=, metadata=(sessionId=378302946, epoch=68561), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 6 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 12:47:58,088] INFO [SocketServer listenerType=BROKER, nodeId=5] Stopped socket server request processors (kafka.network.SocketServer)
[2025-05-21 12:47:58,067] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 255812, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,066] INFO [broker-5-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,066] INFO [broker-5-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,065] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Node 4 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,100] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,102] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Cancelled in-flight FETCH request with correlation id 58558 due to node 4 being disconnected (elapsed time since creation: 61ms, elapsed time since send: 61ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,103] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Client requested connection close from node 4 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,103] INFO [Broker id=5] Leader __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 9 from offset 0 with partition epoch 21, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:47:58,104] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Error sending fetch request (sessionId=622556230, epoch=58558) to node 4: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 4 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 12:47:58,105] WARN [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=5, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={financial_transactions-13=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=255889, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[5], lastFetchedEpoch=Optional[5])}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=622556230, epoch=58558), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 4 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 12:47:58,105] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 12:47:58,119] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,120] INFO [Broker id=5] Leader financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 3 from offset 255590 with partition epoch 7, high watermark 255590, ISR [4,5], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:47:58,127] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,128] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,130] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,131] INFO [Broker id=5] Leader __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 9 from offset 0 with partition epoch 21, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:47:58,142] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,143] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=22, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,144] INFO [Broker id=5] Leader financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 3 from offset 255715 with partition epoch 7, high watermark 255715, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 2. (state.change.logger)
[2025-05-21 12:47:58,152] INFO [Broker id=5] Leader __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 9 from offset 0 with partition epoch 21, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:47:58,158] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,160] INFO [Broker id=5] Leader __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 9 from offset 0 with partition epoch 21, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 12:47:58,166] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 4. Current high watermark 256033, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,167] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[4, 5], partitionEpoch=22, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 0, ISR [4,5], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 12:47:58,168] INFO [Broker id=5] Transitioning 36 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:47:58,168] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=4, leaderEpoch=5, isr=[5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 12:47:58,169] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 10 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:58,169] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 10 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:58,170] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[5, 4], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:47:58,170] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[5, 4], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:47:58,171] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[4, 5], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:47:58,171] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[4, 5], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:47:58,172] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[5, 4], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:58,172] INFO [Broker id=5] Follower financial_transactions-0 starts at leader epoch 3 from offset 257211 with partition epoch 7 and high watermark 257197. Current leader is 4. Previous leader Some(4) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:47:58,173] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[4, 5], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:47:58,173] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 10 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:58,174] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[4, 5], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:58,174] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[5, 4], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:47:58,175] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[4, 5], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:58,175] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=4, leaderEpoch=5, isr=[4, 5], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 12:47:58,176] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 10 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:58,176] INFO [Broker id=5] Follower financial_transactions-12 starts at leader epoch 3 from offset 255590 with partition epoch 7 and high watermark 255590. Current leader is 4. Previous leader Some(4) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:47:58,177] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[4, 5], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:47:58,177] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[4, 5], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:58,178] INFO [Broker id=5] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[4, 5], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:58,178] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=4, leaderEpoch=4, isr=[5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:58,178] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 10 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:58,179] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 10 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:58,179] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 10 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:58,180] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[4, 5], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:58,180] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[5, 4], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:58,181] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[5, 4], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:58,181] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=4, leaderEpoch=5, isr=[5, 4], partitionEpoch=8, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 5. (state.change.logger)
[2025-05-21 12:47:58,182] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 10 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:58,183] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=4, leaderEpoch=4, isr=[5, 4], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:58,183] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[5, 4], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:58,183] INFO [Broker id=5] Follower financial_transactions-7 starts at leader epoch 3 from offset 255844 with partition epoch 7 and high watermark 255844. Current leader is 4. Previous leader Some(4) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:47:58,184] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=4, leaderEpoch=12, isr=[4, 5], partitionEpoch=23, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 12. (state.change.logger)
[2025-05-21 12:47:58,184] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=4, leaderEpoch=4, isr=[4, 5], partitionEpoch=8, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 4. (state.change.logger)
[2025-05-21 12:47:58,185] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 10 from offset 0 with partition epoch 21 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:58,185] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=4, leaderEpoch=13, isr=[5, 4], partitionEpoch=23, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 13. (state.change.logger)
[2025-05-21 12:47:58,187] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-41, __consumer_offsets-22, __consumer_offsets-20, financial_transactions-0, __consumer_offsets-28, __consumer_offsets-25, financial_transactions-7, __consumer_offsets-4, __consumer_offsets-1, financial_transactions-12) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:47:58,187] INFO [Broker id=5] Stopped fetchers as part of become-follower for 12 partitions (state.change.logger)
[2025-05-21 12:47:58,188] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,188] INFO [ReplicaFetcherManager on broker 5] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),10,0), __consumer_offsets-48 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),10,0), __consumer_offsets-41 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),10,0), __consumer_offsets-22 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),10,0), __consumer_offsets-20 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),10,0), financial_transactions-0 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),3,257211), __consumer_offsets-28 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),10,0), __consumer_offsets-25 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),10,0), financial_transactions-7 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),3,255844), __consumer_offsets-4 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),10,0), __consumer_offsets-1 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),10,0), financial_transactions-12 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),3,255590)) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:47:58,189] INFO [UnifiedLog partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:47:58,190] INFO [Broker id=5] Started fetchers as part of become-follower for 12 partitions (state.change.logger)
[2025-05-21 12:47:58,190] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,191] INFO [UnifiedLog partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:47:58,191] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,192] INFO [UnifiedLog partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:47:58,192] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,192] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:47:58,193] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,193] INFO [UnifiedLog partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:47:58,193] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,194] INFO [UnifiedLog partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:47:58,195] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,195] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:47:58,196] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,196] INFO [UnifiedLog partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:47:58,196] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,197] INFO [UnifiedLog partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
[2025-05-21 12:47:58,197] INFO [ReplicaFetcherThread-0-6]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,200] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Node 4 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,200] WARN [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Connection to node 4 (kafka-broker-1/172.19.0.4:19092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,201] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Client requested connection close from node 4 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 12:47:58,201] INFO [ReplicaFetcherThread-0-6]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,201] INFO [ReplicaFetcherThread-0-6]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,201] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Error sending fetch request (sessionId=622556230, epoch=INITIAL) to node 4: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to kafka-broker-1:19092 (id: 4 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 12:47:58,203] WARN [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=5, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={financial_transactions-0=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=257211, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[2]), financial_transactions-12=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=255590, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[2]), financial_transactions-7=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=255844, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[3], lastFetchedEpoch=Optional[2]), __consumer_offsets-15=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[10], lastFetchedEpoch=Optional.empty), __consumer_offsets-48=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[10], lastFetchedEpoch=Optional.empty), __consumer_offsets-28=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[10], lastFetchedEpoch=Optional.empty), __consumer_offsets-1=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[10], lastFetchedEpoch=Optional.empty), __consumer_offsets-41=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[10], lastFetchedEpoch=Optional.empty), __consumer_offsets-22=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[10], lastFetchedEpoch=Optional.empty), __consumer_offsets-20=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[10], lastFetchedEpoch=Optional.empty), __consumer_offsets-25=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[10], lastFetchedEpoch=Optional.empty), __consumer_offsets-4=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[10], lastFetchedEpoch=Optional.empty)}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=622556230, epoch=INITIAL), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to kafka-broker-1:19092 (id: 4 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 12:47:58,205] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 31 in epoch 9 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,205] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,205] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 47 in epoch 9 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,206] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,206] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-31 in 1 milliseconds for epoch 9, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,206] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 14 in epoch 9 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,207] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,208] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 9 in epoch 9 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,208] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,208] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-47 in 2 milliseconds for epoch 9, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,209] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 42 in epoch 9 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,210] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,210] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-14 in 2 milliseconds for epoch 9, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,210] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 8 in epoch 9 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,211] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-9 in 3 milliseconds for epoch 9, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,211] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,211] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-42 in 1 milliseconds for epoch 9, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,211] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 38 in epoch 9 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,212] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-8 in 1 milliseconds for epoch 9, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,212] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,213] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 35 in epoch 9 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,213] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds for epoch 9, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,213] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,214] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,214] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds for epoch 9, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,214] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,215] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,215] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,215] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,216] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,216] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,216] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,217] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,217] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,217] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,217] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,218] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,218] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,219] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,218] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,219] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,220] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,220] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,220] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,221] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,221] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,221] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,222] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,222] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,222] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,223] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,223] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,223] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,224] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,224] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,224] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,225] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,225] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,226] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,227] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,227] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,228] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,228] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,228] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,228] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,229] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,229] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,230] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,230] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,231] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,232] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,233] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,234] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,234] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,234] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,234] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,235] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,235] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,235] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,236] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,236] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,235] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,237] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,236] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,237] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,238] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,240] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,240] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,241] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,241] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,242] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,242] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,242] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,243] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,242] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,243] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,243] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,244] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,244] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[13] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,244] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,245] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,246] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[13]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,247] INFO [Broker id=5] Transitioning 72 partition(s) to local followers. (state.change.logger)
[2025-05-21 12:47:58,247] INFO [Broker id=5] Follower financial_transactions-13 starts at leader epoch 7 from offset 255889 with partition epoch 10 and high watermark 255874. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:47:58,248] INFO [Broker id=5] Follower __consumer_offsets-13 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 12:47:58,248] INFO [Broker id=5] Follower __consumer_offsets-46 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 12:47:58,248] INFO [Broker id=5] Follower financial_transactions-17 starts at leader epoch 4 from offset 255866 with partition epoch 9 and high watermark 255866. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:47:58,249] INFO [Broker id=5] Follower __consumer_offsets-9 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:58,249] INFO [Broker id=5] Follower __consumer_offsets-42 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:58,249] INFO [Broker id=5] Follower __consumer_offsets-21 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,250] INFO [Broker id=5] Follower __consumer_offsets-17 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,250] INFO [Broker id=5] Follower financial_transactions-0 starts at leader epoch 5 from offset 257211 with partition epoch 9 and high watermark 257197. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 12:47:58,250] INFO [Broker id=5] Follower __consumer_offsets-30 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 12:47:58,251] INFO [Broker id=5] Follower financial_transactions-4 starts at leader epoch 5 from offset 255335 with partition epoch 10 and high watermark 255335. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 12:47:58,251] INFO [Broker id=5] Follower __consumer_offsets-26 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 12:47:58,251] INFO [Broker id=5] Follower __consumer_offsets-5 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 12:47:58,252] INFO [Broker id=5] Follower financial_transactions-8 starts at leader epoch 7 from offset 255525 with partition epoch 10 and high watermark 255525. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:47:58,252] INFO [Broker id=5] Follower __consumer_offsets-38 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:58,252] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:47:58,253] INFO [Broker id=5] Follower financial_transactions-12 starts at leader epoch 5 from offset 255590 with partition epoch 9 and high watermark 255590. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 12:47:58,253] INFO [Broker id=5] Follower __consumer_offsets-34 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 12:47:58,253] INFO [Broker id=5] Follower financial_transactions-14 starts at leader epoch 5 from offset 255812 with partition epoch 10 and high watermark 255812. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 12:47:58,253] INFO [Broker id=5] Follower __consumer_offsets-16 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 12:47:58,254] INFO [Broker id=5] Follower _schemas-0 starts at leader epoch 15 from offset 6 with partition epoch 25 and high watermark 6. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 12:47:58,254] INFO [Broker id=5] Follower __consumer_offsets-45 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,254] INFO [Broker id=5] Follower financial_transactions-18 starts at leader epoch 4 from offset 255590 with partition epoch 9 and high watermark 255590. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:47:58,255] INFO [Broker id=5] Follower __consumer_offsets-12 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,256] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:47:58,256] INFO [Broker id=5] Follower __consumer_offsets-24 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,256] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:47:58,257] INFO [Broker id=5] Follower __consumer_offsets-49 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 12:47:58,257] INFO [Broker id=5] Follower __consumer_offsets-0 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,257] INFO [Broker id=5] Follower __consumer_offsets-29 starts at leader epoch 15 from offset 6 with partition epoch 25 and high watermark 6. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 12:47:58,258] INFO [Broker id=5] Follower financial_transactions-1 starts at leader epoch 7 from offset 255371 with partition epoch 10 and high watermark 255371. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 12:47:58,258] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:47:58,258] INFO [Broker id=5] Follower financial_transactions-5 starts at leader epoch 6 from offset 254990 with partition epoch 10 and high watermark 254990. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:47:58,259] INFO [Broker id=5] Follower __consumer_offsets-8 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:58,259] INFO [Broker id=5] Follower __consumer_offsets-37 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 12:47:58,259] INFO [Broker id=5] Follower financial_transactions-9 starts at leader epoch 6 from offset 255435 with partition epoch 10 and high watermark 255435. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:47:58,260] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:47:58,260] INFO [Broker id=5] Follower __consumer_offsets-33 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,260] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:47:58,261] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:47:58,261] INFO [Broker id=5] Follower financial_transactions-15 starts at leader epoch 5 from offset 256237 with partition epoch 10 and high watermark 256237. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 12:47:58,262] INFO [Broker id=5] Follower __consumer_offsets-11 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 12:47:58,262] INFO [Broker id=5] Follower __consumer_offsets-44 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 12:47:58,263] INFO [Broker id=5] Follower financial_transactions-19 starts at leader epoch 5 from offset 255167 with partition epoch 10 and high watermark 255167. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 12:47:58,263] INFO [Broker id=5] Follower __consumer_offsets-23 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 12:47:58,264] INFO [Broker id=5] Follower __consumer_offsets-19 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,264] INFO [Broker id=5] Follower __consumer_offsets-32 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,264] INFO [Broker id=5] Follower financial_transactions-2 starts at leader epoch 5 from offset 255541 with partition epoch 10 and high watermark 255541. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 12:47:58,265] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:47:58,265] INFO [Broker id=5] Follower __consumer_offsets-7 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 12:47:58,265] INFO [Broker id=5] Follower financial_transactions-6 starts at leader epoch 5 from offset 255175 with partition epoch 10 and high watermark 255175. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 12:47:58,266] INFO [Broker id=5] Follower __consumer_offsets-40 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,266] INFO [Broker id=5] Follower __consumer_offsets-3 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,266] INFO [Broker id=5] Follower financial_transactions-10 starts at leader epoch 4 from offset 256308 with partition epoch 9 and high watermark 256277. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:47:58,267] INFO [Broker id=5] Follower __consumer_offsets-36 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,267] INFO [Broker id=5] Follower __consumer_offsets-47 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:58,267] INFO [Broker id=5] Follower financial_transactions-16 starts at leader epoch 6 from offset 255412 with partition epoch 10 and high watermark 255412. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 12:47:58,268] INFO [Broker id=5] Follower __consumer_offsets-14 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:58,268] INFO [Broker id=5] Follower __consumer_offsets-43 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,268] INFO [Broker id=5] Follower __consumer_offsets-10 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,268] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 12:47:58,269] INFO [Broker id=5] Follower __consumer_offsets-18 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 12:47:58,269] INFO [Broker id=5] Follower __consumer_offsets-31 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:58,269] INFO [Broker id=5] Follower aggregated_transactions-0 starts at leader epoch 3 from offset 352 with partition epoch 3 and high watermark 352. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 12:47:58,270] INFO [Broker id=5] Follower __consumer_offsets-27 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,270] INFO [Broker id=5] Follower financial_transactions-3 starts at leader epoch 4 from offset 255715 with partition epoch 9 and high watermark 255715. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 12:47:58,270] INFO [Broker id=5] Follower __consumer_offsets-39 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 12:47:58,271] INFO [Broker id=5] Follower financial_transactions-7 starts at leader epoch 5 from offset 255844 with partition epoch 9 and high watermark 255844. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 12:47:58,271] INFO [Broker id=5] Follower __consumer_offsets-6 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 12:47:58,272] INFO [Broker id=5] Follower __consumer_offsets-35 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 12:47:58,272] INFO [Broker id=5] Follower financial_transactions-11 starts at leader epoch 5 from offset 256046 with partition epoch 10 and high watermark 256033. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 12:47:58,273] INFO [Broker id=5] Follower __consumer_offsets-2 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 12:47:58,278] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, financial_transactions-18, financial_transactions-12, financial_transactions-11, __consumer_offsets-8, __consumer_offsets-21, financial_transactions-15, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, financial_transactions-4, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, financial_transactions-0, financial_transactions-6, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, financial_transactions-16, __consumer_offsets-15, financial_transactions-10, __consumer_offsets-24, financial_transactions-19, financial_transactions-7, __consumer_offsets-17, financial_transactions-17, financial_transactions-1, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, financial_transactions-14, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, financial_transactions-8, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, financial_transactions-2, aggregated_transactions-0, financial_transactions-13, __consumer_offsets-32, financial_transactions-5, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:47:58,279] INFO [ReplicaAlterLogDirsManager on broker 5] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, financial_transactions-18, financial_transactions-12, financial_transactions-11, __consumer_offsets-8, __consumer_offsets-21, financial_transactions-15, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, financial_transactions-4, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, financial_transactions-0, financial_transactions-6, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, financial_transactions-16, __consumer_offsets-15, financial_transactions-10, __consumer_offsets-24, financial_transactions-19, financial_transactions-7, __consumer_offsets-17, financial_transactions-17, financial_transactions-1, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, financial_transactions-14, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, financial_transactions-8, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, financial_transactions-2, aggregated_transactions-0, financial_transactions-13, __consumer_offsets-32, financial_transactions-5, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-21 12:47:58,282] INFO [Broker id=5] Stopped fetchers as part of controlled shutdown for 72 partitions (state.change.logger)
[2025-05-21 12:47:58,283] INFO [ReplicaFetcherThread-0-4]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,286] INFO [ReplicaFetcherThread-0-4]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,286] INFO [ReplicaFetcherThread-0-4]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-21 12:47:58,288] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,289] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,289] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,289] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,289] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,290] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,290] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,290] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,291] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,291] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,291] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,291] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,291] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,293] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,294] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,294] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,294] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,294] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,294] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,295] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,296] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,296] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,296] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,297] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,298] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,297] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,298] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,298] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,298] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,299] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,299] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,299] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,300] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,300] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,300] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,301] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,301] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,301] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,302] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,302] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,302] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,302] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,303] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,303] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,303] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,303] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,304] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,304] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,304] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,304] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,305] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,305] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,305] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,305] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,306] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,306] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,306] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,307] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,307] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,307] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,308] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,307] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,308] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,308] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,308] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,309] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,309] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,309] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,310] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,310] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,310] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,311] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,311] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,311] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,312] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,312] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,313] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,313] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,313] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,313] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,314] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,317] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,317] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,317] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,318] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,318] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,319] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,319] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,322] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,324] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,325] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,324] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,325] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,326] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,326] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,327] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,329] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,329] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,330] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,330] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,406] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,406] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,406] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,406] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,407] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,407] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,407] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,408] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,408] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,408] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,409] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,408] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,409] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,410] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,411] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,411] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,412] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,412] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,412] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,413] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,413] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,413] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,414] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,414] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,414] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,414] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,415] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,414] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,415] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,416] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,416] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,416] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,416] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,416] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,417] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,417] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,417] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,418] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,418] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,418] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,419] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,419] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,419] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,420] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,420] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,420] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,421] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,421] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,421] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,429] INFO [data-plane Kafka Request Handler on Broker 5], shutting down (kafka.server.KafkaRequestHandlerPool)
[2025-05-21 12:47:58,429] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 12:47:58,433] INFO [data-plane Kafka Request Handler on Broker 5], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2025-05-21 12:47:58,433] INFO [ExpirationReaper-5-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,436] INFO [ExpirationReaper-5-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,436] INFO [ExpirationReaper-5-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,437] INFO [KafkaApi-5] Shutdown complete. (kafka.server.KafkaApis)
[2025-05-21 12:47:58,442] INFO [TransactionCoordinator id=5] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 12:47:58,443] INFO [Transaction State Manager 5]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2025-05-21 12:47:58,443] INFO [TxnMarkerSenderThread-5]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 12:47:58,444] INFO [TxnMarkerSenderThread-5]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 12:47:58,444] INFO [TxnMarkerSenderThread-5]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 12:47:58,446] INFO [TransactionCoordinator id=5] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 12:47:58,447] INFO [GroupCoordinator 5]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,448] INFO [ExpirationReaper-5-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,449] INFO [ExpirationReaper-5-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,449] INFO [ExpirationReaper-5-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,450] INFO [ExpirationReaper-5-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,451] INFO [ExpirationReaper-5-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,451] INFO [ExpirationReaper-5-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,452] INFO [GroupCoordinator 5]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 12:47:58,453] INFO [AssignmentsManager id=5]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,453] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,453] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,454] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,454] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 12:47:58,455] INFO [AssignmentsManager id=5]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,457] INFO [ReplicaManager broker=5] Shutting down (kafka.server.ReplicaManager)
[2025-05-21 12:47:58,457] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 12:47:58,458] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 12:47:58,458] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 12:47:58,459] INFO [ReplicaFetcherManager on broker 5] shutting down (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:47:58,460] INFO [ReplicaFetcherManager on broker 5] shutdown completed (kafka.server.ReplicaFetcherManager)
[2025-05-21 12:47:58,462] INFO [ReplicaAlterLogDirsManager on broker 5] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-21 12:47:58,462] INFO [ReplicaAlterLogDirsManager on broker 5] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-21 12:47:58,462] INFO [ExpirationReaper-5-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,463] INFO [ExpirationReaper-5-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,463] INFO [ExpirationReaper-5-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,464] INFO [ExpirationReaper-5-RemoteFetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,465] INFO [ExpirationReaper-5-RemoteFetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,465] INFO [ExpirationReaper-5-RemoteFetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,467] INFO [ExpirationReaper-5-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,468] INFO [ExpirationReaper-5-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,469] INFO [ExpirationReaper-5-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,470] INFO [ExpirationReaper-5-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,471] INFO [ExpirationReaper-5-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,471] INFO [ExpirationReaper-5-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,473] INFO [ExpirationReaper-5-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,474] INFO [ExpirationReaper-5-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,474] INFO [ExpirationReaper-5-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 12:47:58,486] INFO [AddPartitionsToTxnSenderThread-5]: Shutting down (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 12:47:58,487] INFO [AddPartitionsToTxnSenderThread-5]: Stopped (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 12:47:58,487] INFO [AddPartitionsToTxnSenderThread-5]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 12:47:58,488] INFO [ReplicaManager broker=5] Shut down completely (kafka.server.ReplicaManager)
[2025-05-21 12:47:58,489] INFO [broker-5-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,489] INFO [broker-5-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,490] INFO [broker-5-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,490] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 12:47:58,492] INFO [broker-5-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,492] INFO [broker-5-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,492] INFO [broker-5-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 12:47:58,493] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 12:47:58,494] INFO Shutting down. (kafka.log.LogManager)
[2025-05-21 12:47:58,495] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
[2025-05-21 12:47:58,496] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 12:47:58,497] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 12:47:58,497] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 12:47:58,517] INFO [ProducerStateManager partition=financial_transactions-15] Wrote producer snapshot at offset 256237 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,543] INFO [ProducerStateManager partition=financial_transactions-0] Wrote producer snapshot at offset 257211 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,575] INFO [ProducerStateManager partition=financial_transactions-3] Wrote producer snapshot at offset 255715 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,585] INFO [ProducerStateManager partition=financial_transactions-10] Wrote producer snapshot at offset 256308 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,590] INFO [ProducerStateManager partition=financial_transactions-12] Wrote producer snapshot at offset 255590 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,596] INFO [ProducerStateManager partition=financial_transactions-13] Wrote producer snapshot at offset 255889 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,612] INFO [ProducerStateManager partition=financial_transactions-19] Wrote producer snapshot at offset 255167 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,617] INFO [ProducerStateManager partition=financial_transactions-5] Wrote producer snapshot at offset 254990 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,623] INFO [ProducerStateManager partition=financial_transactions-6] Wrote producer snapshot at offset 255175 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,642] INFO [ProducerStateManager partition=financial_transactions-17] Wrote producer snapshot at offset 255866 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,647] INFO [ProducerStateManager partition=financial_transactions-7] Wrote producer snapshot at offset 255844 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,664] INFO [ProducerStateManager partition=__consumer_offsets-29] Wrote producer snapshot at offset 6 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,694] INFO [ProducerStateManager partition=financial_transactions-4] Wrote producer snapshot at offset 255335 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,713] INFO [ProducerStateManager partition=financial_transactions-18] Wrote producer snapshot at offset 255590 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,721] INFO [ProducerStateManager partition=financial_transactions-9] Wrote producer snapshot at offset 255435 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,726] INFO [ProducerStateManager partition=financial_transactions-1] Wrote producer snapshot at offset 255371 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,732] INFO [ProducerStateManager partition=financial_transactions-14] Wrote producer snapshot at offset 255812 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,742] INFO [ProducerStateManager partition=financial_transactions-11] Wrote producer snapshot at offset 256046 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,759] INFO [ProducerStateManager partition=_schemas-0] Wrote producer snapshot at offset 6 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,769] INFO [ProducerStateManager partition=financial_transactions-8] Wrote producer snapshot at offset 255525 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,778] INFO [ProducerStateManager partition=financial_transactions-2] Wrote producer snapshot at offset 255541 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,786] INFO [ProducerStateManager partition=financial_transactions-16] Wrote producer snapshot at offset 255412 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,823] INFO Shutdown complete. (kafka.log.LogManager)
[2025-05-21 12:47:58,824] INFO [broker-5-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,825] INFO [broker-5-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,825] INFO [broker-5-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,825] INFO [broker-5-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,826] INFO [broker-5-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,826] INFO [broker-5-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,826] INFO [broker-5-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,826] INFO [broker-5-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,826] INFO [broker-5-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,827] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,827] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,827] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 12:47:58,829] INFO [SocketServer listenerType=BROKER, nodeId=5] Shutting down socket server (kafka.network.SocketServer)
[2025-05-21 12:47:58,837] INFO [SocketServer listenerType=BROKER, nodeId=5] Shutdown completed (kafka.network.SocketServer)
[2025-05-21 12:47:58,838] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
[2025-05-21 12:47:58,839] INFO [BrokerLifecycleManager id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,840] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 12:47:58,841] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 12:47:58,841] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 12:47:58,842] INFO [SharedServer id=5] Stopping SharedServer (kafka.server.SharedServer)
[2025-05-21 12:47:58,843] INFO [MetadataLoader id=5] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,843] INFO [SnapshotGenerator id=5] close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,844] INFO [SnapshotGenerator id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,844] INFO [MetadataLoader id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,845] INFO [SnapshotGenerator id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 12:47:58,846] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 12:47:58,983] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 12:47:58,983] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 12:47:58,984] INFO [kafka-5-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 12:47:58,985] INFO [RaftManager id=5] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 12:47:58,985] INFO [RaftManager id=5] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 12:47:58,985] INFO [RaftManager id=5] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 12:47:58,986] INFO [kafka-5-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 12:47:58,986] INFO [kafka-5-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 12:47:58,991] INFO [kafka-5-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 12:47:58,992] INFO [kafka-5-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 12:47:58,992] INFO [kafka-5-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 12:47:58,995] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 12473 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 12:47:58,998] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-21 12:47:58,998] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2025-05-21 12:47:58,999] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-21 12:47:58,999] INFO App info kafka.server for 5 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 12:47:59,000] INFO [BrokerServer id=5] shut down completed (kafka.server.BrokerServer)
[2025-05-21 12:47:59,000] INFO [BrokerServer id=5] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
[2025-05-21 12:47:59,001] INFO App info kafka.server for 5 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 14:13:10,574] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-21 14:13:10,891] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-2:19092,PLAINTEXT_HOST://localhost:39092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 5
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 5
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-21 14:13:10,910] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-21 14:13:10,913] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:17,917] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-21 14:13:18,360] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-21 14:13:18,364] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:19,096] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:19,112] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:19,183] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-21 14:13:19,198] INFO [BrokerServer id=5] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-21 14:13:19,201] INFO [SharedServer id=5] Starting SharedServer (kafka.server.SharedServer)
[2025-05-21 14:13:19,209] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:19,468] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
[2025-05-21 14:13:19,483] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:19,488] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:19,489] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000008082.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:19,490] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000012473.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:19,498] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 9ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:20,504] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 12473 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:20,601] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 12473 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:20,616] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 12473 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:20,628] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=12473, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000012473.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:20,637] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 9ms for snapshot load and 0ms for segment recovery from offset 12473 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:20,705] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-21 14:13:20,844] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 14:13:20,847] INFO [RaftManager id=5] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 14:13:21,050] INFO [RaftManager id=5] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 14:13:21,288] INFO [RaftManager id=5] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=17, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from null (org.apache.kafka.raft.QuorumState)
[2025-05-21 14:13:21,295] INFO [kafka-5-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 14:13:21,306] INFO [kafka-5-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 14:13:21,368] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,376] INFO [BrokerServer id=5] Starting broker (kafka.server.BrokerServer)
[2025-05-21 14:13:21,386] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:21,435] INFO [broker-5-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:13:21,435] INFO [broker-5-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:13:21,443] INFO [broker-5-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:13:21,456] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:13:21,471] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,475] INFO [RaftManager id=5] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1591370065 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 14:13:21,488] INFO [BrokerServer id=5] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-21 14:13:21,491] INFO [BrokerServer id=5] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-21 14:13:21,511] INFO [broker-5-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:21,515] INFO [broker-5-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-1:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:21,531] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 14:13:21,574] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,672] INFO [RaftManager id=5] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=18, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=17, leader=kafka-controller-1:9093 (id: 1 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-21 14:13:21,676] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,729] INFO [RaftManager id=5] High watermark set to Optional[LogOffsetMetadata(offset=12477, metadata=Optional.empty)] for the first time for epoch 18 (org.apache.kafka.raft.FollowerState)
[2025-05-21 14:13:21,735] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 12477 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:21,854] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-21 14:13:21,879] INFO [SocketServer listenerType=BROKER, nodeId=5] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-21 14:13:21,886] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-21 14:13:21,912] INFO [SocketServer listenerType=BROKER, nodeId=5] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-21 14:13:21,948] INFO [broker-5-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:21,952] INFO [broker-5-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:21,992] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:21,996] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:22,017] INFO [ExpirationReaper-5-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:22,022] INFO [ExpirationReaper-5-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:22,056] INFO [ExpirationReaper-5-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:22,055] INFO [ExpirationReaper-5-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:22,073] INFO [ExpirationReaper-5-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:22,106] INFO [ExpirationReaper-5-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:22,123] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 12477 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,127] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 12476 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,146] INFO [ExpirationReaper-5-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:22,480] INFO [BrokerLifecycleManager id=5] Incarnation 9VNHj3p6SpCATxlLbNyC6A of broker 5 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:13:22,493] INFO [ExpirationReaper-5-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:13:22,493] INFO [broker-5-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:22,546] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:22,706] INFO [BrokerServer id=5] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-21 14:13:22,708] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing MetadataVersionPublisher(id=5) with a snapshot at offset 12476 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,722] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 12476 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:22,710] INFO [BrokerServer id=5] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-21 14:13:22,735] INFO [BrokerMetadataPublisher id=5] Publishing initial metadata at offset OffsetAndEpoch(offset=12476, epoch=18) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-21 14:13:22,739] INFO [BrokerServer id=5] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-21 14:13:22,771] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:22,873] INFO Skipping recovery of 72 logs from /tmp/kafka-logs since clean shutdown file was found (kafka.log.LogManager)
[2025-05-21 14:13:22,966] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-13/00000000000000053728.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:22,970] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Loading producer state till offset 255889 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:22,972] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255889 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:22,974] INFO [ProducerStateManager partition=financial_transactions-13] Loading producer state from snapshot file 'SnapshotFile(offset=255889, file=/tmp/kafka-logs/financial_transactions-13/00000000000000255889.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:22,982] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Producer state recovery took 8ms for snapshot load and 0ms for segment recovery from offset 255889 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,022] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-13, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255889) with 1 segments, local-log-start-offset 0 and log-end-offset 255889 in 95ms (1/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:23,115] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:13:23,126] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:23,155] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-18/00000000000000053605.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:23,222] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Loading producer state till offset 255590 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,177] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:13:23,240] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255590 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,247] INFO [ProducerStateManager partition=financial_transactions-18] Loading producer state from snapshot file 'SnapshotFile(offset=255590, file=/tmp/kafka-logs/financial_transactions-18/00000000000000255590.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:23,281] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Producer state recovery took 34ms for snapshot load and 0ms for segment recovery from offset 255590 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,285] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-18, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255590) with 1 segments, local-log-start-offset 0 and log-end-offset 255590 in 237ms (2/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:23,590] INFO [BrokerLifecycleManager id=5] Successfully registered broker 5 with broker epoch 12481 (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:13:23,699] INFO Deleted producer state snapshot /tmp/kafka-logs/__consumer_offsets-29/00000000000000000004.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:23,718] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 6 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,721] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 6 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,728] INFO [ProducerStateManager partition=__consumer_offsets-29] Loading producer state from snapshot file 'SnapshotFile(offset=6, file=/tmp/kafka-logs/__consumer_offsets-29/00000000000000000006.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:23,734] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Producer state recovery took 7ms for snapshot load and 0ms for segment recovery from offset 6 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,742] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-29, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=29, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=6) with 1 segments, local-log-start-offset 0 and log-end-offset 6 in 144ms (3/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:23,773] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-12/00000000000000053441.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:23,774] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Loading producer state till offset 255590 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,775] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255590 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,776] INFO [ProducerStateManager partition=financial_transactions-12] Loading producer state from snapshot file 'SnapshotFile(offset=255590, file=/tmp/kafka-logs/financial_transactions-12/00000000000000255590.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:23,780] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Producer state recovery took 5ms for snapshot load and 0ms for segment recovery from offset 255590 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,785] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-12, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255590) with 1 segments, local-log-start-offset 0 and log-end-offset 255590 in 19ms (4/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:23,826] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,844] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-38, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=38, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 57ms (5/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:23,903] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-16/00000000000000053208.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:23,906] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Loading producer state till offset 255412 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,917] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255412 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,919] INFO [ProducerStateManager partition=financial_transactions-16] Loading producer state from snapshot file 'SnapshotFile(offset=255412, file=/tmp/kafka-logs/financial_transactions-16/00000000000000255412.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:23,933] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Producer state recovery took 14ms for snapshot load and 0ms for segment recovery from offset 255412 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,943] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-16, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255412) with 1 segments, local-log-start-offset 0 and log-end-offset 255412 in 97ms (6/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:23,952] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,966] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-27, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=27, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 19ms (7/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:23,979] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:23,990] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-39, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=39, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 23ms (8/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,041] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,049] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-42, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=42, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 58ms (9/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,067] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,080] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-9, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 30ms (10/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,087] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,095] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-22, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=22, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (11/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,105] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,119] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-26, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=26, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 20ms (12/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,174] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,178] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-40, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=40, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 56ms (13/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,210] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,220] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-47, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=47, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 32ms (14/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,241] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,260] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-15, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 38ms (15/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,277] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,300] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-10, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 34ms (16/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,337] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-11/00000000000000053486.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:24,342] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Loading producer state till offset 256046 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,343] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 256046 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,352] INFO [ProducerStateManager partition=financial_transactions-11] Loading producer state from snapshot file 'SnapshotFile(offset=256046, file=/tmp/kafka-logs/financial_transactions-11/00000000000000256046.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:24,354] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 256046 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,384] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-11, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=256046) with 1 segments, local-log-start-offset 0 and log-end-offset 256046 in 83ms (17/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,440] INFO [LogLoader partition=aggregated_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 352 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,443] INFO [LogLoader partition=aggregated_transactions-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 352 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,450] INFO [ProducerStateManager partition=aggregated_transactions-0] Loading producer state from snapshot file 'SnapshotFile(offset=352, file=/tmp/kafka-logs/aggregated_transactions-0/00000000000000000352.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:24,454] INFO [LogLoader partition=aggregated_transactions-0, dir=/tmp/kafka-logs] Producer state recovery took 4ms for snapshot load and 0ms for segment recovery from offset 352 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,461] INFO Completed load of Log(dir=/tmp/kafka-logs/aggregated_transactions-0, topicId=tZKMqbfwSlSmyms8wDFH7g, topic=aggregated_transactions, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=352) with 1 segments, local-log-start-offset 0 and log-end-offset 352 in 68ms (18/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,486] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,489] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-2, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 16ms (19/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,506] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,526] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-18, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 24ms (20/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,542] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,545] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-46, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=46, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 17ms (21/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,563] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-1/00000000000000053591.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:24,580] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Loading producer state till offset 255371 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,587] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255371 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,588] INFO [ProducerStateManager partition=financial_transactions-1] Loading producer state from snapshot file 'SnapshotFile(offset=255371, file=/tmp/kafka-logs/financial_transactions-1/00000000000000255371.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:24,597] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Producer state recovery took 9ms for snapshot load and 0ms for segment recovery from offset 255371 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,599] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-1, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255371) with 1 segments, local-log-start-offset 0 and log-end-offset 255371 in 54ms (22/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,619] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,622] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-12, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 21ms (23/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,626] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,635] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-23, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=23, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (24/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,642] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,645] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-14, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (25/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,657] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,661] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-5, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (26/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,675] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,687] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-41, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=41, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 22ms (27/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,718] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-17/00000000000000053580.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:24,722] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Loading producer state till offset 255866 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,734] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255866 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,746] INFO [ProducerStateManager partition=financial_transactions-17] Loading producer state from snapshot file 'SnapshotFile(offset=255866, file=/tmp/kafka-logs/financial_transactions-17/00000000000000255866.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:24,757] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Producer state recovery took 11ms for snapshot load and 0ms for segment recovery from offset 255866 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,762] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-17, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255866) with 1 segments, local-log-start-offset 0 and log-end-offset 255866 in 66ms (28/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,767] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,790] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-1, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 27ms (29/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,799] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,814] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-19, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 23ms (30/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,832] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,836] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-3, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 22ms (31/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,843] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,848] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-16, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (32/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,861] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,868] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-25, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=25, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 12ms (33/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,887] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-15/00000000000000053502.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:24,899] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Loading producer state till offset 256237 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,903] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 256237 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,906] INFO [ProducerStateManager partition=financial_transactions-15] Loading producer state from snapshot file 'SnapshotFile(offset=256237, file=/tmp/kafka-logs/financial_transactions-15/00000000000000256237.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:24,910] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Producer state recovery took 4ms for snapshot load and 0ms for segment recovery from offset 256237 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,918] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-15, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=256237) with 1 segments, local-log-start-offset 0 and log-end-offset 256237 in 46ms (34/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,928] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-3/00000000000000053622.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:24,930] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Loading producer state till offset 255715 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,931] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255715 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,932] INFO [ProducerStateManager partition=financial_transactions-3] Loading producer state from snapshot file 'SnapshotFile(offset=255715, file=/tmp/kafka-logs/financial_transactions-3/00000000000000255715.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:24,934] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 255715 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,939] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-3, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255715) with 1 segments, local-log-start-offset 0 and log-end-offset 255715 in 19ms (35/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:24,969] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-10/00000000000000053252.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:24,974] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Loading producer state till offset 256308 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,974] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 256308 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,976] INFO [ProducerStateManager partition=financial_transactions-10] Loading producer state from snapshot file 'SnapshotFile(offset=256308, file=/tmp/kafka-logs/financial_transactions-10/00000000000000256308.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:24,985] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Producer state recovery took 9ms for snapshot load and 0ms for segment recovery from offset 256308 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:24,987] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-10, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=256308) with 1 segments, local-log-start-offset 0 and log-end-offset 256308 in 35ms (36/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,003] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-19/00000000000000053309.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,003] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Loading producer state till offset 255167 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,004] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255167 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,009] INFO [ProducerStateManager partition=financial_transactions-19] Loading producer state from snapshot file 'SnapshotFile(offset=255167, file=/tmp/kafka-logs/financial_transactions-19/00000000000000255167.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,010] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 255167 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,014] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-19, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255167) with 1 segments, local-log-start-offset 0 and log-end-offset 255167 in 27ms (37/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,031] INFO Deleted producer state snapshot /tmp/kafka-logs/_schemas-0/00000000000000000004.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,035] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 6 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,036] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 6 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,036] INFO [ProducerStateManager partition=_schemas-0] Loading producer state from snapshot file 'SnapshotFile(offset=6, file=/tmp/kafka-logs/_schemas-0/00000000000000000006.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,038] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 6 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,040] INFO Completed load of Log(dir=/tmp/kafka-logs/_schemas-0, topicId=RrE8eovWRKu4kLR3MRJ0fA, topic=_schemas, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=6) with 1 segments, local-log-start-offset 0 and log-end-offset 6 in 24ms (38/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,051] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,054] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-28, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=28, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (39/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,068] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,076] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-13, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 21ms (40/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,090] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,101] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-24, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=24, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 22ms (41/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,134] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,154] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-32, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=32, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 48ms (42/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,169] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-14/00000000000000053468.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,171] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Loading producer state till offset 255812 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,171] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255812 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,173] INFO [ProducerStateManager partition=financial_transactions-14] Loading producer state from snapshot file 'SnapshotFile(offset=255812, file=/tmp/kafka-logs/financial_transactions-14/00000000000000255812.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,186] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Producer state recovery took 13ms for snapshot load and 0ms for segment recovery from offset 255812 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,198] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-14, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255812) with 1 segments, local-log-start-offset 0 and log-end-offset 255812 in 42ms (43/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,228] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-2/00000000000000053173.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,229] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Loading producer state till offset 255541 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,229] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255541 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,230] INFO [ProducerStateManager partition=financial_transactions-2] Loading producer state from snapshot file 'SnapshotFile(offset=255541, file=/tmp/kafka-logs/financial_transactions-2/00000000000000255541.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,232] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 255541 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,243] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-2, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255541) with 1 segments, local-log-start-offset 0 and log-end-offset 255541 in 44ms (44/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,251] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,253] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-31, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=31, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (45/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,274] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,276] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-21, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=21, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 21ms (46/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,296] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-0/00000000000000053808.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,322] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 257211 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,323] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 257211 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,324] INFO [ProducerStateManager partition=financial_transactions-0] Loading producer state from snapshot file 'SnapshotFile(offset=257211, file=/tmp/kafka-logs/financial_transactions-0/00000000000000257211.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,325] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 257211 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,326] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-0, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=257211) with 1 segments, local-log-start-offset 0 and log-end-offset 257211 in 48ms (47/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,347] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-9/00000000000000053169.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,349] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Loading producer state till offset 255435 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,352] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255435 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,362] INFO [ProducerStateManager partition=financial_transactions-9] Loading producer state from snapshot file 'SnapshotFile(offset=255435, file=/tmp/kafka-logs/financial_transactions-9/00000000000000255435.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,381] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Producer state recovery took 19ms for snapshot load and 0ms for segment recovery from offset 255435 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,384] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-9, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255435) with 1 segments, local-log-start-offset 0 and log-end-offset 255435 in 51ms (48/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,402] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,404] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-6, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 20ms (49/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,408] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,413] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-11, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (50/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,432] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-5/00000000000000053395.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,434] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Loading producer state till offset 254990 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,435] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 254990 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,436] INFO [ProducerStateManager partition=financial_transactions-5] Loading producer state from snapshot file 'SnapshotFile(offset=254990, file=/tmp/kafka-logs/financial_transactions-5/00000000000000254990.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,438] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 254990 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,440] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-5, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=254990) with 1 segments, local-log-start-offset 0 and log-end-offset 254990 in 22ms (51/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,444] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,457] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-30, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=30, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (52/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,469] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-6/00000000000000053206.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,471] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Loading producer state till offset 255175 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,476] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255175 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,476] INFO [ProducerStateManager partition=financial_transactions-6] Loading producer state from snapshot file 'SnapshotFile(offset=255175, file=/tmp/kafka-logs/financial_transactions-6/00000000000000255175.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,478] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 1ms for segment recovery from offset 255175 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,482] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-6, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255175) with 1 segments, local-log-start-offset 0 and log-end-offset 255175 in 23ms (53/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,493] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,497] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-43, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=43, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (54/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,502] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,512] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-7, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (55/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,519] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,524] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-33, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=33, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (56/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,573] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,575] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-36, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=36, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 41ms (57/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,593] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,604] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-45, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=45, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 28ms (58/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,617] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,619] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-0, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (59/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,628] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,631] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-20, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=20, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (60/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,635] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,639] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-4, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (61/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,675] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,693] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-8, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 53ms (62/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,721] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,732] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-49, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=49, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 38ms (63/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,739] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,742] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-34, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=34, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (64/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,749] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,753] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-48, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=48, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (65/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,764] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-4/00000000000000053258.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,764] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Loading producer state till offset 255335 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,765] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255335 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,766] INFO [ProducerStateManager partition=financial_transactions-4] Loading producer state from snapshot file 'SnapshotFile(offset=255335, file=/tmp/kafka-logs/financial_transactions-4/00000000000000255335.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,768] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 255335 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,771] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-4, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255335) with 1 segments, local-log-start-offset 0 and log-end-offset 255335 in 17ms (66/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,774] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,781] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-44, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=44, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (67/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,798] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,799] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-37, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=37, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (68/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,814] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-7/00000000000000053438.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,817] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Loading producer state till offset 255844 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,818] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255844 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,819] INFO [ProducerStateManager partition=financial_transactions-7] Loading producer state from snapshot file 'SnapshotFile(offset=255844, file=/tmp/kafka-logs/financial_transactions-7/00000000000000255844.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,821] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 0ms for segment recovery from offset 255844 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,830] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-7, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255844) with 1 segments, local-log-start-offset 0 and log-end-offset 255844 in 31ms (69/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,835] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,838] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-35, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=35, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (70/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,867] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,870] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-17, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 28ms (71/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,896] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-8/00000000000000053130.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-21 14:13:25,897] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Loading producer state till offset 255525 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,898] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 255525 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,899] INFO [ProducerStateManager partition=financial_transactions-8] Loading producer state from snapshot file 'SnapshotFile(offset=255525, file=/tmp/kafka-logs/financial_transactions-8/00000000000000255525.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:13:25,911] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Producer state recovery took 12ms for snapshot load and 0ms for segment recovery from offset 255525 (kafka.log.UnifiedLog$)
[2025-05-21 14:13:25,924] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-8, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=255525) with 1 segments, local-log-start-offset 0 and log-end-offset 255525 in 53ms (72/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-21 14:13:25,935] INFO Loaded 72 logs in 3159ms (kafka.log.LogManager)
[2025-05-21 14:13:25,937] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-21 14:13:25,938] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-21 14:13:25,944] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-21 14:13:26,152] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 14:13:26,160] INFO [GroupCoordinator 5]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,168] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 14:13:26,163] INFO [AddPartitionsToTxnSenderThread-5]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 14:13:26,174] INFO [GroupCoordinator 5]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,191] INFO [TransactionCoordinator id=5] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 14:13:26,197] INFO [TxnMarkerSenderThread-5]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 14:13:26,197] INFO [TransactionCoordinator id=5] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 14:13:26,213] INFO [Broker id=5] Transitioning 72 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:13:26,218] INFO [Broker id=5] Creating new partition financial_transactions-13 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,270] INFO [Partition financial_transactions-13 broker=5] Log loaded for partition financial_transactions-13 with initial high watermark 255874 (kafka.cluster.Partition)
[2025-05-21 14:13:26,283] INFO [Broker id=5] Follower financial_transactions-13 starts at leader epoch 7 from offset 255889 with partition epoch 10 and high watermark 255874. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:13:26,296] INFO [Broker id=5] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,299] INFO [Partition __consumer_offsets-13 broker=5] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,300] INFO [Broker id=5] Follower __consumer_offsets-13 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,300] INFO [Broker id=5] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,301] INFO [Partition __consumer_offsets-46 broker=5] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,302] INFO [Broker id=5] Follower __consumer_offsets-46 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,302] INFO [Broker id=5] Creating new partition financial_transactions-17 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,310] INFO [Partition financial_transactions-17 broker=5] Log loaded for partition financial_transactions-17 with initial high watermark 255866 (kafka.cluster.Partition)
[2025-05-21 14:13:26,317] INFO [Broker id=5] Follower financial_transactions-17 starts at leader epoch 4 from offset 255866 with partition epoch 9 and high watermark 255866. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 14:13:26,318] INFO [Broker id=5] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,320] INFO [Partition __consumer_offsets-9 broker=5] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,320] INFO [Broker id=5] Follower __consumer_offsets-9 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,321] INFO [Broker id=5] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,322] INFO [Partition __consumer_offsets-42 broker=5] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,323] INFO [Broker id=5] Follower __consumer_offsets-42 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,323] INFO [Broker id=5] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,324] INFO [Partition __consumer_offsets-21 broker=5] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,325] INFO [Broker id=5] Follower __consumer_offsets-21 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,325] INFO [Broker id=5] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,327] INFO [Partition __consumer_offsets-17 broker=5] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,327] INFO [Broker id=5] Follower __consumer_offsets-17 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,327] INFO [Broker id=5] Creating new partition financial_transactions-0 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,338] INFO [Partition financial_transactions-0 broker=5] Log loaded for partition financial_transactions-0 with initial high watermark 257197 (kafka.cluster.Partition)
[2025-05-21 14:13:26,342] INFO [Broker id=5] Follower financial_transactions-0 starts at leader epoch 5 from offset 257211 with partition epoch 9 and high watermark 257197. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,343] INFO [Broker id=5] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,344] INFO [Partition __consumer_offsets-30 broker=5] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,346] INFO [Broker id=5] Follower __consumer_offsets-30 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,348] INFO [Broker id=5] Creating new partition financial_transactions-4 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,349] INFO [Partition financial_transactions-4 broker=5] Log loaded for partition financial_transactions-4 with initial high watermark 255335 (kafka.cluster.Partition)
[2025-05-21 14:13:26,351] INFO [Broker id=5] Follower financial_transactions-4 starts at leader epoch 5 from offset 255335 with partition epoch 10 and high watermark 255335. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,360] INFO [Broker id=5] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,362] INFO [Partition __consumer_offsets-26 broker=5] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,363] INFO [Broker id=5] Follower __consumer_offsets-26 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,363] INFO [Broker id=5] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,364] INFO [Partition __consumer_offsets-5 broker=5] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,372] INFO [Broker id=5] Follower __consumer_offsets-5 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,374] INFO [Broker id=5] Creating new partition financial_transactions-8 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,378] INFO [Partition financial_transactions-8 broker=5] Log loaded for partition financial_transactions-8 with initial high watermark 255525 (kafka.cluster.Partition)
[2025-05-21 14:13:26,378] INFO [Broker id=5] Follower financial_transactions-8 starts at leader epoch 7 from offset 255525 with partition epoch 10 and high watermark 255525. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:13:26,379] INFO [Broker id=5] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,387] INFO [Partition __consumer_offsets-38 broker=5] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,391] INFO [Broker id=5] Follower __consumer_offsets-38 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,395] INFO [Broker id=5] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,396] INFO [Partition __consumer_offsets-1 broker=5] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,413] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,414] INFO [Broker id=5] Creating new partition financial_transactions-12 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,417] INFO [Partition financial_transactions-12 broker=5] Log loaded for partition financial_transactions-12 with initial high watermark 255590 (kafka.cluster.Partition)
[2025-05-21 14:13:26,417] INFO [Broker id=5] Follower financial_transactions-12 starts at leader epoch 5 from offset 255590 with partition epoch 9 and high watermark 255590. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,418] INFO [Broker id=5] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,420] INFO [Partition __consumer_offsets-34 broker=5] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,422] INFO [Broker id=5] Follower __consumer_offsets-34 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,424] INFO [Broker id=5] Creating new partition financial_transactions-14 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,426] INFO [Partition financial_transactions-14 broker=5] Log loaded for partition financial_transactions-14 with initial high watermark 255812 (kafka.cluster.Partition)
[2025-05-21 14:13:26,432] INFO [Broker id=5] Follower financial_transactions-14 starts at leader epoch 5 from offset 255812 with partition epoch 10 and high watermark 255812. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,436] INFO [Broker id=5] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,438] INFO [Partition __consumer_offsets-16 broker=5] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,439] INFO [Broker id=5] Follower __consumer_offsets-16 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,439] INFO [Broker id=5] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-21 14:13:26,442] INFO [Partition _schemas-0 broker=5] Log loaded for partition _schemas-0 with initial high watermark 6 (kafka.cluster.Partition)
[2025-05-21 14:13:26,449] INFO [Broker id=5] Follower _schemas-0 starts at leader epoch 15 from offset 6 with partition epoch 25 and high watermark 6. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,450] INFO [Broker id=5] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,451] INFO [Partition __consumer_offsets-45 broker=5] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,452] INFO [Broker id=5] Follower __consumer_offsets-45 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,453] INFO [Broker id=5] Creating new partition financial_transactions-18 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,454] INFO [Partition financial_transactions-18 broker=5] Log loaded for partition financial_transactions-18 with initial high watermark 255590 (kafka.cluster.Partition)
[2025-05-21 14:13:26,456] INFO [Broker id=5] Follower financial_transactions-18 starts at leader epoch 4 from offset 255590 with partition epoch 9 and high watermark 255590. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 14:13:26,457] INFO [Broker id=5] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,461] INFO [Partition __consumer_offsets-12 broker=5] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,462] INFO [Broker id=5] Follower __consumer_offsets-12 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,463] INFO [Broker id=5] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,466] INFO [Partition __consumer_offsets-41 broker=5] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,469] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,469] INFO [Broker id=5] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,471] INFO [Partition __consumer_offsets-24 broker=5] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,472] INFO [Broker id=5] Follower __consumer_offsets-24 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,474] INFO [Broker id=5] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,497] INFO [Partition __consumer_offsets-20 broker=5] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,499] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,504] INFO [Broker id=5] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,505] INFO [Partition __consumer_offsets-49 broker=5] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,507] INFO [Broker id=5] Follower __consumer_offsets-49 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,507] INFO [Broker id=5] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,508] INFO [Partition __consumer_offsets-0 broker=5] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,510] INFO [Broker id=5] Follower __consumer_offsets-0 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,510] INFO [Broker id=5] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,512] INFO [Partition __consumer_offsets-29 broker=5] Log loaded for partition __consumer_offsets-29 with initial high watermark 6 (kafka.cluster.Partition)
[2025-05-21 14:13:26,512] INFO [Broker id=5] Follower __consumer_offsets-29 starts at leader epoch 15 from offset 6 with partition epoch 25 and high watermark 6. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,514] INFO [Broker id=5] Creating new partition financial_transactions-1 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,518] INFO [Partition financial_transactions-1 broker=5] Log loaded for partition financial_transactions-1 with initial high watermark 255371 (kafka.cluster.Partition)
[2025-05-21 14:13:26,525] INFO [Broker id=5] Follower financial_transactions-1 starts at leader epoch 7 from offset 255371 with partition epoch 10 and high watermark 255371. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:13:26,526] INFO [Broker id=5] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,528] INFO [Partition __consumer_offsets-25 broker=5] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,528] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,532] INFO [Broker id=5] Creating new partition financial_transactions-5 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,534] INFO [Partition financial_transactions-5 broker=5] Log loaded for partition financial_transactions-5 with initial high watermark 254990 (kafka.cluster.Partition)
[2025-05-21 14:13:26,534] INFO [Broker id=5] Follower financial_transactions-5 starts at leader epoch 6 from offset 254990 with partition epoch 10 and high watermark 254990. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:26,538] INFO [Broker id=5] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,541] INFO [Partition __consumer_offsets-8 broker=5] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,542] INFO [Broker id=5] Follower __consumer_offsets-8 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,542] INFO [Broker id=5] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,545] INFO [Partition __consumer_offsets-37 broker=5] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,555] INFO [Broker id=5] Follower __consumer_offsets-37 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,561] INFO [Broker id=5] Creating new partition financial_transactions-9 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,563] INFO [Partition financial_transactions-9 broker=5] Log loaded for partition financial_transactions-9 with initial high watermark 255435 (kafka.cluster.Partition)
[2025-05-21 14:13:26,564] INFO [Broker id=5] Follower financial_transactions-9 starts at leader epoch 6 from offset 255435 with partition epoch 10 and high watermark 255435. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:26,564] INFO [Broker id=5] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,566] INFO [Partition __consumer_offsets-4 broker=5] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,568] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,569] INFO [Broker id=5] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,571] INFO [Partition __consumer_offsets-33 broker=5] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,572] INFO [Broker id=5] Follower __consumer_offsets-33 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,573] INFO [Broker id=5] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,575] INFO [Partition __consumer_offsets-15 broker=5] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,575] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,576] INFO [Broker id=5] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,582] INFO [Partition __consumer_offsets-48 broker=5] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,583] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,583] INFO [Broker id=5] Creating new partition financial_transactions-15 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,584] INFO [Partition financial_transactions-15 broker=5] Log loaded for partition financial_transactions-15 with initial high watermark 256237 (kafka.cluster.Partition)
[2025-05-21 14:13:26,585] INFO [Broker id=5] Follower financial_transactions-15 starts at leader epoch 5 from offset 256237 with partition epoch 10 and high watermark 256237. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,585] INFO [Broker id=5] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,586] INFO [Partition __consumer_offsets-11 broker=5] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,588] INFO [Broker id=5] Follower __consumer_offsets-11 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,589] INFO [Broker id=5] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,591] INFO [Partition __consumer_offsets-44 broker=5] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,592] INFO [Broker id=5] Follower __consumer_offsets-44 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,592] INFO [Broker id=5] Creating new partition financial_transactions-19 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,607] INFO [Partition financial_transactions-19 broker=5] Log loaded for partition financial_transactions-19 with initial high watermark 255167 (kafka.cluster.Partition)
[2025-05-21 14:13:26,607] INFO [Broker id=5] Follower financial_transactions-19 starts at leader epoch 5 from offset 255167 with partition epoch 10 and high watermark 255167. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,607] INFO [Broker id=5] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,609] INFO [Partition __consumer_offsets-23 broker=5] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,609] INFO [Broker id=5] Follower __consumer_offsets-23 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,613] INFO [Broker id=5] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,619] INFO [Partition __consumer_offsets-19 broker=5] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,621] INFO [Broker id=5] Follower __consumer_offsets-19 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,622] INFO [Broker id=5] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,623] INFO [Partition __consumer_offsets-32 broker=5] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,623] INFO [Broker id=5] Follower __consumer_offsets-32 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,624] INFO [Broker id=5] Creating new partition financial_transactions-2 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,625] INFO [Partition financial_transactions-2 broker=5] Log loaded for partition financial_transactions-2 with initial high watermark 255541 (kafka.cluster.Partition)
[2025-05-21 14:13:26,626] INFO [Broker id=5] Follower financial_transactions-2 starts at leader epoch 5 from offset 255541 with partition epoch 10 and high watermark 255541. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,628] INFO [Broker id=5] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,629] INFO [Partition __consumer_offsets-28 broker=5] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,630] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,631] INFO [Broker id=5] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,636] INFO [Partition __consumer_offsets-7 broker=5] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,636] INFO [Broker id=5] Follower __consumer_offsets-7 starts at leader epoch 14 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:26,637] INFO [Broker id=5] Creating new partition financial_transactions-6 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,640] INFO [Partition financial_transactions-6 broker=5] Log loaded for partition financial_transactions-6 with initial high watermark 255175 (kafka.cluster.Partition)
[2025-05-21 14:13:26,641] INFO [Broker id=5] Follower financial_transactions-6 starts at leader epoch 5 from offset 255175 with partition epoch 10 and high watermark 255175. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,642] INFO [Broker id=5] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,644] INFO [Partition __consumer_offsets-40 broker=5] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,644] INFO [Broker id=5] Follower __consumer_offsets-40 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,645] INFO [Broker id=5] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,646] INFO [Partition __consumer_offsets-3 broker=5] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,647] INFO [Broker id=5] Follower __consumer_offsets-3 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,648] INFO [Broker id=5] Creating new partition financial_transactions-10 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,649] INFO [Partition financial_transactions-10 broker=5] Log loaded for partition financial_transactions-10 with initial high watermark 256277 (kafka.cluster.Partition)
[2025-05-21 14:13:26,650] INFO [Broker id=5] Follower financial_transactions-10 starts at leader epoch 4 from offset 256308 with partition epoch 9 and high watermark 256277. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 14:13:26,651] INFO [Broker id=5] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,654] INFO [Partition __consumer_offsets-36 broker=5] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,655] INFO [Broker id=5] Follower __consumer_offsets-36 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,656] INFO [Broker id=5] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,659] INFO [Partition __consumer_offsets-47 broker=5] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,662] INFO [Broker id=5] Follower __consumer_offsets-47 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,662] INFO [Broker id=5] Creating new partition financial_transactions-16 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,664] INFO [Partition financial_transactions-16 broker=5] Log loaded for partition financial_transactions-16 with initial high watermark 255412 (kafka.cluster.Partition)
[2025-05-21 14:13:26,665] INFO [Broker id=5] Follower financial_transactions-16 starts at leader epoch 6 from offset 255412 with partition epoch 10 and high watermark 255412. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:26,666] INFO [Broker id=5] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,671] INFO [Partition __consumer_offsets-14 broker=5] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,673] INFO [Broker id=5] Follower __consumer_offsets-14 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,676] INFO [Broker id=5] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,680] INFO [Partition __consumer_offsets-43 broker=5] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,680] INFO [Broker id=5] Follower __consumer_offsets-43 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,683] INFO [Broker id=5] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,687] INFO [Partition __consumer_offsets-10 broker=5] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,688] INFO [Broker id=5] Follower __consumer_offsets-10 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,688] INFO [Broker id=5] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,690] INFO [Partition __consumer_offsets-22 broker=5] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,690] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 12 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:26,692] INFO [Broker id=5] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,693] INFO [Partition __consumer_offsets-18 broker=5] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,694] INFO [Broker id=5] Follower __consumer_offsets-18 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,695] INFO [Broker id=5] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,697] INFO [Partition __consumer_offsets-31 broker=5] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,700] INFO [Broker id=5] Follower __consumer_offsets-31 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,700] INFO [Broker id=5] Creating new partition aggregated_transactions-0 with topic id tZKMqbfwSlSmyms8wDFH7g. (state.change.logger)
[2025-05-21 14:13:26,701] INFO [Partition aggregated_transactions-0 broker=5] Log loaded for partition aggregated_transactions-0 with initial high watermark 352 (kafka.cluster.Partition)
[2025-05-21 14:13:26,705] INFO [Broker id=5] Follower aggregated_transactions-0 starts at leader epoch 3 from offset 352 with partition epoch 3 and high watermark 352. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 14:13:26,706] INFO [Broker id=5] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,710] INFO [Partition __consumer_offsets-27 broker=5] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,711] INFO [Broker id=5] Follower __consumer_offsets-27 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,711] INFO [Broker id=5] Creating new partition financial_transactions-3 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,714] INFO [Partition financial_transactions-3 broker=5] Log loaded for partition financial_transactions-3 with initial high watermark 255715 (kafka.cluster.Partition)
[2025-05-21 14:13:26,717] INFO [Broker id=5] Follower financial_transactions-3 starts at leader epoch 4 from offset 255715 with partition epoch 9 and high watermark 255715. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 14:13:26,717] INFO [Broker id=5] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,719] INFO [Partition __consumer_offsets-39 broker=5] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,719] INFO [Broker id=5] Follower __consumer_offsets-39 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,719] INFO [Broker id=5] Creating new partition financial_transactions-7 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,724] INFO [Partition financial_transactions-7 broker=5] Log loaded for partition financial_transactions-7 with initial high watermark 255844 (kafka.cluster.Partition)
[2025-05-21 14:13:26,725] INFO [Broker id=5] Follower financial_transactions-7 starts at leader epoch 5 from offset 255844 with partition epoch 9 and high watermark 255844. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,725] INFO [Broker id=5] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,728] INFO [Partition __consumer_offsets-6 broker=5] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,730] INFO [Broker id=5] Follower __consumer_offsets-6 starts at leader epoch 9 from offset 0 with partition epoch 24 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:26,730] INFO [Broker id=5] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,736] INFO [Partition __consumer_offsets-35 broker=5] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,741] INFO [Broker id=5] Follower __consumer_offsets-35 starts at leader epoch 10 from offset 0 with partition epoch 23 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:26,742] INFO [Broker id=5] Creating new partition financial_transactions-11 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-21 14:13:26,743] INFO [Partition financial_transactions-11 broker=5] Log loaded for partition financial_transactions-11 with initial high watermark 256033 (kafka.cluster.Partition)
[2025-05-21 14:13:26,744] INFO [Broker id=5] Follower financial_transactions-11 starts at leader epoch 5 from offset 256046 with partition epoch 10 and high watermark 256033. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:26,745] INFO [Broker id=5] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-21 14:13:26,748] INFO [Partition __consumer_offsets-2 broker=5] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-21 14:13:26,749] INFO [Broker id=5] Follower __consumer_offsets-2 starts at leader epoch 15 from offset 0 with partition epoch 25 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:26,751] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, aggregated_transactions-0, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-6, __consumer_offsets-35, financial_transactions-11, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:13:26,762] INFO [Broker id=5] Stopped fetchers as part of become-follower for 72 partitions (state.change.logger)
[2025-05-21 14:13:26,786] INFO [Broker id=5] Started fetchers as part of become-follower for 72 partitions (state.change.logger)
[2025-05-21 14:13:26,821] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,823] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,825] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,827] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,827] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,827] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,828] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,828] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,828] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,829] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,829] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,829] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,829] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,830] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,829] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,830] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,831] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,831] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,830] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,831] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,832] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,833] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,832] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,834] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,834] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,835] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,836] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,841] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,837] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,843] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,843] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,843] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,844] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,845] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,847] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,847] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,848] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,849] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,848] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,850] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,850] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,853] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,854] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,853] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,855] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,856] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,858] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,861] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,861] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,861] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,862] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,862] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,862] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,863] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,863] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,863] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,864] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,864] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,864] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,864] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,864] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,865] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,865] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,865] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,866] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,866] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,866] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,867] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,867] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,867] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,867] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,867] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,868] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,868] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,868] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,868] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,869] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,869] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,869] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,869] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,870] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,870] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,870] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,871] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,871] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,871] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,872] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,875] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,875] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,876] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,876] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,877] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,878] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,878] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,880] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,881] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,881] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,882] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,883] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,883] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,889] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,890] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,891] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,892] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,894] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,895] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,896] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,897] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,898] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,898] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,899] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,899] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,899] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,900] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,900] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,901] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,902] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,902] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,905] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,906] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,907] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,908] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,910] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,910] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,911] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,912] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,913] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,915] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,916] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,916] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,916] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,917] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,917] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,917] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,918] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,918] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,919] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,919] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,919] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,920] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,920] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,920] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[9] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,922] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,922] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[9]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,922] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[10] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,923] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,924] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[10]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,924] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:26,925] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,926] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:26,943] INFO [DynamicConfigPublisher broker id=5] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-21 14:13:26,960] INFO [DynamicConfigPublisher broker id=5] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-21 14:13:26,977] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=5) with a snapshot at offset 12476 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-21 14:13:26,996] INFO [BrokerLifecycleManager id=5] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:13:27,010] INFO [BrokerServer id=5] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-21 14:13:27,014] INFO [BrokerServer id=5] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-21 14:13:27,017] INFO [BrokerServer id=5] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-21 14:13:27,019] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-2:19092,PLAINTEXT_HOST://localhost:39092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 5
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 5
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-21 14:13:27,021] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-21 14:13:27,026] INFO [BrokerServer id=5] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-21 14:13:27,195] INFO [BrokerLifecycleManager id=5] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:13:27,337] INFO [BrokerLifecycleManager id=5] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:13:27,338] INFO [BrokerServer id=5] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-21 14:13:27,340] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-21 14:13:27,341] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-21 14:13:27,342] INFO [SocketServer listenerType=BROKER, nodeId=5] Enabling request processing. (kafka.network.SocketServer)
[2025-05-21 14:13:27,352] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-21 14:13:27,366] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-21 14:13:27,393] INFO [BrokerServer id=5] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-21 14:13:27,398] INFO [BrokerServer id=5] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-21 14:13:27,406] INFO [BrokerServer id=5] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-21 14:13:27,415] INFO [BrokerServer id=5] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-21 14:13:27,420] INFO [BrokerServer id=5] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-21 14:13:27,421] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 14:13:27,422] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 14:13:27,425] INFO Kafka startTimeMs: 1747836807421 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 14:13:27,427] INFO [KafkaRaftServer nodeId=5] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-21 14:13:27,491] INFO [Broker id=5] Transitioning 72 partition(s) to local leaders. (state.change.logger)
[2025-05-21 14:13:27,491] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, aggregated_transactions-0, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-6, __consumer_offsets-35, financial_transactions-11, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:13:27,520] INFO [Broker id=5] Leader financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 8 from offset 255889 with partition epoch 11, high watermark 255874, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:13:27,556] INFO [Broker id=5] Leader __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 26, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:27,624] INFO [Broker id=5] Leader __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 26, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:27,643] INFO [Broker id=5] Leader financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 5 from offset 255866 with partition epoch 10, high watermark 255866, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 14:13:27,659] INFO [Broker id=5] Leader __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 11 from offset 0 with partition epoch 24, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,673] INFO [Broker id=5] Leader __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 11 from offset 0 with partition epoch 24, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,687] INFO [Broker id=5] Leader __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 25, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:27,697] INFO [Broker id=5] Leader __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 25, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:27,710] INFO [Broker id=5] Leader financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 6 from offset 257211 with partition epoch 10, high watermark 257197, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:27,721] INFO [Broker id=5] Leader __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 26, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:27,732] INFO [Broker id=5] Leader financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 6 from offset 255335 with partition epoch 11, high watermark 255335, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:27,770] INFO [Broker id=5] Leader __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 26, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,804] INFO [Broker id=5] Leader __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 26, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,815] INFO [Broker id=5] Leader financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 8 from offset 255525 with partition epoch 11, high watermark 255525, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:13:27,821] INFO [Broker id=5] Leader __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 11 from offset 0 with partition epoch 24, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:27,839] INFO [Broker id=5] Leader __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 24, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:27,847] INFO [Broker id=5] Leader financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 6 from offset 255590 with partition epoch 10, high watermark 255590, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:27,855] INFO [Broker id=5] Leader __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 26, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:27,870] INFO [Broker id=5] Leader financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 6 from offset 255812 with partition epoch 11, high watermark 255812, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:27,888] INFO [Broker id=5] Leader __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 26, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,895] INFO [Broker id=5] Leader _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) starts at leader epoch 16 from offset 6 with partition epoch 26, high watermark 6, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,902] INFO [Broker id=5] Leader __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 25, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:27,909] INFO [Broker id=5] Leader financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 5 from offset 255590 with partition epoch 10, high watermark 255590, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 14:13:27,919] INFO [Broker id=5] Leader __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 25, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:27,926] INFO [Broker id=5] Leader __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 24, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:27,966] INFO [Broker id=5] Leader __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 25, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:27,983] INFO [Broker id=5] Leader __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 24, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:27,990] INFO [Broker id=5] Leader __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 26, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:27,997] INFO [Broker id=5] Leader __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 25, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:28,013] INFO [Broker id=5] Leader __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 6 with partition epoch 26, high watermark 6, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:28,026] INFO [Broker id=5] Leader financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 8 from offset 255371 with partition epoch 11, high watermark 255371, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:13:28,031] INFO [Broker id=5] Leader __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 24, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:28,063] INFO [Broker id=5] Leader financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 7 from offset 254990 with partition epoch 11, high watermark 254990, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:28,076] INFO [Broker id=5] Leader __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 11 from offset 0 with partition epoch 24, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:28,094] INFO [Broker id=5] Leader __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 26, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:28,107] INFO [Broker id=5] Leader financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 7 from offset 255435 with partition epoch 11, high watermark 255435, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:28,117] INFO [Broker id=5] Leader __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 24, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:28,124] INFO [Broker id=5] Leader __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 25, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:28,133] INFO [Broker id=5] Leader __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 24, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:28,140] INFO [Broker id=5] Leader __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 24, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:28,166] INFO [Broker id=5] Leader financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 6 from offset 256237 with partition epoch 11, high watermark 256237, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:28,198] INFO [Broker id=5] Leader __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 26, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:28,210] INFO [Broker id=5] Leader __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 26, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:28,247] INFO [Broker id=5] Leader financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 6 from offset 255167 with partition epoch 11, high watermark 255167, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:28,256] INFO [Broker id=5] Leader __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 26, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:28,272] INFO [Broker id=5] Leader __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 25, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:28,298] INFO [Broker id=5] Leader __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 25, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:28,299] INFO [Partition financial_transactions-13 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-21 14:13:28,316] INFO [Broker id=5] Leader financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 6 from offset 255541 with partition epoch 11, high watermark 255541, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:28,323] INFO [Broker id=5] Leader __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 24, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:28,340] INFO [Broker id=5] Leader __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 26, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:13:28,353] INFO [Broker id=5] Leader financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 6 from offset 255175 with partition epoch 11, high watermark 255175, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:28,372] INFO [Broker id=5] Leader __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 25, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:28,393] INFO [Broker id=5] Leader __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 25, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:28,398] INFO [Broker id=5] Leader financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 5 from offset 256308 with partition epoch 10, high watermark 256277, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 14:13:28,413] INFO [Broker id=5] Leader __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 25, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:28,419] INFO [Partition __consumer_offsets-15 broker=5] ISR updated to 5,6  and version updated to 25 (kafka.cluster.Partition)
[2025-05-21 14:13:28,536] INFO [Partition __consumer_offsets-48 broker=5] ISR updated to 5,6  and version updated to 25 (kafka.cluster.Partition)
[2025-05-21 14:13:28,548] INFO [Partition financial_transactions-15 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-21 14:13:28,555] INFO [Partition __consumer_offsets-13 broker=5] ISR updated to 5,6  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:28,556] INFO [Partition __consumer_offsets-46 broker=5] ISR updated to 5,6  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:28,556] INFO [Partition financial_transactions-17 broker=5] ISR updated to 5,6  and version updated to 11 (kafka.cluster.Partition)
[2025-05-21 14:13:28,557] INFO [Partition __consumer_offsets-11 broker=5] ISR updated to 5,6  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:28,557] INFO [Partition __consumer_offsets-9 broker=5] ISR updated to 5,6  and version updated to 25 (kafka.cluster.Partition)
[2025-05-21 14:13:28,558] INFO [Partition __consumer_offsets-42 broker=5] ISR updated to 5,6  and version updated to 25 (kafka.cluster.Partition)
[2025-05-21 14:13:28,559] INFO [Partition __consumer_offsets-21 broker=5] ISR updated to 5,6  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:28,566] INFO [Partition __consumer_offsets-17 broker=5] ISR updated to 5,6  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:28,566] INFO [Broker id=5] Leader __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 11 from offset 0 with partition epoch 24, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:28,578] INFO [Partition financial_transactions-0 broker=5] ISR updated to 5,6  and version updated to 11 (kafka.cluster.Partition)
[2025-05-21 14:13:28,579] INFO [Partition __consumer_offsets-30 broker=5] ISR updated to 5,6  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:28,580] INFO [Partition financial_transactions-4 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-21 14:13:28,581] INFO [Partition __consumer_offsets-26 broker=5] ISR updated to 5,6  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:28,581] INFO [Partition __consumer_offsets-5 broker=5] ISR updated to 5,6  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:28,582] INFO [Partition financial_transactions-8 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-21 14:13:28,582] INFO [Partition __consumer_offsets-38 broker=5] ISR updated to 5,6  and version updated to 25 (kafka.cluster.Partition)
[2025-05-21 14:13:28,584] INFO [Partition __consumer_offsets-1 broker=5] ISR updated to 5,6  and version updated to 25 (kafka.cluster.Partition)
[2025-05-21 14:13:28,585] INFO [Partition financial_transactions-12 broker=5] ISR updated to 5,6  and version updated to 11 (kafka.cluster.Partition)
[2025-05-21 14:13:28,586] INFO [Partition __consumer_offsets-34 broker=5] ISR updated to 5,6  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:28,586] INFO [Partition financial_transactions-14 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-21 14:13:28,587] INFO [Partition __consumer_offsets-16 broker=5] ISR updated to 5,6  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:28,587] INFO [Partition _schemas-0 broker=5] ISR updated to 5,6  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:28,589] INFO [Partition __consumer_offsets-45 broker=5] ISR updated to 5,6  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:28,590] INFO [Partition __consumer_offsets-12 broker=5] ISR updated to 5,6  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:28,596] INFO [Partition __consumer_offsets-41 broker=5] ISR updated to 5,6  and version updated to 25 (kafka.cluster.Partition)
[2025-05-21 14:13:28,599] INFO [Partition __consumer_offsets-24 broker=5] ISR updated to 5,6  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:28,602] INFO [Partition __consumer_offsets-20 broker=5] ISR updated to 5,6  and version updated to 25 (kafka.cluster.Partition)
[2025-05-21 14:13:28,613] INFO [Partition __consumer_offsets-49 broker=5] ISR updated to 5,6  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:28,612] INFO [Broker id=5] Leader financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 7 from offset 255412 with partition epoch 11, high watermark 255412, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:13:28,634] INFO [Partition __consumer_offsets-0 broker=5] ISR updated to 5,6  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:28,638] INFO [Partition __consumer_offsets-29 broker=5] ISR updated to 5,6  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:28,638] INFO [Partition financial_transactions-1 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-21 14:13:28,639] INFO [Partition __consumer_offsets-25 broker=5] ISR updated to 5,6  and version updated to 25 (kafka.cluster.Partition)
[2025-05-21 14:13:28,641] INFO [Partition financial_transactions-5 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-21 14:13:28,647] INFO [Broker id=5] Leader __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 11 from offset 0 with partition epoch 24, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:28,651] INFO [Partition __consumer_offsets-8 broker=5] ISR updated to 5,6  and version updated to 25 (kafka.cluster.Partition)
[2025-05-21 14:13:28,654] INFO [Partition __consumer_offsets-37 broker=5] ISR updated to 5,6  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:28,656] INFO [Partition financial_transactions-9 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-21 14:13:28,660] INFO [Broker id=5] Leader __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 25, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:28,674] INFO [Partition __consumer_offsets-4 broker=5] ISR updated to 5,6  and version updated to 25 (kafka.cluster.Partition)
[2025-05-21 14:13:28,693] INFO [Partition __consumer_offsets-33 broker=5] ISR updated to 5,6  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:28,730] INFO [Broker id=5] Leader __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 25, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:28,747] INFO [Broker id=5] Leader __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 24, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:13:28,772] INFO [Broker id=5] Leader __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 26, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:28,773] INFO [Partition financial_transactions-18 broker=5] ISR updated to 5,6  and version updated to 11 (kafka.cluster.Partition)
[2025-05-21 14:13:28,793] INFO [Broker id=5] Leader __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 11 from offset 0 with partition epoch 24, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:28,811] INFO [Broker id=5] Leader aggregated_transactions-0 with topic id Some(tZKMqbfwSlSmyms8wDFH7g) starts at leader epoch 4 from offset 352 with partition epoch 4, high watermark 352, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 3. (state.change.logger)
[2025-05-21 14:13:28,841] INFO [Broker id=5] Leader __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 25, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:28,886] INFO [Broker id=5] Leader financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 5 from offset 255715 with partition epoch 10, high watermark 255715, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 4. (state.change.logger)
[2025-05-21 14:13:28,907] INFO [Broker id=5] Leader __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 26, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:28,919] INFO [Broker id=5] Leader financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 6 from offset 255844 with partition epoch 10, high watermark 255844, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:28,941] INFO [Broker id=5] Leader __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 10 from offset 0 with partition epoch 25, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:13:28,960] INFO [Broker id=5] Leader __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 11 from offset 0 with partition epoch 24, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:13:28,978] INFO [Broker id=5] Leader financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 6 from offset 256046 with partition epoch 11, high watermark 256033, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:13:28,989] INFO [Broker id=5] Leader __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 16 from offset 0 with partition epoch 26, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:13:29,022] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 13 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,025] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,028] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 46 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,031] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,042] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 9 in epoch 11 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,044] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,045] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 42 in epoch 11 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,045] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,046] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 21 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,047] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,047] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 17 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,048] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,048] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 30 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,049] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,050] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 26 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,050] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,051] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 5 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,052] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,053] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 38 in epoch 11 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,054] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,054] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 1 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,055] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,051] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-13 in 23 milliseconds for epoch 15, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,056] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 34 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,056] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,056] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-46 in 14 milliseconds for epoch 15, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,057] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-9 in 13 milliseconds for epoch 11, of which 13 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,057] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 16 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,058] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-42 in 12 milliseconds for epoch 11, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,058] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-21 in 11 milliseconds for epoch 10, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,059] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-17 in 11 milliseconds for epoch 10, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,060] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-30 in 10 milliseconds for epoch 15, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,060] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-26 in 9 milliseconds for epoch 16, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,062] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-5 in 9 milliseconds for epoch 16, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,058] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,065] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 45 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,065] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,065] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 12 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,070] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,071] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 41 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,072] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,072] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 24 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,072] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,072] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 20 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,066] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-38 in 12 milliseconds for epoch 11, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,073] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,074] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 49 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,074] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,074] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-1 in 18 milliseconds for epoch 13, of which 18 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,075] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 0 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,076] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,076] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 29 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,077] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-34 in 20 milliseconds for epoch 15, of which 18 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,077] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,078] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 25 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,078] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,078] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-16 in 12 milliseconds for epoch 16, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,079] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-45 in 14 milliseconds for epoch 10, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,079] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 8 in epoch 11 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,079] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-12 in 8 milliseconds for epoch 10, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,087] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,088] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 37 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,089] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,090] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 4 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,095] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,096] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 33 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,096] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,096] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 15 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,097] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,097] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 48 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,097] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,098] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 11 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,098] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,099] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 44 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,099] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,099] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 23 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,100] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,100] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 19 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,100] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,101] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 32 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,101] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,102] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 28 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,102] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,102] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 7 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,103] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,103] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 40 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,104] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,104] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 3 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,105] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,105] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 36 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,105] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,106] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 47 in epoch 11 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,106] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,107] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 14 in epoch 11 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,107] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,108] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 43 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,108] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,109] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 10 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,110] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,112] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 22 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,088] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-41 in 16 milliseconds for epoch 13, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,118] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-24 in 46 milliseconds for epoch 10, of which 45 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,118] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-20 in 44 milliseconds for epoch 13, of which 44 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,119] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-49 in 44 milliseconds for epoch 16, of which 44 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,120] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-0 in 44 milliseconds for epoch 10, of which 44 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,117] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,125] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 18 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,125] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,125] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 31 in epoch 11 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,126] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,126] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 27 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,127] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,127] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 39 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,127] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,127] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 6 in epoch 10 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,128] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,128] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 35 in epoch 11 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,128] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,129] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 2 in epoch 16 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,130] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,146] INFO [Broker id=5] Transitioning 41 partition(s) to local leaders. (state.change.logger)
[2025-05-21 14:13:29,149] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-11, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-33) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:13:29,158] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 6], partitionEpoch=12, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 255889, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,161] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,162] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,164] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 256237, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,164] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,168] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,172] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=11, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 255866, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,173] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,174] INFO Loaded member MemberMetadata(memberId=sr-1-99122467-ec1a-4fc4-88f0-59fda8f60096, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 1. (kafka.coordinator.group.GroupMetadata$)
[2025-05-21 14:13:29,174] INFO Loaded member MemberMetadata(memberId=sr-1-3027da35-fa63-4fb3-9cba-2acfe293f9a1, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 3. (kafka.coordinator.group.GroupMetadata$)
[2025-05-21 14:13:29,174] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,178] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,179] INFO Loaded member MemberMetadata(memberId=sr-1-960e2a90-cf63-43a4-b2ba-89aa2499d218, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.8, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 5. (kafka.coordinator.group.GroupMetadata$)
[2025-05-21 14:13:29,181] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,184] INFO [GroupCoordinator 5]: Loading group metadata for schema-registry with generation 6 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:29,186] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,199] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=11, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 257211, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,196] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-29 in 119 milliseconds for epoch 16, of which 43 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,209] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,210] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-25 in 132 milliseconds for epoch 13, of which 126 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,213] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-8 in 125 milliseconds for epoch 11, of which 124 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,214] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-37 in 124 milliseconds for epoch 15, of which 124 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,213] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 255335, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,225] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-4 in 130 milliseconds for epoch 13, of which 130 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,228] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-33 in 132 milliseconds for epoch 10, of which 132 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,229] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-15 in 132 milliseconds for epoch 13, of which 132 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,231] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,232] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-48 in 134 milliseconds for epoch 13, of which 133 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,232] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-11 in 133 milliseconds for epoch 15, of which 133 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,232] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,237] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-44 in 138 milliseconds for epoch 15, of which 138 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,240] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-23 in 140 milliseconds for epoch 16, of which 140 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,241] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-19 in 140 milliseconds for epoch 10, of which 140 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,241] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 6], partitionEpoch=12, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 255525, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,242] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,242] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,243] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=11, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 255590, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,241] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-32 in 139 milliseconds for epoch 10, of which 139 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,244] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,244] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 255812, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,244] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-28 in 142 milliseconds for epoch 13, of which 142 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,249] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-7 in 145 milliseconds for epoch 15, of which 145 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,245] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,252] INFO [Broker id=5] Skipped the become-leader state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 6, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,255] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,256] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,260] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-40 in 156 milliseconds for epoch 10, of which 147 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,261] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-3 in 156 milliseconds for epoch 10, of which 156 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,262] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-36 in 155 milliseconds for epoch 10, of which 155 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,262] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-47 in 155 milliseconds for epoch 11, of which 155 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,262] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-14 in 154 milliseconds for epoch 11, of which 154 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,263] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-43 in 154 milliseconds for epoch 10, of which 154 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,263] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-10 in 152 milliseconds for epoch 10, of which 152 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,263] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-22 in 138 milliseconds for epoch 13, of which 138 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,264] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-18 in 139 milliseconds for epoch 16, of which 139 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,264] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-31 in 138 milliseconds for epoch 11, of which 138 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,264] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-27 in 137 milliseconds for epoch 10, of which 137 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,265] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-39 in 138 milliseconds for epoch 16, of which 138 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,265] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,270] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-6 in 142 milliseconds for epoch 10, of which 141 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,270] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,270] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-35 in 141 milliseconds for epoch 11, of which 141 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,270] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,272] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-2 in 141 milliseconds for epoch 16, of which 141 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:13:29,273] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,274] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,275] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 6, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,276] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 6], partitionEpoch=12, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 255371, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,276] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,277] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=12, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 254990, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,277] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,277] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,278] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=12, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 255435, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,279] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,279] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,283] INFO [Broker id=5] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-21 14:13:29,299] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-18) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:13:29,310] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=11, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 255590, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,504] INFO [Partition financial_transactions-13 broker=5] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-21 14:13:29,559] INFO [Broker id=5] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-21 14:13:29,559] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-13) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:13:29,560] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 255889, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,583] INFO [Partition __consumer_offsets-15 broker=5] ISR updated to 5,6,4  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,586] INFO [Partition __consumer_offsets-48 broker=5] ISR updated to 5,6,4  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,587] INFO [Partition __consumer_offsets-13 broker=5] ISR updated to 5,6,4  and version updated to 28 (kafka.cluster.Partition)
[2025-05-21 14:13:29,589] INFO [Partition __consumer_offsets-46 broker=5] ISR updated to 5,6,4  and version updated to 28 (kafka.cluster.Partition)
[2025-05-21 14:13:29,590] INFO [Partition financial_transactions-17 broker=5] ISR updated to 5,6,4  and version updated to 12 (kafka.cluster.Partition)
[2025-05-21 14:13:29,591] INFO [Partition __consumer_offsets-9 broker=5] ISR updated to 5,6,4  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,591] INFO [Partition __consumer_offsets-42 broker=5] ISR updated to 5,6,4  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,592] INFO [Partition __consumer_offsets-21 broker=5] ISR updated to 5,6,4  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:29,592] INFO [Partition __consumer_offsets-17 broker=5] ISR updated to 5,6,4  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:29,593] INFO [Partition __consumer_offsets-30 broker=5] ISR updated to 5,6,4  and version updated to 28 (kafka.cluster.Partition)
[2025-05-21 14:13:29,593] INFO [Partition financial_transactions-0 broker=5] ISR updated to 5,6,4  and version updated to 12 (kafka.cluster.Partition)
[2025-05-21 14:13:29,593] INFO [Partition financial_transactions-4 broker=5] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-21 14:13:29,594] INFO [Partition __consumer_offsets-26 broker=5] ISR updated to 5,6,4  and version updated to 28 (kafka.cluster.Partition)
[2025-05-21 14:13:29,594] INFO [Partition __consumer_offsets-5 broker=5] ISR updated to 5,6,4  and version updated to 28 (kafka.cluster.Partition)
[2025-05-21 14:13:29,595] INFO [Partition financial_transactions-8 broker=5] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-21 14:13:29,597] INFO [Partition __consumer_offsets-38 broker=5] ISR updated to 5,6,4  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,598] INFO [Partition __consumer_offsets-1 broker=5] ISR updated to 5,6,4  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,598] INFO [Partition financial_transactions-12 broker=5] ISR updated to 5,6,4  and version updated to 12 (kafka.cluster.Partition)
[2025-05-21 14:13:29,603] INFO [Partition __consumer_offsets-34 broker=5] ISR updated to 5,6,4  and version updated to 28 (kafka.cluster.Partition)
[2025-05-21 14:13:29,607] INFO [Partition financial_transactions-14 broker=5] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-21 14:13:29,608] INFO [Partition __consumer_offsets-16 broker=5] ISR updated to 5,6,4  and version updated to 28 (kafka.cluster.Partition)
[2025-05-21 14:13:29,608] INFO [Partition _schemas-0 broker=5] ISR updated to 5,6,4  and version updated to 28 (kafka.cluster.Partition)
[2025-05-21 14:13:29,609] INFO [Partition __consumer_offsets-45 broker=5] ISR updated to 5,6,4  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:29,609] INFO [Partition financial_transactions-18 broker=5] ISR updated to 5,6,4  and version updated to 12 (kafka.cluster.Partition)
[2025-05-21 14:13:29,610] INFO [Partition __consumer_offsets-12 broker=5] ISR updated to 5,6,4  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:29,610] INFO [Partition __consumer_offsets-41 broker=5] ISR updated to 5,6,4  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,611] INFO [Partition __consumer_offsets-24 broker=5] ISR updated to 5,6,4  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:29,611] INFO [Partition __consumer_offsets-20 broker=5] ISR updated to 5,6,4  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,611] INFO [Partition __consumer_offsets-49 broker=5] ISR updated to 5,6,4  and version updated to 28 (kafka.cluster.Partition)
[2025-05-21 14:13:29,612] INFO [Partition __consumer_offsets-0 broker=5] ISR updated to 5,6,4  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:29,612] INFO [Partition __consumer_offsets-29 broker=5] ISR updated to 5,6,4  and version updated to 28 (kafka.cluster.Partition)
[2025-05-21 14:13:29,613] INFO [Partition financial_transactions-1 broker=5] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-21 14:13:29,613] INFO [Partition __consumer_offsets-25 broker=5] ISR updated to 5,6,4  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,615] INFO [Partition financial_transactions-5 broker=5] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-21 14:13:29,616] INFO [Partition __consumer_offsets-8 broker=5] ISR updated to 5,6,4  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,617] INFO [Partition __consumer_offsets-37 broker=5] ISR updated to 5,6,4  and version updated to 28 (kafka.cluster.Partition)
[2025-05-21 14:13:29,617] INFO [Partition financial_transactions-9 broker=5] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-21 14:13:29,624] INFO [Partition __consumer_offsets-4 broker=5] ISR updated to 5,6,4  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,626] INFO [Partition __consumer_offsets-33 broker=5] ISR updated to 5,6,4  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:29,674] INFO [Broker id=5] Transitioning 39 partition(s) to local leaders. (state.change.logger)
[2025-05-21 14:13:29,675] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-33) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:13:29,676] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,677] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,682] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,683] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,684] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 255866, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,690] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,692] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,693] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,698] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,699] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 257211, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,700] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,701] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 255335, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,718] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,726] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,736] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 255525, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,740] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,757] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,759] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 255590, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,777] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,779] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 255812, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,780] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,781] INFO [Broker id=5] Skipped the become-leader state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 6, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,782] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,784] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 255590, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,789] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,796] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,799] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,805] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,806] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,807] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,807] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 6, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,808] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 255371, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,808] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,809] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 254990, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,809] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,810] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,810] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 255435, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,811] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,812] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,861] INFO [Partition __consumer_offsets-44 broker=5] ISR updated to 5,6  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:29,906] INFO [Broker id=5] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-21 14:13:29,906] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-44) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:13:29,907] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:29,937] INFO [Partition financial_transactions-19 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-21 14:13:29,938] INFO [Partition __consumer_offsets-23 broker=5] ISR updated to 5,6  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:29,939] INFO [Partition __consumer_offsets-19 broker=5] ISR updated to 5,6  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,940] INFO [Partition __consumer_offsets-32 broker=5] ISR updated to 5,6  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,941] INFO [Partition financial_transactions-2 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-21 14:13:29,942] INFO [Partition __consumer_offsets-28 broker=5] ISR updated to 5,6  and version updated to 25 (kafka.cluster.Partition)
[2025-05-21 14:13:29,943] INFO [Partition __consumer_offsets-7 broker=5] ISR updated to 5,6  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:29,943] INFO [Partition financial_transactions-6 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-21 14:13:29,944] INFO [Partition __consumer_offsets-40 broker=5] ISR updated to 5,6  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,944] INFO [Partition __consumer_offsets-3 broker=5] ISR updated to 5,6  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,944] INFO [Partition __consumer_offsets-36 broker=5] ISR updated to 5,6  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,945] INFO [Partition __consumer_offsets-47 broker=5] ISR updated to 5,6  and version updated to 25 (kafka.cluster.Partition)
[2025-05-21 14:13:29,946] INFO [Partition financial_transactions-16 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-21 14:13:29,949] INFO [Partition __consumer_offsets-14 broker=5] ISR updated to 5,6  and version updated to 25 (kafka.cluster.Partition)
[2025-05-21 14:13:29,951] INFO [Partition __consumer_offsets-43 broker=5] ISR updated to 5,6  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,952] INFO [Partition __consumer_offsets-10 broker=5] ISR updated to 5,6  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,953] INFO [Partition __consumer_offsets-22 broker=5] ISR updated to 5,6  and version updated to 25 (kafka.cluster.Partition)
[2025-05-21 14:13:29,953] INFO [Partition __consumer_offsets-18 broker=5] ISR updated to 5,6  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:29,954] INFO [Partition __consumer_offsets-31 broker=5] ISR updated to 5,6  and version updated to 25 (kafka.cluster.Partition)
[2025-05-21 14:13:29,954] INFO [Partition __consumer_offsets-27 broker=5] ISR updated to 5,6  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,955] INFO [Partition financial_transactions-3 broker=5] ISR updated to 5,6  and version updated to 11 (kafka.cluster.Partition)
[2025-05-21 14:13:29,955] INFO [Partition __consumer_offsets-39 broker=5] ISR updated to 5,6  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:29,955] INFO [Partition financial_transactions-7 broker=5] ISR updated to 5,6  and version updated to 11 (kafka.cluster.Partition)
[2025-05-21 14:13:29,956] INFO [Partition __consumer_offsets-6 broker=5] ISR updated to 5,6  and version updated to 26 (kafka.cluster.Partition)
[2025-05-21 14:13:29,956] INFO [Partition __consumer_offsets-35 broker=5] ISR updated to 5,6  and version updated to 25 (kafka.cluster.Partition)
[2025-05-21 14:13:29,956] INFO [Partition __consumer_offsets-2 broker=5] ISR updated to 5,6  and version updated to 27 (kafka.cluster.Partition)
[2025-05-21 14:13:29,998] INFO [Broker id=5] Transitioning 26 partition(s) to local leaders. (state.change.logger)
[2025-05-21 14:13:30,001] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:13:30,004] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 255167, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,005] INFO [Partition financial_transactions-10 broker=5] ISR updated to 5,6  and version updated to 11 (kafka.cluster.Partition)
[2025-05-21 14:13:30,006] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,007] INFO [Partition financial_transactions-11 broker=5] ISR updated to 5,6  and version updated to 12 (kafka.cluster.Partition)
[2025-05-21 14:13:30,007] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,008] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,008] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 255541, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,009] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,010] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=27, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,010] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 255175, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,011] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,011] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,012] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,012] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,013] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6], partitionEpoch=12, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 255412, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,013] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,014] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,014] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,025] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6], partitionEpoch=25, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,026] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,026] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,027] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,028] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=11, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 255715, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,028] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,029] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=11, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 255844, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,029] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6], partitionEpoch=26, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,030] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=25, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,031] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6], partitionEpoch=27, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,047] INFO [Broker id=5] Transitioning 2 partition(s) to local leaders. (state.change.logger)
[2025-05-21 14:13:30,048] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-10, financial_transactions-11) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:13:30,051] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6], partitionEpoch=11, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 256308, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,053] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6], partitionEpoch=12, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 256046, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,066] INFO [Partition financial_transactions-15 broker=5] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-21 14:13:30,067] INFO [Partition __consumer_offsets-11 broker=5] ISR updated to 5,6,4  and version updated to 28 (kafka.cluster.Partition)
[2025-05-21 14:13:30,070] INFO [Partition __consumer_offsets-44 broker=5] ISR updated to 5,6,4  and version updated to 28 (kafka.cluster.Partition)
[2025-05-21 14:13:30,049] INFO [Partition financial_transactions-19 broker=5] Failed to alter partition to PendingExpandIsr(newInSyncReplicaId=4, sentLeaderAndIsr=LeaderAndIsr(leader=5, leaderEpoch=6, isrWithBrokerEpoch=List(BrokerState(brokerId=5, brokerEpoch=12481), BrokerState(brokerId=6, brokerEpoch=12477), BrokerState(brokerId=4, brokerEpoch=12478)), leaderRecoveryState=RECOVERED, partitionEpoch=12), leaderRecoveryState=RECOVERED, lastCommittedState=CommittedPartitionState(isr=Set(5, 6), leaderRecoveryState=RECOVERED)) since the controller rejected the request with OPERATION_NOT_ATTEMPTED. Partition state has been reset to the latest committed state CommittedPartitionState(isr=Set(5, 6), leaderRecoveryState=RECOVERED). (kafka.cluster.Partition)
[2025-05-21 14:13:30,164] INFO [Broker id=5] Transitioning 28 partition(s) to local leaders. (state.change.logger)
[2025-05-21 14:13:30,193] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:13:30,180] INFO [Partition financial_transactions-10 broker=5] ISR updated to 5,6,4  and version updated to 12 (kafka.cluster.Partition)
[2025-05-21 14:13:30,194] INFO [Partition financial_transactions-16 broker=5] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-21 14:13:30,195] INFO [Partition financial_transactions-11 broker=5] ISR updated to 5,6,4  and version updated to 13 (kafka.cluster.Partition)
[2025-05-21 14:13:30,195] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 256237, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,196] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,197] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,197] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 255167, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,198] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,199] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,200] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,202] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 255541, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,204] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,210] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,213] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 255175, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,213] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,214] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,214] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,215] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,215] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,216] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,218] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,218] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=13, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 13. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,219] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,219] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,220] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,220] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 255715, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,221] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,221] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 255844, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,222] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 6, 4], partitionEpoch=27, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,231] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=26, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,232] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=16, isr=[5, 6, 4], partitionEpoch=28, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 16. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,715] INFO [Broker id=5] Transitioning 3 partition(s) to local leaders. (state.change.logger)
[2025-05-21 14:13:30,716] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-10, financial_transactions-11, financial_transactions-16) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:13:30,718] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=5, isr=[5, 6, 4], partitionEpoch=12, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 5. Current high watermark 256308, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,721] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 6, 4], partitionEpoch=13, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 256046, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:30,723] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=7, isr=[5, 6, 4], partitionEpoch=13, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 7. Current high watermark 255412, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:13:33,733] INFO [GroupCoordinator 5]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-11f3f728-c325-475e-a3a2-6b3e8884ce37 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:33,742] INFO [GroupCoordinator 5]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 6 (__consumer_offsets-29) (reason: Adding new member sr-1-11f3f728-c325-475e-a3a2-6b3e8884ce37 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:33,749] INFO [GroupCoordinator 5]: Stabilized group schema-registry generation 7 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:13:33,773] INFO [GroupCoordinator 5]: Assignment received from leader sr-1-11f3f728-c325-475e-a3a2-6b3e8884ce37 for group schema-registry for generation 7. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,601] INFO [Broker id=5] Transitioning 48 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:18:23,602] INFO [Broker id=5] Follower financial_transactions-13 starts at leader epoch 9 from offset 268526 with partition epoch 14 and high watermark 268526. Current leader is 4. Previous leader Some(4) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:18:23,602] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 14 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:18:23,602] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 14 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:18:23,603] INFO [Broker id=5] Follower __consumer_offsets-13 starts at leader epoch 16 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,603] INFO [Broker id=5] Follower __consumer_offsets-46 starts at leader epoch 16 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,604] INFO [Broker id=5] Follower financial_transactions-17 starts at leader epoch 6 from offset 268373 with partition epoch 13 and high watermark 268373. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:18:23,605] INFO [Broker id=5] Follower __consumer_offsets-11 starts at leader epoch 16 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,605] INFO [Broker id=5] Follower __consumer_offsets-44 starts at leader epoch 16 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,606] INFO [Broker id=5] Follower __consumer_offsets-9 starts at leader epoch 12 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:18:23,607] INFO [Broker id=5] Follower __consumer_offsets-42 starts at leader epoch 12 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:18:23,607] INFO [Broker id=5] Follower __consumer_offsets-23 starts at leader epoch 17 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,607] INFO [Broker id=5] Follower financial_transactions-0 starts at leader epoch 7 from offset 269651 with partition epoch 13 and high watermark 269651. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:18:23,608] INFO [Broker id=5] Follower __consumer_offsets-30 starts at leader epoch 16 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,608] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 14 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:18:23,608] INFO [Broker id=5] Follower __consumer_offsets-26 starts at leader epoch 17 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,609] INFO [Broker id=5] Follower __consumer_offsets-7 starts at leader epoch 16 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,609] INFO [Broker id=5] Follower __consumer_offsets-5 starts at leader epoch 17 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,610] INFO [Broker id=5] Follower financial_transactions-8 starts at leader epoch 9 from offset 267896 with partition epoch 14 and high watermark 267896. Current leader is 4. Previous leader Some(4) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:18:23,610] INFO [Broker id=5] Follower __consumer_offsets-38 starts at leader epoch 12 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:18:23,610] INFO [Broker id=5] Follower financial_transactions-10 starts at leader epoch 6 from offset 268988 with partition epoch 13 and high watermark 268988. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:18:23,611] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 14 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:18:23,611] INFO [Broker id=5] Follower financial_transactions-12 starts at leader epoch 7 from offset 268072 with partition epoch 13 and high watermark 268072. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:18:23,611] INFO [Broker id=5] Follower __consumer_offsets-34 starts at leader epoch 16 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,612] INFO [Broker id=5] Follower __consumer_offsets-47 starts at leader epoch 12 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:18:23,612] INFO [Broker id=5] Follower __consumer_offsets-16 starts at leader epoch 17 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,612] INFO [Broker id=5] Follower _schemas-0 starts at leader epoch 17 from offset 8 with partition epoch 29 and high watermark 8. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,612] INFO [Broker id=5] Follower financial_transactions-16 starts at leader epoch 8 from offset 267900 with partition epoch 14 and high watermark 267900. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:18:23,613] INFO [Broker id=5] Follower __consumer_offsets-14 starts at leader epoch 12 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:18:23,613] INFO [Broker id=5] Follower financial_transactions-18 starts at leader epoch 6 from offset 267868 with partition epoch 13 and high watermark 267868. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:18:23,613] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 14 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:18:23,614] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 14 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:18:23,614] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 14 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:18:23,614] INFO [Broker id=5] Follower __consumer_offsets-49 starts at leader epoch 17 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,614] INFO [Broker id=5] Follower __consumer_offsets-18 starts at leader epoch 17 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,615] INFO [Broker id=5] Follower __consumer_offsets-31 starts at leader epoch 12 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:18:23,615] INFO [Broker id=5] Follower __consumer_offsets-29 starts at leader epoch 17 from offset 7 with partition epoch 29 and high watermark 7. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,615] INFO [Broker id=5] Follower financial_transactions-1 starts at leader epoch 9 from offset 267918 with partition epoch 14 and high watermark 267918. Current leader is 4. Previous leader Some(4) and previous leader epoch was 9. (state.change.logger)
[2025-05-21 14:18:23,616] INFO [Broker id=5] Follower financial_transactions-3 starts at leader epoch 6 from offset 268318 with partition epoch 13 and high watermark 268318. Current leader is 6. Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:18:23,616] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 14 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:18:23,616] INFO [Broker id=5] Follower financial_transactions-5 starts at leader epoch 8 from offset 267422 with partition epoch 14 and high watermark 267422. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:18:23,617] INFO [Broker id=5] Follower __consumer_offsets-39 starts at leader epoch 17 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,617] INFO [Broker id=5] Follower __consumer_offsets-8 starts at leader epoch 12 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:18:23,617] INFO [Broker id=5] Follower financial_transactions-7 starts at leader epoch 7 from offset 268422 with partition epoch 13 and high watermark 268422. Current leader is 6. Previous leader Some(6) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:18:23,618] INFO [Broker id=5] Follower __consumer_offsets-37 starts at leader epoch 16 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 16. (state.change.logger)
[2025-05-21 14:18:23,618] INFO [Broker id=5] Follower financial_transactions-9 starts at leader epoch 8 from offset 268043 with partition epoch 14 and high watermark 268043. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:18:23,619] INFO [Broker id=5] Follower __consumer_offsets-35 starts at leader epoch 12 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:18:23,619] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 14 from offset 0 with partition epoch 27 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:18:23,619] INFO [Broker id=5] Follower __consumer_offsets-2 starts at leader epoch 17 from offset 0 with partition epoch 29 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:18:23,619] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, financial_transactions-0, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, financial_transactions-18, __consumer_offsets-41, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-10, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:18:23,620] INFO [Broker id=5] Stopped fetchers as part of become-follower for 48 partitions (state.change.logger)
[2025-05-21 14:18:23,635] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,638] INFO [ReplicaFetcherManager on broker 5] Added fetcher to broker 4 for partitions HashMap(financial_transactions-13 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),9,268526), __consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),16,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),16,0), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),16,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),16,0), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,0), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),16,0), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),16,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,0), financial_transactions-8 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),9,267896), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),16,0), __consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,0), _schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,8), financial_transactions-16 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,267900), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,7), financial_transactions-1 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),9,267918), financial_transactions-5 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,267422), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),16,0), financial_transactions-9 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,268043), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),17,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:18:23,640] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:18:23,640] INFO [ReplicaFetcherManager on broker 5] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),14,0), __consumer_offsets-48 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),14,0), financial_transactions-17 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,268373), __consumer_offsets-9 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), __consumer_offsets-42 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), financial_transactions-0 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,269651), __consumer_offsets-28 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),14,0), __consumer_offsets-38 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), financial_transactions-10 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,268988), __consumer_offsets-1 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),14,0), financial_transactions-12 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,268072), __consumer_offsets-47 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), __consumer_offsets-14 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), financial_transactions-18 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,267868), __consumer_offsets-41 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),14,0), __consumer_offsets-22 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),14,0), __consumer_offsets-20 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),14,0), __consumer_offsets-31 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), financial_transactions-3 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),6,268318), __consumer_offsets-25 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),14,0), __consumer_offsets-8 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), financial_transactions-7 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),7,268422), __consumer_offsets-35 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,0), __consumer_offsets-4 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),14,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:18:23,641] INFO [Broker id=5] Started fetchers as part of become-follower for 48 partitions (state.change.logger)
[2025-05-21 14:18:23,663] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,664] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,664] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,665] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,667] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,668] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,668] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,668] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,669] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,669] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,670] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,670] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,671] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,671] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,672] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,671] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,673] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,674] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,674] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,675] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,675] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,677] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,676] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,678] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,678] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,678] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,679] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,680] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,680] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,681] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,680] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,682] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,682] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,682] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,683] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,683] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,683] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,683] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,684] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,684] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,684] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,685] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,685] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,685] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,687] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,687] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,687] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,688] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,688] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,689] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,689] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,689] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,689] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,690] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,690] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,691] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,691] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,691] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,692] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,692] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,692] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,693] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,691] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,693] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,693] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,694] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,694] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,694] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,695] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,695] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,696] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,696] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,697] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,698] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,698] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,698] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,699] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,700] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,700] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,700] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,701] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,702] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,702] INFO [GroupCoordinator 5]: Unloading group metadata for schema-registry with generation 7 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,702] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,702] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,703] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,703] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,703] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,704] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,704] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[12] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,704] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,705] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,705] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,705] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,706] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:18:23,706] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,705] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,707] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,707] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,707] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,708] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[12]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,708] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:18:23,709] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:23:14,598] INFO [NodeToControllerChannelManager id=5 name=forwarding] Client requested disconnect from node 1 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:23:14,611] INFO [broker-5-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:23:21,830] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:23:30,258] INFO [NodeToControllerChannelManager id=5 name=alter-partition] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:33:14,851] INFO [NodeToControllerChannelManager id=5 name=forwarding] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:45:31,884] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-21 14:45:31,893] INFO [BrokerServer id=5] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
[2025-05-21 14:45:31,894] INFO [BrokerServer id=5] shutting down (kafka.server.BrokerServer)
[2025-05-21 14:45:31,896] INFO [BrokerLifecycleManager id=5] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:45:31,947] INFO [Broker id=5] Transitioning 35 partition(s) to local leaders. (state.change.logger)
[2025-05-21 14:45:31,952] INFO [BrokerLifecycleManager id=5] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:45:31,974] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-15, financial_transactions-17, financial_transactions-19, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-17, __consumer_offsets-32, financial_transactions-2, financial_transactions-4, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-38, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-14, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-43, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-31, __consumer_offsets-0, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-8, __consumer_offsets-6, __consumer_offsets-35, financial_transactions-11, __consumer_offsets-33) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:45:31,975] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-1 has an older epoch (14) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,977] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-1 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,977] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 305550, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:31,977] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-0 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,978] INFO [Broker id=5] Leader financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 7 from offset 305279 with partition epoch 14, high watermark 305279, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:45:31,979] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-0 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,982] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-12 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,983] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-12 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,985] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 304471, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:31,986] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-41 has an older epoch (14) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,986] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-41 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,986] INFO [Broker id=5] Leader __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 28, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:45:31,987] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-20 has an older epoch (14) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,988] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-20 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,988] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-25 has an older epoch (14) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,989] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-25 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,989] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-4 has an older epoch (14) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,990] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-4 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,992] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-15 has an older epoch (14) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,993] INFO [Broker id=5] Leader __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 28, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:45:31,994] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-15 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,995] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-48 has an older epoch (14) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:31,997] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-48 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,000] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-28 has an older epoch (14) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,001] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-28 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,002] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,005] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,010] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,011] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,015] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 305044, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,019] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 304575, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,031] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 304608, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,033] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,033] INFO [Broker id=5] Leader __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 28, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:45:32,082] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-22 has an older epoch (14) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,085] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition __consumer_offsets-22 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,087] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-7 has an older epoch (7) than the current leader. Will await the new LeaderAndIsr state before resuming fetching. (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,087] WARN [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Partition financial_transactions-7 marked as failed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,118] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,134] INFO [Broker id=5] Leader financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 7 from offset 305673 with partition epoch 14, high watermark 305673, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:45:32,151] INFO [BrokerLifecycleManager id=5] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:45:32,170] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,203] INFO [Broker id=5] Leader __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 28, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:45:32,202] INFO [BrokerLifecycleManager id=5] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:32,206] INFO [BrokerLifecycleManager id=5] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
[2025-05-21 14:45:32,206] INFO [broker-5-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,207] INFO [broker-5-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,208] INFO [broker-5-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,209] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 305191, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,210] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,210] INFO [SocketServer listenerType=BROKER, nodeId=5] Stopping socket server request processors (kafka.network.SocketServer)
[2025-05-21 14:45:32,210] INFO [Broker id=5] Leader __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 28, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:45:32,224] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,226] INFO [Broker id=5] Leader financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 7 from offset 304723 with partition epoch 14, high watermark 304705, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:45:32,238] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,240] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,244] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 14:45:32,247] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,249] INFO [Broker id=5] Leader __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 28, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:45:32,287] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,288] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,288] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Node 4 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:45:32,288] INFO [SocketServer listenerType=BROKER, nodeId=5] Stopped socket server request processors (kafka.network.SocketServer)
[2025-05-21 14:45:32,289] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Cancelled in-flight FETCH request with correlation id 19119 due to node 4 being disconnected (elapsed time since creation: 3ms, elapsed time since send: 3ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:45:32,288] INFO [Broker id=5] Leader financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 7 from offset 305161 with partition epoch 14, high watermark 305161, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 6. (state.change.logger)
[2025-05-21 14:45:32,290] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Client requested connection close from node 4 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:45:32,300] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Error sending fetch request (sessionId=90756003, epoch=19119) to node 4: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to 4 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 14:45:32,308] INFO [Broker id=5] Leader __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 28, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:45:32,320] WARN [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=5, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={financial_transactions-13=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=305495, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[9], lastFetchedEpoch=Optional[9]), financial_transactions-16=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=304634, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[8], lastFetchedEpoch=Optional[8]), financial_transactions-1=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=304797, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[9], lastFetchedEpoch=Optional[9]), financial_transactions-5=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=304293, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[8], lastFetchedEpoch=Optional[8]), financial_transactions-8=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=304771, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[9], lastFetchedEpoch=Optional[9]), financial_transactions-9=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=304868, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[8], lastFetchedEpoch=Optional[8])}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=90756003, epoch=19119), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 4 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:114)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 14:45:32,333] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,333] INFO [Broker id=5] Leader __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 13 from offset 0 with partition epoch 28, high watermark 0, ISR [5,4], adding replicas [] and removing replicas [] . Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-21 14:45:32,342] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=6, isr=[5, 4], partitionEpoch=14, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 6. Current high watermark 305362, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,343] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=10, isr=[5, 4], partitionEpoch=28, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 10. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-21 14:45:32,343] INFO [Broker id=5] Transitioning 36 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:45:32,345] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=4, leaderEpoch=9, isr=[5, 4], partitionEpoch=15, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 14:45:32,346] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 15 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:45:32,347] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 15 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:45:32,348] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:45:32,349] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:45:32,349] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:45:32,350] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:45:32,350] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:32,351] INFO [Broker id=5] Follower financial_transactions-0 starts at leader epoch 8 from offset 306847 with partition epoch 14 and high watermark 306847. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:45:32,352] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:45:32,352] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 15 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:45:32,354] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:32,354] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:45:32,356] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:32,357] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=4, leaderEpoch=9, isr=[5, 4], partitionEpoch=15, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 14:45:32,362] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 15 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:45:32,367] INFO [Broker id=5] Follower financial_transactions-12 starts at leader epoch 8 from offset 305079 with partition epoch 14 and high watermark 305062. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:45:32,368] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:45:32,369] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:32,374] INFO [Broker id=5] Skipped the become-follower state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:32,377] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=4, leaderEpoch=8, isr=[5, 4], partitionEpoch=15, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 14:45:32,378] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 15 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:45:32,383] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 15 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:45:32,384] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 15 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:45:32,384] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:32,385] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:32,385] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:32,385] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=4, leaderEpoch=9, isr=[5, 4], partitionEpoch=15, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 9. (state.change.logger)
[2025-05-21 14:45:32,390] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 15 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:45:32,392] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=4, leaderEpoch=8, isr=[5, 4], partitionEpoch=15, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 14:45:32,393] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:32,394] INFO [Broker id=5] Follower financial_transactions-7 starts at leader epoch 8 from offset 305027 with partition epoch 14 and high watermark 305027. Current leader is 4. Previous leader Some(4) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:45:32,395] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=4, leaderEpoch=16, isr=[5, 4], partitionEpoch=30, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 16. (state.change.logger)
[2025-05-21 14:45:32,396] INFO [Broker id=5] Skipped the become-follower state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=4, leaderEpoch=8, isr=[5, 4], partitionEpoch=15, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 8. (state.change.logger)
[2025-05-21 14:45:32,397] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 15 from offset 0 with partition epoch 28 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 15. (state.change.logger)
[2025-05-21 14:45:32,397] INFO [Broker id=5] Skipped the become-follower state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=4, leaderEpoch=17, isr=[5, 4], partitionEpoch=30, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 17. (state.change.logger)
[2025-05-21 14:45:32,402] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-41, __consumer_offsets-22, __consumer_offsets-20, financial_transactions-0, __consumer_offsets-28, __consumer_offsets-25, financial_transactions-7, __consumer_offsets-4, __consumer_offsets-1, financial_transactions-12) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:45:32,403] INFO [Broker id=5] Stopped fetchers as part of become-follower for 12 partitions (state.change.logger)
[2025-05-21 14:45:32,408] INFO [ReplicaFetcherManager on broker 5] Added fetcher to broker 4 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),15,0), __consumer_offsets-48 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),15,0), __consumer_offsets-41 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),15,0), __consumer_offsets-22 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),15,0), __consumer_offsets-20 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),15,0), financial_transactions-0 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,306847), __consumer_offsets-28 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),15,0), __consumer_offsets-25 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),15,0), financial_transactions-7 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,305027), __consumer_offsets-4 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),15,0), __consumer_offsets-1 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),15,0), financial_transactions-12 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),8,305079)) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:45:32,409] INFO [Broker id=5] Started fetchers as part of become-follower for 12 partitions (state.change.logger)
[2025-05-21 14:45:32,410] INFO [ReplicaFetcherThread-0-6]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,411] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Node 4 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:45:32,412] WARN [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Connection to node 4 (kafka-broker-1/172.19.0.12:19092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:45:32,412] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Client requested connection close from node 4 (org.apache.kafka.clients.NetworkClient)
[2025-05-21 14:45:32,412] INFO [ReplicaFetcherThread-0-6]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,412] INFO [ReplicaFetcherThread-0-6]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,413] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Error sending fetch request (sessionId=90756003, epoch=INITIAL) to node 4: (org.apache.kafka.clients.FetchSessionHandler)
java.io.IOException: Connection to kafka-broker-1:19092 (id: 4 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 14:45:32,420] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 31 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,421] WARN [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=5, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={__consumer_offsets-15=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[15], lastFetchedEpoch=Optional[13]), __consumer_offsets-48=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[15], lastFetchedEpoch=Optional[13]), __consumer_offsets-41=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[15], lastFetchedEpoch=Optional[13]), __consumer_offsets-22=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[15], lastFetchedEpoch=Optional[13]), __consumer_offsets-20=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[15], lastFetchedEpoch=Optional[13]), financial_transactions-0=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=306847, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[8], lastFetchedEpoch=Optional[7]), __consumer_offsets-28=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[15], lastFetchedEpoch=Optional[13]), __consumer_offsets-25=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[15], lastFetchedEpoch=Optional[13]), financial_transactions-7=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=305027, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[8], lastFetchedEpoch=Optional[7]), __consumer_offsets-4=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[15], lastFetchedEpoch=Optional[13]), __consumer_offsets-1=PartitionData(topicId=94Q8ilNOTgqGgcE4hkgLtw, fetchOffset=0, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[15], lastFetchedEpoch=Optional[13]), financial_transactions-12=PartitionData(topicId=0e8v3fGFR_uwy9DAR-lNZA, fetchOffset=305079, logStartOffset=0, maxBytes=1048576, currentLeaderEpoch=Optional[8], lastFetchedEpoch=Optional[7])}, isolationLevel=read_uncommitted, removed=, replaced=, metadata=(sessionId=90756003, epoch=INITIAL), rackId=) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to kafka-broker-1:19092 (id: 4 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:109)
	at kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:317)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:130)
	at scala.Option.foreach(Option.scala:437)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)
	at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:135)
[2025-05-21 14:45:32,423] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,425] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 47 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,426] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,428] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 14 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,428] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-31 in 3 milliseconds for epoch 13, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,430] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,431] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-47 in 3 milliseconds for epoch 13, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,432] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 9 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,435] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-14 in 3 milliseconds for epoch 13, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,435] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,440] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 42 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,441] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-9 in 1 milliseconds for epoch 13, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,444] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,449] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 8 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,451] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,451] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 38 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,452] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,453] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 35 in epoch 13 (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,454] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-42 in 5 milliseconds for epoch 13, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,456] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,461] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-8 in 10 milliseconds for epoch 13, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,464] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,466] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-38 in 14 milliseconds for epoch 13, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,468] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,469] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-35 in 7 milliseconds for epoch 13, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,469] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,470] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,483] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,485] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,485] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,488] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,489] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,489] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,490] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,490] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,491] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,491] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,493] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,494] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,495] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,489] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,502] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,502] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,503] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,506] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,506] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,507] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,507] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,508] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,508] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,511] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,524] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,524] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,526] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,527] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,528] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,529] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,528] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,531] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,532] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,533] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,534] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,534] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,534] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,535] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,535] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,536] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,535] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,537] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,537] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,538] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,538] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,539] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,539] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,540] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,539] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,541] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,543] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,547] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,548] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,549] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,541] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,549] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,550] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,552] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,566] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,567] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,572] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,569] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,573] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,577] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,579] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[15] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,582] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,582] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,583] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,574] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,587] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,589] INFO [Broker id=5] Transitioning 72 partition(s) to local followers. (state.change.logger)
[2025-05-21 14:45:32,589] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,590] INFO [Broker id=5] Follower financial_transactions-13 starts at leader epoch 11 from offset 305495 with partition epoch 17 and high watermark 305480. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,590] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,591] INFO [Broker id=5] Follower __consumer_offsets-13 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,595] INFO [Broker id=5] Follower __consumer_offsets-46 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,595] INFO [Broker id=5] Follower financial_transactions-17 starts at leader epoch 8 from offset 305294 with partition epoch 16 and high watermark 305279. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:45:32,593] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[15]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,607] INFO [Broker id=5] Follower __consumer_offsets-9 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,608] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,609] INFO [Broker id=5] Follower __consumer_offsets-42 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,611] INFO [Broker id=5] Follower __consumer_offsets-21 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,612] INFO [Broker id=5] Follower __consumer_offsets-17 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,614] INFO [Broker id=5] Follower financial_transactions-0 starts at leader epoch 10 from offset 306847 with partition epoch 16 and high watermark 306847. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:45:32,615] INFO [Broker id=5] Follower __consumer_offsets-30 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,616] INFO [Broker id=5] Follower financial_transactions-4 starts at leader epoch 7 from offset 304585 with partition epoch 16 and high watermark 304585. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,620] INFO [Broker id=5] Follower __consumer_offsets-26 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,622] INFO [Broker id=5] Follower __consumer_offsets-5 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,624] INFO [Broker id=5] Follower financial_transactions-8 starts at leader epoch 11 from offset 304771 with partition epoch 17 and high watermark 304757. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,625] INFO [Broker id=5] Follower __consumer_offsets-38 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,626] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,626] INFO [Broker id=5] Follower financial_transactions-12 starts at leader epoch 10 from offset 305079 with partition epoch 16 and high watermark 305062. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:45:32,628] INFO [Broker id=5] Follower __consumer_offsets-34 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,631] INFO [Broker id=5] Follower financial_transactions-14 starts at leader epoch 7 from offset 305196 with partition epoch 16 and high watermark 305196. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,632] INFO [Broker id=5] Follower __consumer_offsets-16 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,633] INFO [Broker id=5] Follower _schemas-0 starts at leader epoch 19 from offset 8 with partition epoch 32 and high watermark 8. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,639] INFO [Broker id=5] Follower __consumer_offsets-45 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,642] INFO [Broker id=5] Follower financial_transactions-18 starts at leader epoch 8 from offset 304723 with partition epoch 16 and high watermark 304705. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:45:32,644] INFO [Broker id=5] Follower __consumer_offsets-12 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,645] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,645] INFO [Broker id=5] Follower __consumer_offsets-24 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,646] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,649] INFO [Broker id=5] Follower __consumer_offsets-49 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,652] INFO [Broker id=5] Follower __consumer_offsets-0 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,656] INFO [Broker id=5] Follower __consumer_offsets-29 starts at leader epoch 19 from offset 8 with partition epoch 32 and high watermark 8. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,658] INFO [Broker id=5] Follower financial_transactions-1 starts at leader epoch 11 from offset 304797 with partition epoch 17 and high watermark 304782. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,661] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,662] INFO [Broker id=5] Follower financial_transactions-5 starts at leader epoch 10 from offset 304293 with partition epoch 17 and high watermark 304275. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:45:32,663] INFO [Broker id=5] Follower __consumer_offsets-8 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,664] INFO [Broker id=5] Follower __consumer_offsets-37 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,664] INFO [Broker id=5] Follower financial_transactions-9 starts at leader epoch 10 from offset 304868 with partition epoch 17 and high watermark 304857. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:45:32,668] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,669] INFO [Broker id=5] Follower __consumer_offsets-33 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,670] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,671] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,673] INFO [Broker id=5] Follower financial_transactions-15 starts at leader epoch 7 from offset 305572 with partition epoch 16 and high watermark 305572. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,675] INFO [Broker id=5] Follower __consumer_offsets-11 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,676] INFO [Broker id=5] Follower __consumer_offsets-44 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,678] INFO [Broker id=5] Follower financial_transactions-19 starts at leader epoch 7 from offset 304475 with partition epoch 16 and high watermark 304475. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,680] INFO [Broker id=5] Follower __consumer_offsets-23 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,681] INFO [Broker id=5] Follower __consumer_offsets-19 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,682] INFO [Broker id=5] Follower __consumer_offsets-32 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,683] INFO [Broker id=5] Follower financial_transactions-2 starts at leader epoch 7 from offset 305058 with partition epoch 16 and high watermark 305058. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,683] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,685] INFO [Broker id=5] Follower __consumer_offsets-7 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-21 14:45:32,686] INFO [Broker id=5] Follower financial_transactions-6 starts at leader epoch 7 from offset 304616 with partition epoch 16 and high watermark 304616. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,687] INFO [Broker id=5] Follower __consumer_offsets-40 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,688] INFO [Broker id=5] Follower __consumer_offsets-3 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,689] INFO [Broker id=5] Follower financial_transactions-10 starts at leader epoch 8 from offset 305695 with partition epoch 16 and high watermark 305673. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:45:32,689] INFO [Broker id=5] Follower __consumer_offsets-36 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,690] INFO [Broker id=5] Follower __consumer_offsets-47 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,690] INFO [Broker id=5] Follower financial_transactions-16 starts at leader epoch 10 from offset 304634 with partition epoch 17 and high watermark 304604. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:45:32,691] INFO [Broker id=5] Follower __consumer_offsets-14 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,691] INFO [Broker id=5] Follower __consumer_offsets-43 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,692] INFO [Broker id=5] Follower __consumer_offsets-10 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,693] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-21 14:45:32,694] INFO [Broker id=5] Follower __consumer_offsets-18 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,694] INFO [Broker id=5] Follower __consumer_offsets-31 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,695] INFO [Broker id=5] Follower aggregated_transactions-0 starts at leader epoch 5 from offset 616 with partition epoch 5 and high watermark 616. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-21 14:45:32,698] INFO [Broker id=5] Follower __consumer_offsets-27 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,700] INFO [Broker id=5] Follower financial_transactions-3 starts at leader epoch 8 from offset 305161 with partition epoch 16 and high watermark 305161. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-21 14:45:32,705] INFO [Broker id=5] Follower __consumer_offsets-39 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,706] INFO [Broker id=5] Follower financial_transactions-7 starts at leader epoch 10 from offset 305027 with partition epoch 16 and high watermark 305027. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-21 14:45:32,708] INFO [Broker id=5] Follower __consumer_offsets-6 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-21 14:45:32,709] INFO [Broker id=5] Follower __consumer_offsets-35 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-21 14:45:32,710] INFO [Broker id=5] Follower financial_transactions-11 starts at leader epoch 7 from offset 305371 with partition epoch 16 and high watermark 305371. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-21 14:45:32,711] INFO [Broker id=5] Follower __consumer_offsets-2 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-21 14:45:32,715] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, financial_transactions-18, financial_transactions-12, financial_transactions-11, __consumer_offsets-8, __consumer_offsets-21, financial_transactions-15, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, financial_transactions-4, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, financial_transactions-0, financial_transactions-6, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, financial_transactions-16, __consumer_offsets-15, financial_transactions-10, __consumer_offsets-24, financial_transactions-19, financial_transactions-7, __consumer_offsets-17, financial_transactions-17, financial_transactions-1, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, financial_transactions-14, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, financial_transactions-8, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, financial_transactions-2, aggregated_transactions-0, financial_transactions-13, __consumer_offsets-32, financial_transactions-5, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:45:32,718] INFO [ReplicaAlterLogDirsManager on broker 5] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, financial_transactions-18, financial_transactions-12, financial_transactions-11, __consumer_offsets-8, __consumer_offsets-21, financial_transactions-15, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, financial_transactions-4, financial_transactions-3, __consumer_offsets-25, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, _schemas-0, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, financial_transactions-0, financial_transactions-6, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, financial_transactions-16, __consumer_offsets-15, financial_transactions-10, __consumer_offsets-24, financial_transactions-19, financial_transactions-7, __consumer_offsets-17, financial_transactions-17, financial_transactions-1, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, financial_transactions-14, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, financial_transactions-8, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, financial_transactions-2, aggregated_transactions-0, financial_transactions-13, __consumer_offsets-32, financial_transactions-5, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-21 14:45:32,722] INFO [Broker id=5] Stopped fetchers as part of controlled shutdown for 72 partitions (state.change.logger)
[2025-05-21 14:45:32,724] INFO [ReplicaFetcherThread-0-4]: Shutting down (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,725] INFO [ReplicaFetcherThread-0-4]: Stopped (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,725] INFO [ReplicaFetcherThread-0-4]: Shutdown completed (kafka.server.ReplicaFetcherThread)
[2025-05-21 14:45:32,730] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,731] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,732] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,733] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,732] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,736] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,734] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,738] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,740] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,740] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,741] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,743] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,743] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,744] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,745] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,746] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,748] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,749] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,746] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,750] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,751] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,753] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,757] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,762] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,751] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,762] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,767] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,767] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,769] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,773] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,781] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,783] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,786] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,788] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,789] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,790] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,791] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,791] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,792] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,792] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,794] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,795] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,795] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,796] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,798] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,798] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,800] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,801] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,801] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,802] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,803] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,803] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,804] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,806] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,806] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,807] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,809] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,809] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,809] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,811] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,811] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,811] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,812] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,812] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,815] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,816] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,816] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,817] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,818] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,818] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,819] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,825] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,825] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,825] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,828] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,829] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,830] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,834] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,837] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,837] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,838] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,838] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,839] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,839] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,839] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,839] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,839] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,840] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,840] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,840] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,840] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,844] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,844] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,834] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,845] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,845] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,845] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,857] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,847] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,862] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,862] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,862] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,866] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,867] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,867] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,867] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,868] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,869] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,868] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,869] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,871] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,873] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,873] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,873] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,874] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,875] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,875] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,877] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,878] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,878] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,879] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,880] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,882] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,882] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,882] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,883] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,884] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,884] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,885] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,885] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,885] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,887] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,889] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,890] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,890] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,890] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,891] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,892] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,892] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,893] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,892] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,898] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,898] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,899] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,899] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,900] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,901] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,902] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,901] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,905] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-21 14:45:32,907] INFO [data-plane Kafka Request Handler on Broker 5], shutting down (kafka.server.KafkaRequestHandlerPool)
[2025-05-21 14:45:32,911] INFO [data-plane Kafka Request Handler on Broker 5], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2025-05-21 14:45:32,912] INFO [ExpirationReaper-5-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,917] INFO [ExpirationReaper-5-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,917] INFO [ExpirationReaper-5-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,925] INFO [KafkaApi-5] Shutdown complete. (kafka.server.KafkaApis)
[2025-05-21 14:45:32,934] INFO [TransactionCoordinator id=5] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 14:45:32,938] INFO [Transaction State Manager 5]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2025-05-21 14:45:32,939] INFO [TxnMarkerSenderThread-5]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 14:45:32,940] INFO [TxnMarkerSenderThread-5]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 14:45:32,940] INFO [TxnMarkerSenderThread-5]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-21 14:45:32,944] INFO [TransactionCoordinator id=5] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-21 14:45:32,947] INFO [GroupCoordinator 5]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,949] INFO [ExpirationReaper-5-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,951] INFO [ExpirationReaper-5-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,951] INFO [ExpirationReaper-5-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,952] INFO [ExpirationReaper-5-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,953] INFO [ExpirationReaper-5-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,953] INFO [ExpirationReaper-5-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,955] INFO [GroupCoordinator 5]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-21 14:45:32,963] INFO [AssignmentsManager id=5]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:32,965] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,966] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,966] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:32,972] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 14:45:32,974] INFO [AssignmentsManager id=5]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:32,975] INFO [ReplicaManager broker=5] Shutting down (kafka.server.ReplicaManager)
[2025-05-21 14:45:32,979] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 14:45:32,980] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 14:45:32,980] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-21 14:45:32,982] INFO [ReplicaFetcherManager on broker 5] shutting down (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:45:32,984] INFO [ReplicaFetcherManager on broker 5] shutdown completed (kafka.server.ReplicaFetcherManager)
[2025-05-21 14:45:32,986] INFO [ReplicaAlterLogDirsManager on broker 5] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-21 14:45:32,986] INFO [ReplicaAlterLogDirsManager on broker 5] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2025-05-21 14:45:32,987] INFO [ExpirationReaper-5-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,988] INFO [ExpirationReaper-5-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,988] INFO [ExpirationReaper-5-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:32,995] INFO [ExpirationReaper-5-RemoteFetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,001] INFO [ExpirationReaper-5-RemoteFetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,001] INFO [ExpirationReaper-5-RemoteFetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,003] INFO [ExpirationReaper-5-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,004] INFO [ExpirationReaper-5-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,004] INFO [ExpirationReaper-5-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,005] INFO [ExpirationReaper-5-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,006] INFO [ExpirationReaper-5-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,006] INFO [ExpirationReaper-5-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,008] INFO [ExpirationReaper-5-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,010] INFO [ExpirationReaper-5-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,010] INFO [ExpirationReaper-5-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-21 14:45:33,025] INFO [AddPartitionsToTxnSenderThread-5]: Shutting down (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 14:45:33,025] INFO [AddPartitionsToTxnSenderThread-5]: Stopped (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 14:45:33,025] INFO [AddPartitionsToTxnSenderThread-5]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
[2025-05-21 14:45:33,028] INFO [ReplicaManager broker=5] Shut down completely (kafka.server.ReplicaManager)
[2025-05-21 14:45:33,033] INFO [broker-5-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:33,035] INFO [broker-5-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:33,035] INFO [broker-5-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:33,038] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 14:45:33,039] INFO [broker-5-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:33,041] INFO [broker-5-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:33,042] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
[2025-05-21 14:45:33,041] INFO [broker-5-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
[2025-05-21 14:45:33,043] INFO Shutting down. (kafka.log.LogManager)
[2025-05-21 14:45:33,045] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
[2025-05-21 14:45:33,050] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 14:45:33,051] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 14:45:33,051] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
[2025-05-21 14:45:33,084] INFO [ProducerStateManager partition=financial_transactions-15] Wrote producer snapshot at offset 305572 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,133] INFO [ProducerStateManager partition=financial_transactions-0] Wrote producer snapshot at offset 306847 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,182] INFO [ProducerStateManager partition=financial_transactions-3] Wrote producer snapshot at offset 305161 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,200] INFO [ProducerStateManager partition=financial_transactions-10] Wrote producer snapshot at offset 305695 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,207] INFO [ProducerStateManager partition=financial_transactions-12] Wrote producer snapshot at offset 305079 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,219] INFO [ProducerStateManager partition=financial_transactions-13] Wrote producer snapshot at offset 305495 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,328] INFO [ProducerStateManager partition=financial_transactions-19] Wrote producer snapshot at offset 304475 with 0 producer ids in 13 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,344] INFO [ProducerStateManager partition=financial_transactions-5] Wrote producer snapshot at offset 304293 with 0 producer ids in 6 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,350] INFO [ProducerStateManager partition=financial_transactions-6] Wrote producer snapshot at offset 304616 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,373] INFO [ProducerStateManager partition=financial_transactions-17] Wrote producer snapshot at offset 305294 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,379] INFO [ProducerStateManager partition=financial_transactions-7] Wrote producer snapshot at offset 305027 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,401] INFO [ProducerStateManager partition=__consumer_offsets-29] Wrote producer snapshot at offset 8 with 0 producer ids in 4 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,437] INFO [ProducerStateManager partition=financial_transactions-4] Wrote producer snapshot at offset 304585 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,464] INFO [ProducerStateManager partition=financial_transactions-18] Wrote producer snapshot at offset 304723 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,474] INFO [ProducerStateManager partition=financial_transactions-9] Wrote producer snapshot at offset 304868 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,481] INFO [ProducerStateManager partition=financial_transactions-1] Wrote producer snapshot at offset 304797 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,487] INFO [ProducerStateManager partition=financial_transactions-14] Wrote producer snapshot at offset 305196 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,499] INFO [ProducerStateManager partition=financial_transactions-11] Wrote producer snapshot at offset 305371 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,511] INFO [ProducerStateManager partition=aggregated_transactions-0] Wrote producer snapshot at offset 616 with 11 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,521] INFO [ProducerStateManager partition=_schemas-0] Wrote producer snapshot at offset 8 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,530] INFO [ProducerStateManager partition=financial_transactions-8] Wrote producer snapshot at offset 304771 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,540] INFO [ProducerStateManager partition=financial_transactions-2] Wrote producer snapshot at offset 305058 with 0 producer ids in 3 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,547] INFO [ProducerStateManager partition=financial_transactions-16] Wrote producer snapshot at offset 304634 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,599] INFO Shutdown complete. (kafka.log.LogManager)
[2025-05-21 14:45:33,600] INFO [broker-5-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,602] INFO [broker-5-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,602] INFO [broker-5-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,603] INFO [broker-5-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,603] INFO [broker-5-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,603] INFO [broker-5-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,604] INFO [broker-5-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,604] INFO [broker-5-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,604] INFO [broker-5-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,605] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,605] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,605] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-21 14:45:33,607] INFO [SocketServer listenerType=BROKER, nodeId=5] Shutting down socket server (kafka.network.SocketServer)
[2025-05-21 14:45:33,626] INFO [SocketServer listenerType=BROKER, nodeId=5] Shutdown completed (kafka.network.SocketServer)
[2025-05-21 14:45:33,628] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
[2025-05-21 14:45:33,629] INFO [BrokerLifecycleManager id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,630] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 14:45:33,631] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 14:45:33,631] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-21 14:45:33,633] INFO [SharedServer id=5] Stopping SharedServer (kafka.server.SharedServer)
[2025-05-21 14:45:33,634] INFO [MetadataLoader id=5] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,634] INFO [SnapshotGenerator id=5] close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,635] INFO [SnapshotGenerator id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,636] INFO [MetadataLoader id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,637] INFO [SnapshotGenerator id=5] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
[2025-05-21 14:45:33,637] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 14:45:33,792] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 14:45:33,792] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-21 14:45:33,794] INFO [kafka-5-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 14:45:33,794] INFO [RaftManager id=5] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 14:45:33,795] INFO [RaftManager id=5] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-21 14:45:33,795] INFO [RaftManager id=5] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 14:45:33,795] INFO [kafka-5-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 14:45:33,795] INFO [kafka-5-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-21 14:45:33,801] INFO [kafka-5-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 14:45:33,802] INFO [kafka-5-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 14:45:33,802] INFO [kafka-5-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-21 14:45:33,805] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 16829 with 0 producer ids in 2 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-21 14:45:33,809] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-21 14:45:33,809] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2025-05-21 14:45:33,810] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2025-05-21 14:45:33,810] INFO App info kafka.server for 5 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-21 14:45:33,811] INFO [BrokerServer id=5] shut down completed (kafka.server.BrokerServer)
[2025-05-21 14:45:33,811] INFO [BrokerServer id=5] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
[2025-05-21 14:45:33,812] INFO App info kafka.server for 5 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-22 17:40:21,834] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-22 17:40:22,029] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-2:19092,PLAINTEXT_HOST://localhost:39092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 5
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 5
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-22 17:40:22,049] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-22 17:40:22,051] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-22 17:40:24,077] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-05-22 17:40:24,168] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-05-22 17:40:24,173] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-22 17:40:24,290] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-22 17:40:24,300] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-22 17:40:24,326] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-05-22 17:40:24,330] INFO [BrokerServer id=5] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2025-05-22 17:40:24,333] INFO [SharedServer id=5] Starting SharedServer (kafka.server.SharedServer)
[2025-05-22 17:40:24,337] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-22 17:40:24,384] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0. 0/1 recovered for __cluster_metadata-0. (kafka.log.LogLoader)
[2025-05-22 17:40:24,387] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:24,388] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:24,390] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000012473.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:24,391] INFO Deleted producer state snapshot /tmp/kafka-logs/__cluster_metadata-0/00000000000000016829.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:24,393] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 4ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:24,621] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 16829 with 0 producer ids in 5 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:24,646] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 16829 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:24,648] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 16829 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:24,653] INFO [ProducerStateManager partition=__cluster_metadata-0] Loading producer state from snapshot file 'SnapshotFile(offset=16829, file=/tmp/kafka-logs/__cluster_metadata-0/00000000000000016829.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:24,668] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 16ms for snapshot load and 0ms for segment recovery from offset 16829 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:24,692] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-05-22 17:40:24,711] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-05-22 17:40:24,714] INFO [RaftManager id=5] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-22 17:40:24,862] INFO [RaftManager id=5] Starting request manager with static voters: [kafka-controller-1:9093 (id: 1 rack: null), kafka-controller-2:9093 (id: 2 rack: null), kafka-controller-3:9093 (id: 3 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-22 17:40:24,956] INFO [RaftManager id=5] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=18, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from null (org.apache.kafka.raft.QuorumState)
[2025-05-22 17:40:24,962] INFO [kafka-5-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-05-22 17:40:25,013] INFO [kafka-5-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
[2025-05-22 17:40:25,026] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:25,027] INFO [BrokerServer id=5] Starting broker (kafka.server.BrokerServer)
[2025-05-22 17:40:25,033] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-22 17:40:25,128] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:25,134] INFO [broker-5-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-22 17:40:25,137] INFO [broker-5-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-22 17:40:25,141] INFO [broker-5-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-22 17:40:25,138] INFO [broker-5-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-05-22 17:40:25,171] INFO [BrokerServer id=5] Waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-22 17:40:25,192] INFO [BrokerServer id=5] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
[2025-05-22 17:40:25,229] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:25,258] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,260] WARN [RaftManager id=5] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,258] INFO [RaftManager id=5] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1616152967 (org.apache.kafka.raft.KafkaRaftClient)
[2025-05-22 17:40:25,311] INFO [broker-5-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:25,314] INFO [broker-5-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:25,345] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:25,351] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
[2025-05-22 17:40:25,374] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,381] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,383] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,397] WARN [RaftManager id=5] Connection to node 2 (kafka-controller-2/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,412] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,412] WARN [RaftManager id=5] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,462] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:25,509] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,512] WARN [RaftManager id=5] Connection to node 2 (kafka-controller-2/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,518] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,518] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,536] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,552] WARN [RaftManager id=5] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,565] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:25,674] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:25,748] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,752] WARN [RaftManager id=5] Connection to node 1 (kafka-controller-1/172.19.0.7:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,762] INFO [RaftManager id=5] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,773] WARN [RaftManager id=5] Connection to node 2 (kafka-controller-2/172.19.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,775] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:25,781] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-22 17:40:25,783] INFO [RaftManager id=5] Node 3 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,786] WARN [RaftManager id=5] Connection to node 3 (kafka-controller-3/172.19.0.6:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:25,797] INFO [SocketServer listenerType=BROKER, nodeId=5] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2025-05-22 17:40:25,800] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
[2025-05-22 17:40:25,811] INFO [SocketServer listenerType=BROKER, nodeId=5] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[2025-05-22 17:40:25,867] INFO [broker-5-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:25,870] INFO [broker-5-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:25,885] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:25,888] INFO [broker-5-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:25,892] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:25,898] INFO [ExpirationReaper-5-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-22 17:40:25,903] INFO [ExpirationReaper-5-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-22 17:40:25,917] INFO [ExpirationReaper-5-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-22 17:40:25,922] INFO [ExpirationReaper-5-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-22 17:40:25,934] INFO [ExpirationReaper-5-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-22 17:40:25,941] INFO [ExpirationReaper-5-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-22 17:40:25,942] INFO [ExpirationReaper-5-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-22 17:40:26,002] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,097] INFO [broker-5-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:26,103] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,101] INFO [BrokerLifecycleManager id=5] Incarnation nzsOPFRqTUyI37ZcRolLyw of broker 5 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
[2025-05-22 17:40:26,109] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:26,116] INFO [ExpirationReaper-5-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-05-22 17:40:26,151] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:26,156] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:26,172] INFO [BrokerServer id=5] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-22 17:40:26,174] INFO [BrokerServer id=5] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
[2025-05-22 17:40:26,174] INFO [BrokerServer id=5] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-22 17:40:26,174] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,206] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:26,223] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:26,224] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-3:9093 (id: 3 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:26,273] INFO [RaftManager id=5] Completed transition to FollowerState(fetchTimeoutMs=2000, epoch=20, leader=kafka-controller-2:9093 (id: 2 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) from FollowerState(fetchTimeoutMs=2000, epoch=18, leader=kafka-controller-3:9093 (id: 3 rack: null) voters=[1, 2, 3], highWatermark=Optional.empty, fetchingSnapshot=Optional.empty) (org.apache.kafka.raft.QuorumState)
[2025-05-22 17:40:26,274] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:26,276] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,315] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:26,318] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:26,371] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:26,377] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,478] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,585] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,690] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,791] INFO [MetadataLoader id=5] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,860] INFO [NodeToControllerChannelManager id=5 name=heartbeat] Client requested disconnect from node 2 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:26,862] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:26,863] INFO [RaftManager id=5] High watermark set to Optional[LogOffsetMetadata(offset=16834, metadata=Optional.empty)] for the first time for epoch 20 (org.apache.kafka.raft.FollowerState)
[2025-05-22 17:40:26,877] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 0, but the high water mark is 16834 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:26,911] INFO [broker-5-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:27,066] INFO [BrokerLifecycleManager id=5] Successfully registered broker 5 with broker epoch 16834 (kafka.server.BrokerLifecycleManager)
[2025-05-22 17:40:27,090] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have loaded up to offset 16833, but the high water mark is 16838 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:27,093] INFO [MetadataLoader id=5] initializeNewPublishers: The loader is still catching up because we have loaded up to offset 16833, but the high water mark is 16838 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:27,110] INFO [MetadataLoader id=5] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 16838 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:27,114] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 16837 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:27,123] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing MetadataVersionPublisher(id=5) with a snapshot at offset 16837 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:27,124] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 16837 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:27,128] INFO [BrokerMetadataPublisher id=5] Publishing initial metadata at offset OffsetAndEpoch(offset=16837, epoch=20) with metadata.version 3.8-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2025-05-22 17:40:27,132] INFO [BrokerLifecycleManager id=5] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-22 17:40:27,133] INFO [BrokerServer id=5] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
[2025-05-22 17:40:27,134] INFO [BrokerServer id=5] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-22 17:40:27,144] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,150] INFO [BrokerLifecycleManager id=5] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2025-05-22 17:40:27,185] INFO Skipping recovery of 72 logs from /tmp/kafka-logs since clean shutdown file was found (kafka.log.LogManager)
[2025-05-22 17:40:27,253] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-13/00000000000000255889.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,253] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Loading producer state till offset 305495 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,253] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305495 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,254] INFO [ProducerStateManager partition=financial_transactions-13] Loading producer state from snapshot file 'SnapshotFile(offset=305495, file=/tmp/kafka-logs/financial_transactions-13/00000000000000305495.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,257] INFO [LogLoader partition=financial_transactions-13, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 0ms for segment recovery from offset 305495 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,286] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-13, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305495) with 1 segments, local-log-start-offset 0 and log-end-offset 305495 in 63ms (1/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,333] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-18/00000000000000255590.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,337] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Loading producer state till offset 304723 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,350] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 304723 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,351] INFO [ProducerStateManager partition=financial_transactions-18] Loading producer state from snapshot file 'SnapshotFile(offset=304723, file=/tmp/kafka-logs/financial_transactions-18/00000000000000304723.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,360] INFO [LogLoader partition=financial_transactions-18, dir=/tmp/kafka-logs] Producer state recovery took 10ms for snapshot load and 0ms for segment recovery from offset 304723 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,366] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-18, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=304723) with 1 segments, local-log-start-offset 0 and log-end-offset 304723 in 72ms (2/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,388] INFO Deleted producer state snapshot /tmp/kafka-logs/__consumer_offsets-29/00000000000000000006.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,388] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 8 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,389] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 8 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,390] INFO [ProducerStateManager partition=__consumer_offsets-29] Loading producer state from snapshot file 'SnapshotFile(offset=8, file=/tmp/kafka-logs/__consumer_offsets-29/00000000000000000008.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,391] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 8 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,405] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-29, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=29, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=8) with 1 segments, local-log-start-offset 0 and log-end-offset 8 in 39ms (3/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,419] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-12/00000000000000255590.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,421] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Loading producer state till offset 305079 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,421] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305079 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,423] INFO [ProducerStateManager partition=financial_transactions-12] Loading producer state from snapshot file 'SnapshotFile(offset=305079, file=/tmp/kafka-logs/financial_transactions-12/00000000000000305079.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,425] INFO [LogLoader partition=financial_transactions-12, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 305079 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,441] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-12, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305079) with 1 segments, local-log-start-offset 0 and log-end-offset 305079 in 35ms (4/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,465] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,471] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-38, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=38, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 28ms (5/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,488] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-16/00000000000000255412.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,490] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Loading producer state till offset 304634 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,491] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 304634 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,492] INFO [ProducerStateManager partition=financial_transactions-16] Loading producer state from snapshot file 'SnapshotFile(offset=304634, file=/tmp/kafka-logs/financial_transactions-16/00000000000000304634.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,495] INFO [LogLoader partition=financial_transactions-16, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 0ms for segment recovery from offset 304634 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,497] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-16, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=304634) with 1 segments, local-log-start-offset 0 and log-end-offset 304634 in 24ms (6/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,505] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,507] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-27, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=27, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (7/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,527] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,530] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-39, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=39, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 22ms (8/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,534] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,538] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-42, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=42, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (9/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,550] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,552] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-9, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (10/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,557] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,565] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-22, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=22, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 11ms (11/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,569] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,571] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-26, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=26, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (12/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,585] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,587] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-40, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=40, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 15ms (13/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,591] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,593] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-47, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=47, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (14/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,602] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,604] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-15, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 10ms (15/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,610] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,613] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-10, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (16/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,622] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-11/00000000000000256046.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,623] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Loading producer state till offset 305371 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,624] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305371 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,624] INFO [ProducerStateManager partition=financial_transactions-11] Loading producer state from snapshot file 'SnapshotFile(offset=305371, file=/tmp/kafka-logs/financial_transactions-11/00000000000000305371.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,625] INFO [LogLoader partition=financial_transactions-11, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 305371 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,629] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-11, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305371) with 1 segments, local-log-start-offset 0 and log-end-offset 305371 in 14ms (17/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,636] INFO Deleted producer state snapshot /tmp/kafka-logs/aggregated_transactions-0/00000000000000000352.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,639] INFO [LogLoader partition=aggregated_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 616 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,640] INFO [LogLoader partition=aggregated_transactions-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 616 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,641] INFO [ProducerStateManager partition=aggregated_transactions-0] Loading producer state from snapshot file 'SnapshotFile(offset=616, file=/tmp/kafka-logs/aggregated_transactions-0/00000000000000000616.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,643] INFO [LogLoader partition=aggregated_transactions-0, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 616 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,647] INFO Completed load of Log(dir=/tmp/kafka-logs/aggregated_transactions-0, topicId=tZKMqbfwSlSmyms8wDFH7g, topic=aggregated_transactions, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=616) with 1 segments, local-log-start-offset 0 and log-end-offset 616 in 18ms (18/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,653] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,658] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-2, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (19/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,662] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,664] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-18, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (20/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,668] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,670] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-46, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=46, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (21/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,678] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-1/00000000000000255371.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,679] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Loading producer state till offset 304797 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,680] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 304797 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,680] INFO [ProducerStateManager partition=financial_transactions-1] Loading producer state from snapshot file 'SnapshotFile(offset=304797, file=/tmp/kafka-logs/financial_transactions-1/00000000000000304797.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,681] INFO [LogLoader partition=financial_transactions-1, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 304797 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,682] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-1, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=304797) with 1 segments, local-log-start-offset 0 and log-end-offset 304797 in 10ms (22/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,686] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,688] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-12, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (23/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,691] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,694] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-23, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=23, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (24/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,702] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,704] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-14, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (25/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,710] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,712] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-5, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (26/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,716] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,717] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-41, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=41, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (27/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,740] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-17/00000000000000255866.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,741] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Loading producer state till offset 305294 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,742] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305294 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,744] INFO [ProducerStateManager partition=financial_transactions-17] Loading producer state from snapshot file 'SnapshotFile(offset=305294, file=/tmp/kafka-logs/financial_transactions-17/00000000000000305294.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,745] INFO [LogLoader partition=financial_transactions-17, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 305294 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,764] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-17, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305294) with 1 segments, local-log-start-offset 0 and log-end-offset 305294 in 45ms (28/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,774] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,777] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-1, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (29/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,783] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,785] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-19, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (30/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,788] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,790] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-3, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (31/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,795] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,798] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-16, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (32/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,804] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,806] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-25, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=25, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (33/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,815] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-15/00000000000000256237.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,816] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Loading producer state till offset 305572 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,819] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305572 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,820] INFO [ProducerStateManager partition=financial_transactions-15] Loading producer state from snapshot file 'SnapshotFile(offset=305572, file=/tmp/kafka-logs/financial_transactions-15/00000000000000305572.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,822] INFO [LogLoader partition=financial_transactions-15, dir=/tmp/kafka-logs] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 305572 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,824] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-15, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305572) with 1 segments, local-log-start-offset 0 and log-end-offset 305572 in 17ms (34/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,830] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-3/00000000000000255715.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,830] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Loading producer state till offset 305161 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,831] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305161 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,831] INFO [ProducerStateManager partition=financial_transactions-3] Loading producer state from snapshot file 'SnapshotFile(offset=305161, file=/tmp/kafka-logs/financial_transactions-3/00000000000000305161.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,834] INFO [LogLoader partition=financial_transactions-3, dir=/tmp/kafka-logs] Producer state recovery took 3ms for snapshot load and 0ms for segment recovery from offset 305161 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,836] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-3, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305161) with 1 segments, local-log-start-offset 0 and log-end-offset 305161 in 12ms (35/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,842] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-10/00000000000000256308.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,843] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Loading producer state till offset 305695 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,843] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305695 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,844] INFO [ProducerStateManager partition=financial_transactions-10] Loading producer state from snapshot file 'SnapshotFile(offset=305695, file=/tmp/kafka-logs/financial_transactions-10/00000000000000305695.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,844] INFO [LogLoader partition=financial_transactions-10, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 305695 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,848] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-10, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305695) with 1 segments, local-log-start-offset 0 and log-end-offset 305695 in 11ms (36/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,854] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-19/00000000000000255167.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,854] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Loading producer state till offset 304475 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,854] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 304475 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,855] INFO [ProducerStateManager partition=financial_transactions-19] Loading producer state from snapshot file 'SnapshotFile(offset=304475, file=/tmp/kafka-logs/financial_transactions-19/00000000000000304475.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,855] INFO [LogLoader partition=financial_transactions-19, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 304475 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,857] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-19, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=304475) with 1 segments, local-log-start-offset 0 and log-end-offset 304475 in 8ms (37/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,867] INFO Deleted producer state snapshot /tmp/kafka-logs/_schemas-0/00000000000000000006.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,867] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 8 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,868] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 8 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,868] INFO [ProducerStateManager partition=_schemas-0] Loading producer state from snapshot file 'SnapshotFile(offset=8, file=/tmp/kafka-logs/_schemas-0/00000000000000000008.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,869] INFO [LogLoader partition=_schemas-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 8 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,875] INFO Completed load of Log(dir=/tmp/kafka-logs/_schemas-0, topicId=RrE8eovWRKu4kLR3MRJ0fA, topic=_schemas, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=8) with 1 segments, local-log-start-offset 0 and log-end-offset 8 in 17ms (38/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,880] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,883] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-28, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=28, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (39/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,886] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,888] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-13, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (40/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,891] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,892] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-24, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=24, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (41/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,897] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,900] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-32, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=32, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 7ms (42/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,910] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-14/00000000000000255812.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,910] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Loading producer state till offset 305196 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,911] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305196 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,911] INFO [ProducerStateManager partition=financial_transactions-14] Loading producer state from snapshot file 'SnapshotFile(offset=305196, file=/tmp/kafka-logs/financial_transactions-14/00000000000000305196.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,912] INFO [LogLoader partition=financial_transactions-14, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 305196 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,914] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-14, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305196) with 1 segments, local-log-start-offset 0 and log-end-offset 305196 in 14ms (43/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,920] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-2/00000000000000255541.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,921] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Loading producer state till offset 305058 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,922] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305058 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,922] INFO [ProducerStateManager partition=financial_transactions-2] Loading producer state from snapshot file 'SnapshotFile(offset=305058, file=/tmp/kafka-logs/financial_transactions-2/00000000000000305058.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,923] INFO [LogLoader partition=financial_transactions-2, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 305058 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,925] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-2, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305058) with 1 segments, local-log-start-offset 0 and log-end-offset 305058 in 11ms (44/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,937] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,939] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-31, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=31, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 14ms (45/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,947] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,949] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-21, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=21, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 9ms (46/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,956] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-0/00000000000000257211.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,957] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Loading producer state till offset 306847 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,957] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 306847 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,958] INFO [ProducerStateManager partition=financial_transactions-0] Loading producer state from snapshot file 'SnapshotFile(offset=306847, file=/tmp/kafka-logs/financial_transactions-0/00000000000000306847.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,959] INFO [LogLoader partition=financial_transactions-0, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 306847 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,961] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-0, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=306847) with 1 segments, local-log-start-offset 0 and log-end-offset 306847 in 11ms (47/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,967] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-9/00000000000000255435.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,968] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Loading producer state till offset 304868 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,968] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 304868 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,969] INFO [ProducerStateManager partition=financial_transactions-9] Loading producer state from snapshot file 'SnapshotFile(offset=304868, file=/tmp/kafka-logs/financial_transactions-9/00000000000000304868.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,969] INFO [LogLoader partition=financial_transactions-9, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 304868 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,971] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-9, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=304868) with 1 segments, local-log-start-offset 0 and log-end-offset 304868 in 10ms (48/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,974] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,976] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-6, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (49/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,980] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,982] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-11, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (50/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,989] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-5/00000000000000254990.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:27,989] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Loading producer state till offset 304293 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,990] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 304293 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,990] INFO [ProducerStateManager partition=financial_transactions-5] Loading producer state from snapshot file 'SnapshotFile(offset=304293, file=/tmp/kafka-logs/financial_transactions-5/00000000000000304293.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:27,991] INFO [LogLoader partition=financial_transactions-5, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 304293 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:27,992] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-5, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=304293) with 1 segments, local-log-start-offset 0 and log-end-offset 304293 in 9ms (51/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:27,999] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,001] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-30, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=30, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 8ms (52/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,007] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-6/00000000000000255175.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:28,008] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Loading producer state till offset 304616 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,009] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 304616 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,010] INFO [ProducerStateManager partition=financial_transactions-6] Loading producer state from snapshot file 'SnapshotFile(offset=304616, file=/tmp/kafka-logs/financial_transactions-6/00000000000000304616.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:28,011] INFO [LogLoader partition=financial_transactions-6, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 304616 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,013] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-6, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=304616) with 1 segments, local-log-start-offset 0 and log-end-offset 304616 in 11ms (53/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,018] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,019] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-43, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=43, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (54/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,024] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,026] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-7, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (55/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,030] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,032] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-33, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=33, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (56/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,035] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,038] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-36, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=36, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (57/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,042] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,043] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-45, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=45, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (58/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,046] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,049] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-0, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (59/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,052] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,054] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-20, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=20, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (60/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,057] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,059] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-4, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (61/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,063] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,065] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-8, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 6ms (62/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,067] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,069] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-49, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=49, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (63/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,072] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,074] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-34, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=34, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (64/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,078] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,080] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-48, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=48, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (65/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,084] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-4/00000000000000255335.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:28,085] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Loading producer state till offset 304585 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,085] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 304585 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,086] INFO [ProducerStateManager partition=financial_transactions-4] Loading producer state from snapshot file 'SnapshotFile(offset=304585, file=/tmp/kafka-logs/financial_transactions-4/00000000000000304585.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:28,087] INFO [LogLoader partition=financial_transactions-4, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 304585 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,088] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-4, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=304585) with 1 segments, local-log-start-offset 0 and log-end-offset 304585 in 8ms (66/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,091] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,093] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-44, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=44, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (67/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,097] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,098] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-37, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=37, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (68/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,105] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-7/00000000000000255844.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:28,105] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Loading producer state till offset 305027 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,105] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 305027 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,106] INFO [ProducerStateManager partition=financial_transactions-7] Loading producer state from snapshot file 'SnapshotFile(offset=305027, file=/tmp/kafka-logs/financial_transactions-7/00000000000000305027.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:28,107] INFO [LogLoader partition=financial_transactions-7, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 305027 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,109] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-7, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=305027) with 1 segments, local-log-start-offset 0 and log-end-offset 305027 in 10ms (69/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,113] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,115] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-35, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=35, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 5ms (70/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,119] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,120] INFO Completed load of Log(dir=/tmp/kafka-logs/__consumer_offsets-17, topicId=94Q8ilNOTgqGgcE4hkgLtw, topic=__consumer_offsets, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments, local-log-start-offset 0 and log-end-offset 0 in 4ms (71/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,126] INFO Deleted producer state snapshot /tmp/kafka-logs/financial_transactions-8/00000000000000255525.snapshot (org.apache.kafka.storage.internals.log.SnapshotFile)
[2025-05-22 17:40:28,127] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Loading producer state till offset 304771 with message format version 2 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,127] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 304771 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,128] INFO [ProducerStateManager partition=financial_transactions-8] Loading producer state from snapshot file 'SnapshotFile(offset=304771, file=/tmp/kafka-logs/financial_transactions-8/00000000000000304771.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)
[2025-05-22 17:40:28,129] INFO [LogLoader partition=financial_transactions-8, dir=/tmp/kafka-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 304771 (kafka.log.UnifiedLog$)
[2025-05-22 17:40:28,130] INFO Completed load of Log(dir=/tmp/kafka-logs/financial_transactions-8, topicId=0e8v3fGFR_uwy9DAR-lNZA, topic=financial_transactions, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=304771) with 1 segments, local-log-start-offset 0 and log-end-offset 304771 in 9ms (72/72 completed in /tmp/kafka-logs) (kafka.log.LogManager)
[2025-05-22 17:40:28,135] INFO Loaded 72 logs in 972ms (kafka.log.LogManager)
[2025-05-22 17:40:28,136] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2025-05-22 17:40:28,137] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2025-05-22 17:40:28,146] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2025-05-22 17:40:28,261] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
[2025-05-22 17:40:28,264] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2025-05-22 17:40:28,264] INFO [AddPartitionsToTxnSenderThread-5]: Starting (kafka.server.AddPartitionsToTxnManager)
[2025-05-22 17:40:28,265] INFO [GroupCoordinator 5]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,267] INFO [GroupCoordinator 5]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,268] INFO [TransactionCoordinator id=5] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-22 17:40:28,271] INFO [TxnMarkerSenderThread-5]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2025-05-22 17:40:28,271] INFO [TransactionCoordinator id=5] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2025-05-22 17:40:28,297] INFO [Broker id=5] Transitioning 72 partition(s) to local followers. (state.change.logger)
[2025-05-22 17:40:28,300] INFO [Broker id=5] Creating new partition financial_transactions-13 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,314] INFO [Partition financial_transactions-13 broker=5] Log loaded for partition financial_transactions-13 with initial high watermark 305480 (kafka.cluster.Partition)
[2025-05-22 17:40:28,316] INFO [Broker id=5] Follower financial_transactions-13 starts at leader epoch 11 from offset 305495 with partition epoch 17 and high watermark 305480. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,317] INFO [Broker id=5] Creating new partition __consumer_offsets-13 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,319] INFO [Partition __consumer_offsets-13 broker=5] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,319] INFO [Broker id=5] Follower __consumer_offsets-13 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,320] INFO [Broker id=5] Creating new partition __consumer_offsets-46 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,322] INFO [Partition __consumer_offsets-46 broker=5] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,322] INFO [Broker id=5] Follower __consumer_offsets-46 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,323] INFO [Broker id=5] Creating new partition financial_transactions-17 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,324] INFO [Partition financial_transactions-17 broker=5] Log loaded for partition financial_transactions-17 with initial high watermark 305279 (kafka.cluster.Partition)
[2025-05-22 17:40:28,325] INFO [Broker id=5] Follower financial_transactions-17 starts at leader epoch 8 from offset 305294 with partition epoch 16 and high watermark 305279. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:28,325] INFO [Broker id=5] Creating new partition __consumer_offsets-9 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,327] INFO [Partition __consumer_offsets-9 broker=5] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,327] INFO [Broker id=5] Follower __consumer_offsets-9 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:28,327] INFO [Broker id=5] Creating new partition __consumer_offsets-42 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,329] INFO [Partition __consumer_offsets-42 broker=5] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,329] INFO [Broker id=5] Follower __consumer_offsets-42 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:28,330] INFO [Broker id=5] Creating new partition __consumer_offsets-21 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,331] INFO [Partition __consumer_offsets-21 broker=5] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,331] INFO [Broker id=5] Follower __consumer_offsets-21 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,331] INFO [Broker id=5] Creating new partition __consumer_offsets-17 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,333] INFO [Partition __consumer_offsets-17 broker=5] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,334] INFO [Broker id=5] Follower __consumer_offsets-17 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,334] INFO [Broker id=5] Creating new partition financial_transactions-0 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,335] INFO [Partition financial_transactions-0 broker=5] Log loaded for partition financial_transactions-0 with initial high watermark 306847 (kafka.cluster.Partition)
[2025-05-22 17:40:28,335] INFO [Broker id=5] Follower financial_transactions-0 starts at leader epoch 10 from offset 306847 with partition epoch 16 and high watermark 306847. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:40:28,336] INFO [Broker id=5] Creating new partition __consumer_offsets-30 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,336] INFO [Partition __consumer_offsets-30 broker=5] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,337] INFO [Broker id=5] Follower __consumer_offsets-30 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,337] INFO [Broker id=5] Creating new partition financial_transactions-4 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,338] INFO [Partition financial_transactions-4 broker=5] Log loaded for partition financial_transactions-4 with initial high watermark 304585 (kafka.cluster.Partition)
[2025-05-22 17:40:28,339] INFO [Broker id=5] Follower financial_transactions-4 starts at leader epoch 7 from offset 304585 with partition epoch 16 and high watermark 304585. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:28,339] INFO [Broker id=5] Creating new partition __consumer_offsets-26 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,340] INFO [Partition __consumer_offsets-26 broker=5] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,340] INFO [Broker id=5] Follower __consumer_offsets-26 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,340] INFO [Broker id=5] Creating new partition __consumer_offsets-5 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,341] INFO [Partition __consumer_offsets-5 broker=5] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,342] INFO [Broker id=5] Follower __consumer_offsets-5 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,342] INFO [Broker id=5] Creating new partition financial_transactions-8 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,343] INFO [Partition financial_transactions-8 broker=5] Log loaded for partition financial_transactions-8 with initial high watermark 304757 (kafka.cluster.Partition)
[2025-05-22 17:40:28,343] INFO [Broker id=5] Follower financial_transactions-8 starts at leader epoch 11 from offset 304771 with partition epoch 17 and high watermark 304757. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,343] INFO [Broker id=5] Creating new partition __consumer_offsets-38 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,345] INFO [Partition __consumer_offsets-38 broker=5] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,345] INFO [Broker id=5] Follower __consumer_offsets-38 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:28,345] INFO [Broker id=5] Creating new partition __consumer_offsets-1 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,346] INFO [Partition __consumer_offsets-1 broker=5] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,347] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:28,347] INFO [Broker id=5] Creating new partition financial_transactions-12 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,348] INFO [Partition financial_transactions-12 broker=5] Log loaded for partition financial_transactions-12 with initial high watermark 305062 (kafka.cluster.Partition)
[2025-05-22 17:40:28,349] INFO [Broker id=5] Follower financial_transactions-12 starts at leader epoch 10 from offset 305079 with partition epoch 16 and high watermark 305062. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:40:28,350] INFO [Broker id=5] Creating new partition __consumer_offsets-34 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,351] INFO [Partition __consumer_offsets-34 broker=5] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,352] INFO [Broker id=5] Follower __consumer_offsets-34 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,352] INFO [Broker id=5] Creating new partition financial_transactions-14 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,353] INFO [Partition financial_transactions-14 broker=5] Log loaded for partition financial_transactions-14 with initial high watermark 305196 (kafka.cluster.Partition)
[2025-05-22 17:40:28,353] INFO [Broker id=5] Follower financial_transactions-14 starts at leader epoch 7 from offset 305196 with partition epoch 16 and high watermark 305196. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:28,354] INFO [Broker id=5] Creating new partition __consumer_offsets-16 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,355] INFO [Partition __consumer_offsets-16 broker=5] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,355] INFO [Broker id=5] Follower __consumer_offsets-16 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,355] INFO [Broker id=5] Creating new partition _schemas-0 with topic id RrE8eovWRKu4kLR3MRJ0fA. (state.change.logger)
[2025-05-22 17:40:28,356] INFO [Partition _schemas-0 broker=5] Log loaded for partition _schemas-0 with initial high watermark 8 (kafka.cluster.Partition)
[2025-05-22 17:40:28,357] INFO [Broker id=5] Follower _schemas-0 starts at leader epoch 19 from offset 8 with partition epoch 32 and high watermark 8. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,357] INFO [Broker id=5] Creating new partition __consumer_offsets-45 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,358] INFO [Partition __consumer_offsets-45 broker=5] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,358] INFO [Broker id=5] Follower __consumer_offsets-45 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,359] INFO [Broker id=5] Creating new partition financial_transactions-18 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,360] INFO [Partition financial_transactions-18 broker=5] Log loaded for partition financial_transactions-18 with initial high watermark 304705 (kafka.cluster.Partition)
[2025-05-22 17:40:28,361] INFO [Broker id=5] Follower financial_transactions-18 starts at leader epoch 8 from offset 304723 with partition epoch 16 and high watermark 304705. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:28,361] INFO [Broker id=5] Creating new partition __consumer_offsets-12 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,362] INFO [Partition __consumer_offsets-12 broker=5] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,362] INFO [Broker id=5] Follower __consumer_offsets-12 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,363] INFO [Broker id=5] Creating new partition __consumer_offsets-41 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,364] INFO [Partition __consumer_offsets-41 broker=5] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,364] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:28,364] INFO [Broker id=5] Creating new partition __consumer_offsets-24 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,365] INFO [Partition __consumer_offsets-24 broker=5] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,365] INFO [Broker id=5] Follower __consumer_offsets-24 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,366] INFO [Broker id=5] Creating new partition __consumer_offsets-20 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,367] INFO [Partition __consumer_offsets-20 broker=5] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,367] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:28,367] INFO [Broker id=5] Creating new partition __consumer_offsets-49 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,368] INFO [Partition __consumer_offsets-49 broker=5] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,369] INFO [Broker id=5] Follower __consumer_offsets-49 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,369] INFO [Broker id=5] Creating new partition __consumer_offsets-0 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,370] INFO [Partition __consumer_offsets-0 broker=5] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,370] INFO [Broker id=5] Follower __consumer_offsets-0 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,370] INFO [Broker id=5] Creating new partition __consumer_offsets-29 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,371] INFO [Partition __consumer_offsets-29 broker=5] Log loaded for partition __consumer_offsets-29 with initial high watermark 8 (kafka.cluster.Partition)
[2025-05-22 17:40:28,372] INFO [Broker id=5] Follower __consumer_offsets-29 starts at leader epoch 19 from offset 8 with partition epoch 32 and high watermark 8. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,372] INFO [Broker id=5] Creating new partition financial_transactions-1 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,373] INFO [Partition financial_transactions-1 broker=5] Log loaded for partition financial_transactions-1 with initial high watermark 304782 (kafka.cluster.Partition)
[2025-05-22 17:40:28,374] INFO [Broker id=5] Follower financial_transactions-1 starts at leader epoch 11 from offset 304797 with partition epoch 17 and high watermark 304782. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,374] INFO [Broker id=5] Creating new partition __consumer_offsets-25 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,376] INFO [Partition __consumer_offsets-25 broker=5] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,376] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:28,376] INFO [Broker id=5] Creating new partition financial_transactions-5 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,377] INFO [Partition financial_transactions-5 broker=5] Log loaded for partition financial_transactions-5 with initial high watermark 304275 (kafka.cluster.Partition)
[2025-05-22 17:40:28,377] INFO [Broker id=5] Follower financial_transactions-5 starts at leader epoch 10 from offset 304293 with partition epoch 17 and high watermark 304275. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:40:28,378] INFO [Broker id=5] Creating new partition __consumer_offsets-8 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,379] INFO [Partition __consumer_offsets-8 broker=5] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,379] INFO [Broker id=5] Follower __consumer_offsets-8 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:28,379] INFO [Broker id=5] Creating new partition __consumer_offsets-37 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,380] INFO [Partition __consumer_offsets-37 broker=5] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,380] INFO [Broker id=5] Follower __consumer_offsets-37 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,380] INFO [Broker id=5] Creating new partition financial_transactions-9 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,381] INFO [Partition financial_transactions-9 broker=5] Log loaded for partition financial_transactions-9 with initial high watermark 304857 (kafka.cluster.Partition)
[2025-05-22 17:40:28,381] INFO [Broker id=5] Follower financial_transactions-9 starts at leader epoch 10 from offset 304868 with partition epoch 17 and high watermark 304857. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:40:28,382] INFO [Broker id=5] Creating new partition __consumer_offsets-4 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,383] INFO [Partition __consumer_offsets-4 broker=5] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,383] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:28,383] INFO [Broker id=5] Creating new partition __consumer_offsets-33 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,384] INFO [Partition __consumer_offsets-33 broker=5] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,385] INFO [Broker id=5] Follower __consumer_offsets-33 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,385] INFO [Broker id=5] Creating new partition __consumer_offsets-15 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,386] INFO [Partition __consumer_offsets-15 broker=5] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,386] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:28,387] INFO [Broker id=5] Creating new partition __consumer_offsets-48 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,387] INFO [Partition __consumer_offsets-48 broker=5] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,388] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:28,388] INFO [Broker id=5] Creating new partition financial_transactions-15 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,389] INFO [Partition financial_transactions-15 broker=5] Log loaded for partition financial_transactions-15 with initial high watermark 305572 (kafka.cluster.Partition)
[2025-05-22 17:40:28,389] INFO [Broker id=5] Follower financial_transactions-15 starts at leader epoch 7 from offset 305572 with partition epoch 16 and high watermark 305572. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:28,390] INFO [Broker id=5] Creating new partition __consumer_offsets-11 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,391] INFO [Partition __consumer_offsets-11 broker=5] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,391] INFO [Broker id=5] Follower __consumer_offsets-11 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,391] INFO [Broker id=5] Creating new partition __consumer_offsets-44 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,392] INFO [Partition __consumer_offsets-44 broker=5] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,392] INFO [Broker id=5] Follower __consumer_offsets-44 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,393] INFO [Broker id=5] Creating new partition financial_transactions-19 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,393] INFO [Partition financial_transactions-19 broker=5] Log loaded for partition financial_transactions-19 with initial high watermark 304475 (kafka.cluster.Partition)
[2025-05-22 17:40:28,394] INFO [Broker id=5] Follower financial_transactions-19 starts at leader epoch 7 from offset 304475 with partition epoch 16 and high watermark 304475. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:28,394] INFO [Broker id=5] Creating new partition __consumer_offsets-23 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,395] INFO [Partition __consumer_offsets-23 broker=5] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,395] INFO [Broker id=5] Follower __consumer_offsets-23 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,396] INFO [Broker id=5] Creating new partition __consumer_offsets-19 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,396] INFO [Partition __consumer_offsets-19 broker=5] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,397] INFO [Broker id=5] Follower __consumer_offsets-19 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,397] INFO [Broker id=5] Creating new partition __consumer_offsets-32 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,398] INFO [Partition __consumer_offsets-32 broker=5] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,399] INFO [Broker id=5] Follower __consumer_offsets-32 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,399] INFO [Broker id=5] Creating new partition financial_transactions-2 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,400] INFO [Partition financial_transactions-2 broker=5] Log loaded for partition financial_transactions-2 with initial high watermark 305058 (kafka.cluster.Partition)
[2025-05-22 17:40:28,400] INFO [Broker id=5] Follower financial_transactions-2 starts at leader epoch 7 from offset 305058 with partition epoch 16 and high watermark 305058. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:28,400] INFO [Broker id=5] Creating new partition __consumer_offsets-28 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,401] INFO [Partition __consumer_offsets-28 broker=5] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,401] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:28,402] INFO [Broker id=5] Creating new partition __consumer_offsets-7 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,403] INFO [Partition __consumer_offsets-7 broker=5] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,404] INFO [Broker id=5] Follower __consumer_offsets-7 starts at leader epoch 18 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:28,404] INFO [Broker id=5] Creating new partition financial_transactions-6 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,405] INFO [Partition financial_transactions-6 broker=5] Log loaded for partition financial_transactions-6 with initial high watermark 304616 (kafka.cluster.Partition)
[2025-05-22 17:40:28,405] INFO [Broker id=5] Follower financial_transactions-6 starts at leader epoch 7 from offset 304616 with partition epoch 16 and high watermark 304616. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:28,406] INFO [Broker id=5] Creating new partition __consumer_offsets-40 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,407] INFO [Partition __consumer_offsets-40 broker=5] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,407] INFO [Broker id=5] Follower __consumer_offsets-40 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,407] INFO [Broker id=5] Creating new partition __consumer_offsets-3 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,408] INFO [Partition __consumer_offsets-3 broker=5] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,408] INFO [Broker id=5] Follower __consumer_offsets-3 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,409] INFO [Broker id=5] Creating new partition financial_transactions-10 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,410] INFO [Partition financial_transactions-10 broker=5] Log loaded for partition financial_transactions-10 with initial high watermark 305673 (kafka.cluster.Partition)
[2025-05-22 17:40:28,411] INFO [Broker id=5] Follower financial_transactions-10 starts at leader epoch 8 from offset 305695 with partition epoch 16 and high watermark 305673. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:28,411] INFO [Broker id=5] Creating new partition __consumer_offsets-36 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,412] INFO [Partition __consumer_offsets-36 broker=5] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,412] INFO [Broker id=5] Follower __consumer_offsets-36 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,415] INFO [Broker id=5] Creating new partition __consumer_offsets-47 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,417] INFO [Partition __consumer_offsets-47 broker=5] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,418] INFO [Broker id=5] Follower __consumer_offsets-47 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:28,418] INFO [Broker id=5] Creating new partition financial_transactions-16 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,419] INFO [Partition financial_transactions-16 broker=5] Log loaded for partition financial_transactions-16 with initial high watermark 304604 (kafka.cluster.Partition)
[2025-05-22 17:40:28,419] INFO [Broker id=5] Follower financial_transactions-16 starts at leader epoch 10 from offset 304634 with partition epoch 17 and high watermark 304604. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:40:28,420] INFO [Broker id=5] Creating new partition __consumer_offsets-14 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,420] INFO [Partition __consumer_offsets-14 broker=5] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,421] INFO [Broker id=5] Follower __consumer_offsets-14 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:28,421] INFO [Broker id=5] Creating new partition __consumer_offsets-43 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,422] INFO [Partition __consumer_offsets-43 broker=5] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,422] INFO [Broker id=5] Follower __consumer_offsets-43 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,423] INFO [Broker id=5] Creating new partition __consumer_offsets-10 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,424] INFO [Partition __consumer_offsets-10 broker=5] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,425] INFO [Broker id=5] Follower __consumer_offsets-10 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,425] INFO [Broker id=5] Creating new partition __consumer_offsets-22 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,426] INFO [Partition __consumer_offsets-22 broker=5] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,426] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 17 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:28,427] INFO [Broker id=5] Creating new partition __consumer_offsets-18 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,428] INFO [Partition __consumer_offsets-18 broker=5] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,428] INFO [Broker id=5] Follower __consumer_offsets-18 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,429] INFO [Broker id=5] Creating new partition __consumer_offsets-31 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,430] INFO [Partition __consumer_offsets-31 broker=5] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,430] INFO [Broker id=5] Follower __consumer_offsets-31 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:28,431] INFO [Broker id=5] Creating new partition aggregated_transactions-0 with topic id tZKMqbfwSlSmyms8wDFH7g. (state.change.logger)
[2025-05-22 17:40:28,432] INFO [Partition aggregated_transactions-0 broker=5] Log loaded for partition aggregated_transactions-0 with initial high watermark 616 (kafka.cluster.Partition)
[2025-05-22 17:40:28,432] INFO [Broker id=5] Follower aggregated_transactions-0 starts at leader epoch 5 from offset 616 with partition epoch 5 and high watermark 616. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-22 17:40:28,432] INFO [Broker id=5] Creating new partition __consumer_offsets-27 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,433] INFO [Partition __consumer_offsets-27 broker=5] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,434] INFO [Broker id=5] Follower __consumer_offsets-27 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,435] INFO [Broker id=5] Creating new partition financial_transactions-3 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,436] INFO [Partition financial_transactions-3 broker=5] Log loaded for partition financial_transactions-3 with initial high watermark 305161 (kafka.cluster.Partition)
[2025-05-22 17:40:28,436] INFO [Broker id=5] Follower financial_transactions-3 starts at leader epoch 8 from offset 305161 with partition epoch 16 and high watermark 305161. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:28,436] INFO [Broker id=5] Creating new partition __consumer_offsets-39 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,438] INFO [Partition __consumer_offsets-39 broker=5] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,438] INFO [Broker id=5] Follower __consumer_offsets-39 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,438] INFO [Broker id=5] Creating new partition financial_transactions-7 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,440] INFO [Partition financial_transactions-7 broker=5] Log loaded for partition financial_transactions-7 with initial high watermark 305027 (kafka.cluster.Partition)
[2025-05-22 17:40:28,440] INFO [Broker id=5] Follower financial_transactions-7 starts at leader epoch 10 from offset 305027 with partition epoch 16 and high watermark 305027. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:40:28,440] INFO [Broker id=5] Creating new partition __consumer_offsets-6 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,442] INFO [Partition __consumer_offsets-6 broker=5] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,442] INFO [Broker id=5] Follower __consumer_offsets-6 starts at leader epoch 11 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,442] INFO [Broker id=5] Creating new partition __consumer_offsets-35 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,443] INFO [Partition __consumer_offsets-35 broker=5] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,444] INFO [Broker id=5] Follower __consumer_offsets-35 starts at leader epoch 14 from offset 0 with partition epoch 30 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:28,444] INFO [Broker id=5] Creating new partition financial_transactions-11 with topic id 0e8v3fGFR_uwy9DAR-lNZA. (state.change.logger)
[2025-05-22 17:40:28,445] INFO [Partition financial_transactions-11 broker=5] Log loaded for partition financial_transactions-11 with initial high watermark 305371 (kafka.cluster.Partition)
[2025-05-22 17:40:28,446] INFO [Broker id=5] Follower financial_transactions-11 starts at leader epoch 7 from offset 305371 with partition epoch 16 and high watermark 305371. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:28,446] INFO [Broker id=5] Creating new partition __consumer_offsets-2 with topic id 94Q8ilNOTgqGgcE4hkgLtw. (state.change.logger)
[2025-05-22 17:40:28,447] INFO [Partition __consumer_offsets-2 broker=5] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2025-05-22 17:40:28,447] INFO [Broker id=5] Follower __consumer_offsets-2 starts at leader epoch 19 from offset 0 with partition epoch 32 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:28,449] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, aggregated_transactions-0, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-6, __consumer_offsets-35, financial_transactions-11, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-22 17:40:28,449] INFO [Broker id=5] Stopped fetchers as part of become-follower for 72 partitions (state.change.logger)
[2025-05-22 17:40:28,453] INFO [Broker id=5] Started fetchers as part of become-follower for 72 partitions (state.change.logger)
[2025-05-22 17:40:28,464] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,467] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,468] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,469] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,470] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,470] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,470] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,472] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,472] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,472] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,473] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,474] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,474] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,474] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,475] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,474] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,475] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,476] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,476] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,476] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,477] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,477] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,477] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,478] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,478] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,478] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,478] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,479] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,479] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,479] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,480] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,479] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,480] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,480] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,480] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,481] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,481] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,481] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,481] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,482] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,481] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,482] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,482] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,482] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,483] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,483] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,483] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,484] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,484] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,482] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,485] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,484] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,485] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,485] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,486] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,486] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,487] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,487] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,486] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,487] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,488] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,487] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,488] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,489] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,489] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,489] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,490] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,490] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,490] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,490] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,490] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,492] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,493] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,493] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,494] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,494] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,494] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,494] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,495] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,495] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,495] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,496] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,496] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,496] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,496] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,497] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,497] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,497] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,497] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,498] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,498] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,498] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,499] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,499] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,499] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,500] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,500] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,500] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,500] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,500] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,501] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,501] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[18] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,501] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,501] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,502] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,502] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[18]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,502] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,503] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,503] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,504] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,504] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,504] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,505] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,505] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,505] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,506] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,506] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,506] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,505] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,507] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,507] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,508] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,508] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,508] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,508] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,509] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[17] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,509] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,509] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,510] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,510] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,510] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[17]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,510] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,510] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,511] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,511] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,511] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,511] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,512] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,512] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,512] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,513] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[11] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,513] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,513] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,514] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[14] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,514] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,514] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[11]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,514] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:28,515] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[14]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,515] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,515] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:28,519] INFO [DynamicConfigPublisher broker id=5] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-22 17:40:28,525] INFO [DynamicConfigPublisher broker id=5] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2025-05-22 17:40:28,529] INFO [BrokerServer id=5] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
[2025-05-22 17:40:28,529] INFO [MetadataLoader id=5] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=5) with a snapshot at offset 16837 (org.apache.kafka.image.loader.MetadataLoader)
[2025-05-22 17:40:28,531] INFO KafkaConfig values: 
	advertised.listeners = PLAINTEXT://kafka-broker-2:19092,PLAINTEXT_HOST://localhost:39092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 5
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = producer
	compression.zstd.level = 3
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.bootstrap.servers = []
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-controller-1:9093, 2@kafka-controller-2:9093, 3@kafka-controller-3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.migration.policy = disabled
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.append.linger.ms = 10
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = PLAINTEXT
	inter.broker.protocol.version = 3.8-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
	listeners = PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dir.failure.timeout.ms = 30000
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.initial.task.delay.ms = 30000
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 5
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.feature.versions.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = null
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-05-22 17:40:28,533] INFO RemoteLogManagerConfig values: 
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	remote.fetch.max.wait.ms = 500
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.copier.thread.pool.size = 10
	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
	remote.log.manager.copy.quota.window.num = 11
	remote.log.manager.copy.quota.window.size.seconds = 1
	remote.log.manager.expiration.thread.pool.size = 10
	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
	remote.log.manager.fetch.quota.window.num = 11
	remote.log.manager.fetch.quota.window.size.seconds = 1
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
 (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
[2025-05-22 17:40:28,537] INFO [BrokerServer id=5] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-22 17:40:28,595] INFO [BrokerLifecycleManager id=5] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2025-05-22 17:40:28,595] INFO [BrokerServer id=5] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
[2025-05-22 17:40:28,598] INFO authorizerStart completed for endpoint PLAINTEXT_HOST. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-22 17:40:28,599] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
[2025-05-22 17:40:28,600] INFO [SocketServer listenerType=BROKER, nodeId=5] Enabling request processing. (kafka.network.SocketServer)
[2025-05-22 17:40:28,602] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.DataPlaneAcceptor)
[2025-05-22 17:40:28,605] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[2025-05-22 17:40:28,609] INFO [BrokerServer id=5] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-22 17:40:28,609] INFO [BrokerServer id=5] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
[2025-05-22 17:40:28,610] INFO [BrokerServer id=5] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-22 17:40:28,610] INFO [BrokerServer id=5] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
[2025-05-22 17:40:28,610] INFO [BrokerServer id=5] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2025-05-22 17:40:28,611] INFO Kafka version: 3.8.1 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-22 17:40:28,611] INFO Kafka commitId: 70d6ff42debf7e17 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-22 17:40:28,612] INFO Kafka startTimeMs: 1747935628611 (org.apache.kafka.common.utils.AppInfoParser)
[2025-05-22 17:40:28,614] INFO [KafkaRaftServer nodeId=5] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-05-22 17:40:28,966] INFO [Broker id=5] Transitioning 72 partition(s) to local leaders. (state.change.logger)
[2025-05-22 17:40:28,968] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, financial_transactions-18, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, aggregated_transactions-0, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-6, __consumer_offsets-35, financial_transactions-11, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-22 17:40:28,981] INFO [Broker id=5] Leader financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 12 from offset 305495 with partition epoch 18, high watermark 305480, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:28,996] INFO [Broker id=5] Leader __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 19 from offset 0 with partition epoch 33, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:29,009] INFO [Broker id=5] Leader __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 19 from offset 0 with partition epoch 33, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:29,030] INFO [Broker id=5] Leader financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 9 from offset 305294 with partition epoch 17, high watermark 305279, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:29,035] INFO [Broker id=5] Leader __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:29,041] INFO [Broker id=5] Leader __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:29,046] INFO [Broker id=5] Leader __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,050] INFO [Broker id=5] Leader __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,055] INFO [Broker id=5] Leader financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 11 from offset 306847 with partition epoch 17, high watermark 306847, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:40:29,059] INFO [Broker id=5] Leader __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 19 from offset 0 with partition epoch 33, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:29,065] INFO [Broker id=5] Leader financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 8 from offset 304585 with partition epoch 17, high watermark 304585, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:29,076] INFO [Broker id=5] Leader __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 20 from offset 0 with partition epoch 33, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:29,081] INFO [Broker id=5] Leader __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 20 from offset 0 with partition epoch 33, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:29,086] INFO [Broker id=5] Leader financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 12 from offset 304771 with partition epoch 18, high watermark 304757, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,091] INFO [Broker id=5] Leader __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:29,097] INFO [Broker id=5] Leader __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 18 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:29,102] INFO [Broker id=5] Leader financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 11 from offset 305079 with partition epoch 17, high watermark 305062, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:40:29,107] INFO [Broker id=5] Leader __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 19 from offset 0 with partition epoch 33, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:29,112] INFO [Broker id=5] Leader financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 8 from offset 305196 with partition epoch 17, high watermark 305196, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:29,117] INFO [Broker id=5] Leader __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 20 from offset 0 with partition epoch 33, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:29,134] INFO [Broker id=5] Leader _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) starts at leader epoch 20 from offset 8 with partition epoch 33, high watermark 8, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:29,165] INFO [Broker id=5] Leader __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,183] INFO [Broker id=5] Leader financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 9 from offset 304723 with partition epoch 17, high watermark 304705, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:29,203] INFO [NodeToControllerChannelManager id=5 name=alter-partition] Client requested disconnect from node 3 (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:40:29,204] INFO [Broker id=5] Leader __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,205] INFO [broker-5-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node kafka-controller-2:9093 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-05-22 17:40:29,219] INFO [Broker id=5] Leader __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 18 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:29,229] INFO [Broker id=5] Leader __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,237] INFO [Broker id=5] Leader __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 18 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:29,245] INFO [Broker id=5] Leader __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 20 from offset 0 with partition epoch 33, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:29,257] INFO [Broker id=5] Leader __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,287] INFO [Partition financial_transactions-13 broker=5] ISR updated to 5,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-22 17:40:29,291] INFO [Broker id=5] Leader __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 20 from offset 8 with partition epoch 33, high watermark 8, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:29,295] INFO [Partition financial_transactions-12 broker=5] ISR updated to 5,6  and version updated to 18 (kafka.cluster.Partition)
[2025-05-22 17:40:29,304] INFO [Broker id=5] Leader financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 12 from offset 304797 with partition epoch 18, high watermark 304782, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,312] INFO [Broker id=5] Leader __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 18 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:29,318] INFO [Broker id=5] Leader financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 11 from offset 304293 with partition epoch 18, high watermark 304275, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:40:29,325] INFO [Broker id=5] Leader __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:29,329] INFO [Broker id=5] Leader __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 19 from offset 0 with partition epoch 33, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:29,352] INFO [Partition financial_transactions-14 broker=5] ISR updated to 5,4  and version updated to 18 (kafka.cluster.Partition)
[2025-05-22 17:40:29,352] INFO [Broker id=5] Leader financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 11 from offset 304868 with partition epoch 18, high watermark 304857, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:40:29,353] INFO [Partition __consumer_offsets-16 broker=5] ISR updated to 5,4  and version updated to 34 (kafka.cluster.Partition)
[2025-05-22 17:40:29,353] INFO [Partition _schemas-0 broker=5] ISR updated to 5,4  and version updated to 34 (kafka.cluster.Partition)
[2025-05-22 17:40:29,353] INFO [Partition __consumer_offsets-13 broker=5] ISR updated to 5,6  and version updated to 34 (kafka.cluster.Partition)
[2025-05-22 17:40:29,354] INFO [Partition __consumer_offsets-45 broker=5] ISR updated to 5,4  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:29,354] INFO [Partition __consumer_offsets-46 broker=5] ISR updated to 5,6  and version updated to 34 (kafka.cluster.Partition)
[2025-05-22 17:40:29,354] INFO [Partition __consumer_offsets-9 broker=5] ISR updated to 5,6  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:29,355] INFO [Partition __consumer_offsets-42 broker=5] ISR updated to 5,4  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:29,355] INFO [Partition __consumer_offsets-21 broker=5] ISR updated to 5,4  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:29,355] INFO [Partition __consumer_offsets-17 broker=5] ISR updated to 5,4  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:29,356] INFO [Partition __consumer_offsets-30 broker=5] ISR updated to 5,4  and version updated to 34 (kafka.cluster.Partition)
[2025-05-22 17:40:29,356] INFO [Partition financial_transactions-4 broker=5] ISR updated to 5,4  and version updated to 18 (kafka.cluster.Partition)
[2025-05-22 17:40:29,356] INFO [Partition __consumer_offsets-26 broker=5] ISR updated to 5,4  and version updated to 34 (kafka.cluster.Partition)
[2025-05-22 17:40:29,357] INFO [Partition __consumer_offsets-5 broker=5] ISR updated to 5,4  and version updated to 34 (kafka.cluster.Partition)
[2025-05-22 17:40:29,357] INFO [Partition financial_transactions-8 broker=5] ISR updated to 5,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-22 17:40:29,357] INFO [Partition __consumer_offsets-38 broker=5] ISR updated to 5,4  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:29,358] INFO [Partition __consumer_offsets-1 broker=5] ISR updated to 5,4  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:29,358] INFO [Broker id=5] Leader __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 18 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:29,358] INFO [Partition __consumer_offsets-34 broker=5] ISR updated to 5,4  and version updated to 34 (kafka.cluster.Partition)
[2025-05-22 17:40:29,363] INFO [Broker id=5] Leader __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,368] INFO [Broker id=5] Leader __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 18 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:29,372] INFO [Broker id=5] Leader __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 18 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:29,376] INFO [Broker id=5] Leader financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 8 from offset 305572 with partition epoch 17, high watermark 305572, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:29,382] INFO [Broker id=5] Leader __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 19 from offset 0 with partition epoch 33, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:29,387] INFO [Broker id=5] Leader __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 19 from offset 0 with partition epoch 33, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:29,394] INFO [Broker id=5] Leader financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 8 from offset 304475 with partition epoch 17, high watermark 304475, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:29,401] INFO [Partition financial_transactions-13 broker=5] ISR updated to 5,4,6  and version updated to 20 (kafka.cluster.Partition)
[2025-05-22 17:40:29,401] INFO [Partition financial_transactions-17 broker=5] ISR updated to 5,6  and version updated to 18 (kafka.cluster.Partition)
[2025-05-22 17:40:29,401] INFO [Broker id=5] Leader __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 20 from offset 0 with partition epoch 33, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:29,402] INFO [Partition financial_transactions-0 broker=5] ISR updated to 5,6  and version updated to 18 (kafka.cluster.Partition)
[2025-05-22 17:40:29,405] INFO [Partition financial_transactions-12 broker=5] ISR updated to 5,6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-22 17:40:29,411] INFO [Broker id=5] Leader __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,417] INFO [Broker id=5] Leader __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,426] INFO [Broker id=5] Leader financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 8 from offset 305058 with partition epoch 17, high watermark 305058, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:29,431] INFO [Broker id=5] Leader __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 18 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:29,435] INFO [Broker id=5] Leader __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 19 from offset 0 with partition epoch 33, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 18. (state.change.logger)
[2025-05-22 17:40:29,439] INFO [Broker id=5] Leader financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 8 from offset 304616 with partition epoch 17, high watermark 304616, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:29,444] INFO [Broker id=5] Leader __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,447] INFO [Broker id=5] Leader __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,451] INFO [Broker id=5] Leader financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 9 from offset 305695 with partition epoch 17, high watermark 305673, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:29,455] INFO [Broker id=5] Leader __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,459] INFO [Broker id=5] Leader __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:29,465] INFO [Broker id=5] Leader financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 11 from offset 304634 with partition epoch 18, high watermark 304604, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:40:29,470] INFO [Broker id=5] Leader __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:29,475] INFO [Broker id=5] Leader __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,479] INFO [Broker id=5] Leader __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,483] INFO [Broker id=5] Leader __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 18 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 17. (state.change.logger)
[2025-05-22 17:40:29,488] INFO [Broker id=5] Leader __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 20 from offset 0 with partition epoch 33, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:29,492] INFO [Broker id=5] Leader __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:29,496] INFO [Broker id=5] Leader aggregated_transactions-0 with topic id Some(tZKMqbfwSlSmyms8wDFH7g) starts at leader epoch 6 from offset 616 with partition epoch 6, high watermark 616, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 5. (state.change.logger)
[2025-05-22 17:40:29,501] INFO [Broker id=5] Leader __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,505] INFO [Broker id=5] Leader financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 9 from offset 305161 with partition epoch 17, high watermark 305161, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 8. (state.change.logger)
[2025-05-22 17:40:29,509] INFO [Broker id=5] Leader __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 20 from offset 0 with partition epoch 33, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:29,512] INFO [Broker id=5] Leader financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 11 from offset 305027 with partition epoch 17, high watermark 305027, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:40:29,517] INFO [Broker id=5] Leader __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 12 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 11. (state.change.logger)
[2025-05-22 17:40:29,521] INFO [Broker id=5] Leader __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 15 from offset 0 with partition epoch 31, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 14. (state.change.logger)
[2025-05-22 17:40:29,525] INFO [Broker id=5] Leader financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) starts at leader epoch 8 from offset 305371 with partition epoch 17, high watermark 305371, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 7. (state.change.logger)
[2025-05-22 17:40:29,529] INFO [Broker id=5] Leader __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) starts at leader epoch 20 from offset 0 with partition epoch 33, high watermark 0, ISR [5], adding replicas [] and removing replicas [] . Previous leader Some(-1) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:40:29,535] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 13 in epoch 19 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,535] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,536] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 46 in epoch 19 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,537] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,537] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 9 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,537] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,537] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 42 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,538] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,538] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 21 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,538] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,538] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 17 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,539] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,539] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 30 in epoch 19 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,539] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,540] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 26 in epoch 20 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,540] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,540] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-13 in 3 milliseconds for epoch 19, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,540] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 5 in epoch 20 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,541] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,541] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-46 in 4 milliseconds for epoch 19, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,541] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 38 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,541] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-9 in 4 milliseconds for epoch 15, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,541] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,542] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-42 in 4 milliseconds for epoch 15, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,542] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 1 in epoch 18 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,542] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,542] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-21 in 4 milliseconds for epoch 12, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,543] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 34 in epoch 19 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,543] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,543] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-17 in 4 milliseconds for epoch 12, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,543] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 16 in epoch 20 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,544] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,544] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-30 in 5 milliseconds for epoch 19, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,544] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 45 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,544] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-26 in 4 milliseconds for epoch 20, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,544] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,545] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-5 in 4 milliseconds for epoch 20, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,545] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 12 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,545] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,545] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-38 in 3 milliseconds for epoch 15, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,546] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 41 in epoch 18 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,546] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-1 in 3 milliseconds for epoch 18, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,546] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,547] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-34 in 4 milliseconds for epoch 19, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,547] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 24 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,547] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-16 in 3 milliseconds for epoch 20, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,547] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,548] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 20 in epoch 18 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,548] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,549] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 49 in epoch 20 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,549] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-45 in 4 milliseconds for epoch 12, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,549] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,550] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-12 in 4 milliseconds for epoch 12, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,550] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 0 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,551] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,551] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 29 in epoch 20 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,551] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-41 in 3 milliseconds for epoch 18, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,551] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,552] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-24 in 4 milliseconds for epoch 12, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,552] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 25 in epoch 18 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,552] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-20 in 3 milliseconds for epoch 18, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,552] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,553] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-49 in 3 milliseconds for epoch 20, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,553] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 8 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,553] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,553] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-0 in 2 milliseconds for epoch 12, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,553] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 37 in epoch 19 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,554] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,554] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 4 in epoch 18 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,557] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,557] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 33 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,566] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,566] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 15 in epoch 18 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,568] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,568] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 48 in epoch 18 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,568] INFO Loaded member MemberMetadata(memberId=sr-1-99122467-ec1a-4fc4-88f0-59fda8f60096, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 1. (kafka.coordinator.group.GroupMetadata$)
[2025-05-22 17:40:29,569] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,569] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 11 in epoch 19 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,569] INFO Loaded member MemberMetadata(memberId=sr-1-3027da35-fa63-4fb3-9cba-2acfe293f9a1, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 3. (kafka.coordinator.group.GroupMetadata$)
[2025-05-22 17:40:29,570] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,570] INFO Loaded member MemberMetadata(memberId=sr-1-960e2a90-cf63-43a4-b2ba-89aa2499d218, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.8, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 5. (kafka.coordinator.group.GroupMetadata$)
[2025-05-22 17:40:29,570] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 44 in epoch 19 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,571] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,571] INFO Loaded member MemberMetadata(memberId=sr-1-11f3f728-c325-475e-a3a2-6b3e8884ce37, groupInstanceId=None, clientId=sr-1, clientHost=/172.19.0.13, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) in group schema-registry with generation 7. (kafka.coordinator.group.GroupMetadata$)
[2025-05-22 17:40:29,572] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 23 in epoch 20 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,572] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,572] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 19 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,573] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,573] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 32 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,574] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,574] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 28 in epoch 18 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,574] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,574] INFO [GroupCoordinator 5]: Loading group metadata for schema-registry with generation 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,575] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 7 in epoch 19 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,576] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 19 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,576] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-29 in 24 milliseconds for epoch 20, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,577] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 40 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,577] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,577] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-25 in 24 milliseconds for epoch 18, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,577] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 3 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,578] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-8 in 25 milliseconds for epoch 15, of which 25 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,578] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,578] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-37 in 24 milliseconds for epoch 19, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,578] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 36 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,579] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-4 in 22 milliseconds for epoch 18, of which 22 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,579] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,579] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-33 in 13 milliseconds for epoch 12, of which 13 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,579] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 47 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,580] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,580] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-15 in 12 milliseconds for epoch 18, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,580] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 14 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,580] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-48 in 11 milliseconds for epoch 18, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,580] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,581] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-11 in 11 milliseconds for epoch 19, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,581] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 43 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,581] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-44 in 9 milliseconds for epoch 19, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,581] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,582] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-23 in 10 milliseconds for epoch 20, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,582] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 10 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,582] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,582] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-19 in 9 milliseconds for epoch 12, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,582] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 22 in epoch 18 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,583] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,583] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-32 in 9 milliseconds for epoch 12, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,583] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 18 in epoch 20 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,584] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,583] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-28 in 8 milliseconds for epoch 18, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,584] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 31 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,584] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,584] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 27 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,584] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-7 in 7 milliseconds for epoch 19, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,585] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,585] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-40 in 8 milliseconds for epoch 12, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,585] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 39 in epoch 20 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,586] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,586] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-3 in 8 milliseconds for epoch 12, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,586] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 6 in epoch 12 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,587] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 12 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,586] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-36 in 7 milliseconds for epoch 12, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,587] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 35 in epoch 15 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,587] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,587] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-47 in 7 milliseconds for epoch 15, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,588] INFO [GroupCoordinator 5]: Elected as the group coordinator for partition 2 in epoch 20 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:40:29,588] INFO [GroupMetadataManager brokerId=5] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,588] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-14 in 7 milliseconds for epoch 15, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,589] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-43 in 7 milliseconds for epoch 12, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,589] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-10 in 7 milliseconds for epoch 12, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,590] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-22 in 7 milliseconds for epoch 18, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,590] INFO [Broker id=5] Transitioning 22 partition(s) to local leaders. (state.change.logger)
[2025-05-22 17:40:29,590] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-18 in 6 milliseconds for epoch 20, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,590] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-13, financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-13, __consumer_offsets-45, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, financial_transactions-4, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34) (kafka.server.ReplicaFetcherManager)
[2025-05-22 17:40:29,591] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-31 in 7 milliseconds for epoch 15, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,591] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-27 in 6 milliseconds for epoch 12, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,591] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-39 in 5 milliseconds for epoch 20, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,592] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-13 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 305495, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,592] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-6 in 5 milliseconds for epoch 12, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,592] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=18, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 305196, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,592] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-35 in 5 milliseconds for epoch 15, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,593] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,593] INFO [GroupMetadataManager brokerId=5] Finished loading offsets and group metadata from __consumer_offsets-2 in 5 milliseconds for epoch 20, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:40:29,593] INFO [Broker id=5] Skipped the become-leader state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 8, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,594] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 6], partitionEpoch=34, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 19. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,594] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,594] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 6], partitionEpoch=34, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 19. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,595] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=9, isr=[5, 6], partitionEpoch=18, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 305294, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,595] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=32, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,596] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 4], partitionEpoch=32, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,596] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,596] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,597] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6], partitionEpoch=18, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 306847, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,597] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4], partitionEpoch=34, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 19. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,597] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=18, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 304585, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,598] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,598] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,599] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=19, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 304771, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,599] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 4], partitionEpoch=32, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,600] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 4], partitionEpoch=32, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 18. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,600] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-12 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 305079, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,600] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4], partitionEpoch=34, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 19. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,885] INFO [Partition financial_transactions-4 broker=5] ISR updated to 5,4,6  and version updated to 19 (kafka.cluster.Partition)
[2025-05-22 17:40:29,919] INFO [Broker id=5] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-22 17:40:29,919] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-4) (kafka.server.ReplicaFetcherManager)
[2025-05-22 17:40:29,920] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-4 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4, 6], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 304585, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,925] INFO [Partition financial_transactions-14 broker=5] ISR updated to 5,4,6  and version updated to 19 (kafka.cluster.Partition)
[2025-05-22 17:40:29,925] INFO [Partition __consumer_offsets-16 broker=5] ISR updated to 5,4,6  and version updated to 35 (kafka.cluster.Partition)
[2025-05-22 17:40:29,925] INFO [Partition _schemas-0 broker=5] ISR updated to 5,4,6  and version updated to 35 (kafka.cluster.Partition)
[2025-05-22 17:40:29,926] INFO [Partition __consumer_offsets-45 broker=5] ISR updated to 5,4,6  and version updated to 33 (kafka.cluster.Partition)
[2025-05-22 17:40:29,926] INFO [Partition __consumer_offsets-13 broker=5] ISR updated to 5,6,4  and version updated to 35 (kafka.cluster.Partition)
[2025-05-22 17:40:29,926] INFO [Partition __consumer_offsets-46 broker=5] ISR updated to 5,6,4  and version updated to 35 (kafka.cluster.Partition)
[2025-05-22 17:40:29,927] INFO [Partition financial_transactions-17 broker=5] ISR updated to 5,6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-22 17:40:29,927] INFO [Partition __consumer_offsets-9 broker=5] ISR updated to 5,6,4  and version updated to 33 (kafka.cluster.Partition)
[2025-05-22 17:40:29,927] INFO [Partition __consumer_offsets-42 broker=5] ISR updated to 5,4,6  and version updated to 33 (kafka.cluster.Partition)
[2025-05-22 17:40:29,928] INFO [Partition __consumer_offsets-21 broker=5] ISR updated to 5,4,6  and version updated to 33 (kafka.cluster.Partition)
[2025-05-22 17:40:29,928] INFO [Partition __consumer_offsets-17 broker=5] ISR updated to 5,4,6  and version updated to 33 (kafka.cluster.Partition)
[2025-05-22 17:40:29,928] INFO [Partition __consumer_offsets-30 broker=5] ISR updated to 5,4,6  and version updated to 35 (kafka.cluster.Partition)
[2025-05-22 17:40:29,929] INFO [Partition financial_transactions-0 broker=5] ISR updated to 5,6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-22 17:40:29,929] INFO [Partition __consumer_offsets-26 broker=5] ISR updated to 5,4,6  and version updated to 35 (kafka.cluster.Partition)
[2025-05-22 17:40:29,929] INFO [Partition __consumer_offsets-5 broker=5] ISR updated to 5,4,6  and version updated to 35 (kafka.cluster.Partition)
[2025-05-22 17:40:29,930] INFO [Partition financial_transactions-8 broker=5] ISR updated to 5,4,6  and version updated to 20 (kafka.cluster.Partition)
[2025-05-22 17:40:29,930] INFO [Partition __consumer_offsets-38 broker=5] ISR updated to 5,4,6  and version updated to 33 (kafka.cluster.Partition)
[2025-05-22 17:40:29,930] INFO [Partition __consumer_offsets-1 broker=5] ISR updated to 5,4,6  and version updated to 33 (kafka.cluster.Partition)
[2025-05-22 17:40:29,930] INFO [Partition __consumer_offsets-34 broker=5] ISR updated to 5,4,6  and version updated to 35 (kafka.cluster.Partition)
[2025-05-22 17:40:29,965] INFO [Broker id=5] Transitioning 19 partition(s) to local leaders. (state.change.logger)
[2025-05-22 17:40:29,967] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-14, __consumer_offsets-16, _schemas-0, __consumer_offsets-45, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, financial_transactions-0, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34) (kafka.server.ReplicaFetcherManager)
[2025-05-22 17:40:29,968] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-14 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4, 6], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 305196, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,969] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-16 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,970] INFO [Broker id=5] Skipped the become-leader state change for _schemas-0 with topic id Some(RrE8eovWRKu4kLR3MRJ0fA) and partition state LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 8, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,970] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-45 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,971] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-13 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 6, 4], partitionEpoch=35, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 19. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,972] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-46 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 6, 4], partitionEpoch=35, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 19. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,973] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-17 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=9, isr=[5, 6, 4], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 305294, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,975] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-9 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=33, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,976] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-42 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 4, 6], partitionEpoch=33, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,976] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-21 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,978] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-17 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,978] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-0 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 6, 4], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 306847, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,979] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-30 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 19. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,979] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-26 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,980] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-5 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,980] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-8 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 304771, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,981] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-38 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 4, 6], partitionEpoch=33, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,981] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-1 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 4, 6], partitionEpoch=33, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 18. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:29,982] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-34 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 19. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,399] INFO [Partition financial_transactions-18 broker=5] ISR updated to 5,6  and version updated to 18 (kafka.cluster.Partition)
[2025-05-22 17:40:30,445] INFO [Broker id=5] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-22 17:40:30,446] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-18) (kafka.server.ReplicaFetcherManager)
[2025-05-22 17:40:30,447] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=9, isr=[5, 6], partitionEpoch=18, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 304723, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,453] INFO [Partition __consumer_offsets-12 broker=5] ISR updated to 5,6  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,453] INFO [Partition __consumer_offsets-41 broker=5] ISR updated to 5,6  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,454] INFO [Partition __consumer_offsets-24 broker=5] ISR updated to 5,6  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,454] INFO [Partition __consumer_offsets-20 broker=5] ISR updated to 5,6  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,455] INFO [Partition __consumer_offsets-49 broker=5] ISR updated to 5,6  and version updated to 34 (kafka.cluster.Partition)
[2025-05-22 17:40:30,455] INFO [Partition __consumer_offsets-0 broker=5] ISR updated to 5,6  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,456] INFO [Partition __consumer_offsets-29 broker=5] ISR updated to 5,6  and version updated to 34 (kafka.cluster.Partition)
[2025-05-22 17:40:30,456] INFO [Partition financial_transactions-1 broker=5] ISR updated to 5,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-22 17:40:30,456] INFO [Partition __consumer_offsets-25 broker=5] ISR updated to 5,4  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,457] INFO [Partition financial_transactions-5 broker=5] ISR updated to 5,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-22 17:40:30,457] INFO [Partition __consumer_offsets-8 broker=5] ISR updated to 5,4  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,457] INFO [Partition __consumer_offsets-37 broker=5] ISR updated to 5,4  and version updated to 34 (kafka.cluster.Partition)
[2025-05-22 17:40:30,458] INFO [Partition financial_transactions-9 broker=5] ISR updated to 5,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-22 17:40:30,458] INFO [Partition __consumer_offsets-4 broker=5] ISR updated to 5,6  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,458] INFO [Partition __consumer_offsets-33 broker=5] ISR updated to 5,4  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,459] INFO [Partition __consumer_offsets-15 broker=5] ISR updated to 5,6  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,459] INFO [Partition __consumer_offsets-48 broker=5] ISR updated to 5,4  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,459] INFO [Partition financial_transactions-15 broker=5] ISR updated to 5,4  and version updated to 18 (kafka.cluster.Partition)
[2025-05-22 17:40:30,460] INFO [Partition __consumer_offsets-11 broker=5] ISR updated to 5,4  and version updated to 34 (kafka.cluster.Partition)
[2025-05-22 17:40:30,460] INFO [Partition __consumer_offsets-44 broker=5] ISR updated to 5,4  and version updated to 34 (kafka.cluster.Partition)
[2025-05-22 17:40:30,460] INFO [Partition financial_transactions-19 broker=5] ISR updated to 5,4  and version updated to 18 (kafka.cluster.Partition)
[2025-05-22 17:40:30,460] INFO [Partition __consumer_offsets-23 broker=5] ISR updated to 5,6  and version updated to 34 (kafka.cluster.Partition)
[2025-05-22 17:40:30,461] INFO [Partition __consumer_offsets-19 broker=5] ISR updated to 5,4  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,461] INFO [Partition __consumer_offsets-32 broker=5] ISR updated to 5,6  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,461] INFO [Partition financial_transactions-2 broker=5] ISR updated to 5,4  and version updated to 18 (kafka.cluster.Partition)
[2025-05-22 17:40:30,462] INFO [Partition __consumer_offsets-28 broker=5] ISR updated to 5,4  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,462] INFO [Partition __consumer_offsets-7 broker=5] ISR updated to 5,6  and version updated to 34 (kafka.cluster.Partition)
[2025-05-22 17:40:30,462] INFO [Partition financial_transactions-6 broker=5] ISR updated to 5,4  and version updated to 18 (kafka.cluster.Partition)
[2025-05-22 17:40:30,462] INFO [Partition __consumer_offsets-40 broker=5] ISR updated to 5,4  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,463] INFO [Partition __consumer_offsets-3 broker=5] ISR updated to 5,4  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,463] INFO [Partition __consumer_offsets-36 broker=5] ISR updated to 5,4  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,463] INFO [Partition financial_transactions-10 broker=5] ISR updated to 5,4  and version updated to 18 (kafka.cluster.Partition)
[2025-05-22 17:40:30,464] INFO [Partition __consumer_offsets-47 broker=5] ISR updated to 5,6  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,464] INFO [Partition financial_transactions-16 broker=5] ISR updated to 5,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-22 17:40:30,464] INFO [Partition __consumer_offsets-14 broker=5] ISR updated to 5,6  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,464] INFO [Partition __consumer_offsets-43 broker=5] ISR updated to 5,6  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,465] INFO [Partition __consumer_offsets-10 broker=5] ISR updated to 5,4  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,465] INFO [Partition __consumer_offsets-22 broker=5] ISR updated to 5,6  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,465] INFO [Partition __consumer_offsets-18 broker=5] ISR updated to 5,6  and version updated to 34 (kafka.cluster.Partition)
[2025-05-22 17:40:30,465] INFO [Partition __consumer_offsets-31 broker=5] ISR updated to 5,6  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,466] INFO [Partition __consumer_offsets-27 broker=5] ISR updated to 5,6  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,466] INFO [Partition financial_transactions-3 broker=5] ISR updated to 5,6  and version updated to 18 (kafka.cluster.Partition)
[2025-05-22 17:40:30,466] INFO [Partition __consumer_offsets-39 broker=5] ISR updated to 5,6  and version updated to 34 (kafka.cluster.Partition)
[2025-05-22 17:40:30,466] INFO [Partition financial_transactions-7 broker=5] ISR updated to 5,4  and version updated to 18 (kafka.cluster.Partition)
[2025-05-22 17:40:30,467] INFO [Partition __consumer_offsets-6 broker=5] ISR updated to 5,4  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,467] INFO [Partition __consumer_offsets-35 broker=5] ISR updated to 5,4  and version updated to 32 (kafka.cluster.Partition)
[2025-05-22 17:40:30,467] INFO [Partition financial_transactions-11 broker=5] ISR updated to 5,4  and version updated to 18 (kafka.cluster.Partition)
[2025-05-22 17:40:30,468] INFO [Partition __consumer_offsets-2 broker=5] ISR updated to 5,4  and version updated to 34 (kafka.cluster.Partition)
[2025-05-22 17:40:30,933] INFO [Broker id=5] Transitioning 48 partition(s) to local leaders. (state.change.logger)
[2025-05-22 17:40:30,933] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-39, __consumer_offsets-8, financial_transactions-7, __consumer_offsets-37, __consumer_offsets-6, financial_transactions-9, __consumer_offsets-35, __consumer_offsets-4, financial_transactions-11, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-22 17:40:30,934] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6], partitionEpoch=32, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 18. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,934] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 4], partitionEpoch=32, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 18. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,935] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=18, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 305572, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,936] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4], partitionEpoch=34, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 19. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,936] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4], partitionEpoch=34, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 19. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,937] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=18, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 304475, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,937] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,938] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,942] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6], partitionEpoch=32, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,944] INFO [Partition financial_transactions-18 broker=5] ISR updated to 5,6,4  and version updated to 19 (kafka.cluster.Partition)
[2025-05-22 17:40:30,944] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=18, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 305058, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,946] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 4], partitionEpoch=32, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 18. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,946] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 6], partitionEpoch=34, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 19. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,947] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=18, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 304616, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,948] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,948] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,948] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=9, isr=[5, 4], partitionEpoch=18, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 305695, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,949] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,951] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=32, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,951] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 4], partitionEpoch=19, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 304634, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,951] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=32, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,952] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6], partitionEpoch=32, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,954] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6], partitionEpoch=32, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,955] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6], partitionEpoch=32, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 18. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,956] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,957] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6], partitionEpoch=32, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,957] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6], partitionEpoch=32, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 18. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,958] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6], partitionEpoch=32, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 18. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,959] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,960] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,961] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6], partitionEpoch=32, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,961] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6], partitionEpoch=32, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,962] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 8, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,962] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=19, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 304797, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,963] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6], partitionEpoch=32, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,964] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=9, isr=[5, 6], partitionEpoch=18, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 305161, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,967] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 4], partitionEpoch=32, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 18. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,969] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 4], partitionEpoch=19, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 304293, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,969] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,970] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 4], partitionEpoch=32, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,975] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 4], partitionEpoch=18, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 305027, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,978] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4], partitionEpoch=34, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 19. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,979] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,981] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 4], partitionEpoch=19, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 304868, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,981] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 4], partitionEpoch=32, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,983] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6], partitionEpoch=32, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 18. Current high watermark 0, ISR [5,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,984] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4], partitionEpoch=18, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 305371, ISR [5,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:30,985] INFO [Partition __consumer_offsets-33 broker=5] ISR updated to 5,4,6  and version updated to 33 (kafka.cluster.Partition)
[2025-05-22 17:40:30,986] INFO [Partition __consumer_offsets-2 broker=5] ISR updated to 5,4,6  and version updated to 35 (kafka.cluster.Partition)
[2025-05-22 17:40:30,987] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4], partitionEpoch=32, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since the leader is already at a newer partition epoch 33. (state.change.logger)
[2025-05-22 17:40:30,990] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4], partitionEpoch=34, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since the leader is already at a newer partition epoch 35. (state.change.logger)
[2025-05-22 17:40:30,992] INFO [Broker id=5] Transitioning 1 partition(s) to local leaders. (state.change.logger)
[2025-05-22 17:40:30,993] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-18) (kafka.server.ReplicaFetcherManager)
[2025-05-22 17:40:30,993] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-18 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=9, isr=[5, 6, 4], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 304723, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,466] INFO [Broker id=5] Transitioning 48 partition(s) to local leaders. (state.change.logger)
[2025-05-22 17:40:31,467] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(__consumer_offsets-15, __consumer_offsets-48, financial_transactions-15, __consumer_offsets-11, __consumer_offsets-44, financial_transactions-19, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, financial_transactions-2, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-6, __consumer_offsets-40, __consumer_offsets-3, financial_transactions-10, __consumer_offsets-36, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-22, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-0, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-27, financial_transactions-3, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-39, __consumer_offsets-8, financial_transactions-7, __consumer_offsets-37, __consumer_offsets-6, financial_transactions-9, __consumer_offsets-35, __consumer_offsets-4, financial_transactions-11, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-22 17:40:31,468] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-15 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6, 4], partitionEpoch=33, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 18. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,468] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-48 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 4, 6], partitionEpoch=33, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 18. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,469] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-15 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=15, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4, 6], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 305572, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,470] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-11 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 19. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,471] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-44 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 19. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,472] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-19 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4, 6], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 304475, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,474] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-23 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6, 4], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,474] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-19 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,475] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-32 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6, 4], partitionEpoch=33, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,475] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-2 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4, 6], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 305058, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,476] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-28 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 4, 6], partitionEpoch=33, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 18. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,476] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-7 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 6, 4], partitionEpoch=35, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 19. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,477] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-6 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4, 6], partitionEpoch=19, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 304616, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,477] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-40 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,478] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-3 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,478] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-10 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=9, isr=[5, 4, 6], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 305695, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,479] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-36 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,480] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-47 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=33, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,481] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-16 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=16, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 4, 6], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 304634, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,482] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-14 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=33, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,482] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-43 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6, 4], partitionEpoch=33, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,483] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-12 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6, 4], partitionEpoch=33, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,484] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-41 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6, 4], partitionEpoch=33, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 18. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,485] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-10 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,485] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-24 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6, 4], partitionEpoch=33, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,486] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-22 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6, 4], partitionEpoch=33, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 18. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,486] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-20 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6, 4], partitionEpoch=33, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 18. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,487] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-49 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6, 4], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,488] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-18 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6, 4], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,489] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-31 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 6, 4], partitionEpoch=33, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,490] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-0 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6, 4], partitionEpoch=33, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,491] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-29 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6, 4], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 8, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,491] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-1 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=1, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=20, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 304797, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,492] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-27 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 6, 4], partitionEpoch=33, replicas=[5, 6, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,492] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-3 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=3, controllerEpoch=-1, leader=5, leaderEpoch=9, isr=[5, 6, 4], partitionEpoch=19, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 9. Current high watermark 305161, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,493] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-25 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 4, 6], partitionEpoch=33, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 18. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,493] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-5 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=5, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 4, 6], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 304293, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,495] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-39 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 6, 4], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,495] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-8 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 4, 6], partitionEpoch=33, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,496] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-7 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=7, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 4, 6], partitionEpoch=19, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 305027, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,497] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-37 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=-1, leader=5, leaderEpoch=19, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 19. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,497] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-6 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,498] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-9 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=9, controllerEpoch=-1, leader=5, leaderEpoch=11, isr=[5, 4, 6], partitionEpoch=20, replicas=[4, 6, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 11. Current high watermark 304868, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,499] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-35 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=-1, leader=5, leaderEpoch=15, isr=[5, 4, 6], partitionEpoch=33, replicas=[6, 5, 4], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 15. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,499] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-4 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=-1, leader=5, leaderEpoch=18, isr=[5, 6, 4], partitionEpoch=33, replicas=[6, 4, 5], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 18. Current high watermark 0, ISR [5,6,4], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,500] INFO [Broker id=5] Skipped the become-leader state change for financial_transactions-11 with topic id Some(0e8v3fGFR_uwy9DAR-lNZA) and partition state LeaderAndIsrPartitionState(topicName='financial_transactions', partitionIndex=11, controllerEpoch=-1, leader=5, leaderEpoch=8, isr=[5, 4, 6], partitionEpoch=19, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 8. Current high watermark 305371, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,501] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-33 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=-1, leader=5, leaderEpoch=12, isr=[5, 4, 6], partitionEpoch=33, replicas=[5, 4, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 12. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:40:31,501] INFO [Broker id=5] Skipped the become-leader state change for __consumer_offsets-2 with topic id Some(94Q8ilNOTgqGgcE4hkgLtw) and partition state LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=-1, leader=5, leaderEpoch=20, isr=[5, 4, 6], partitionEpoch=35, replicas=[4, 5, 6], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already the leader with leader epoch 20. Current high watermark 0, ISR [5,4,6], adding replicas [] and removing replicas []. (state.change.logger)
[2025-05-22 17:45:27,066] INFO [Broker id=5] Transitioning 48 partition(s) to local followers. (state.change.logger)
[2025-05-22 17:45:27,066] INFO [Broker id=5] Follower financial_transactions-13 starts at leader epoch 13 from offset 305495 with partition epoch 21 and high watermark 305495. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-22 17:45:27,067] INFO [Broker id=5] Follower __consumer_offsets-15 starts at leader epoch 19 from offset 0 with partition epoch 34 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:45:27,067] INFO [Broker id=5] Follower __consumer_offsets-48 starts at leader epoch 19 from offset 0 with partition epoch 34 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:45:27,068] INFO [Broker id=5] Follower __consumer_offsets-13 starts at leader epoch 20 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:45:27,068] INFO [Broker id=5] Follower __consumer_offsets-46 starts at leader epoch 20 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:45:27,068] INFO [Broker id=5] Follower financial_transactions-17 starts at leader epoch 10 from offset 305294 with partition epoch 20 and high watermark 305294. Current leader is 6. Previous leader Some(6) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:45:27,068] INFO [Broker id=5] Follower __consumer_offsets-11 starts at leader epoch 20 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:45:27,069] INFO [Broker id=5] Follower __consumer_offsets-44 starts at leader epoch 20 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:45:27,069] INFO [Broker id=5] Follower __consumer_offsets-9 starts at leader epoch 16 from offset 0 with partition epoch 34 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 16. (state.change.logger)
[2025-05-22 17:45:27,069] INFO [Broker id=5] Follower __consumer_offsets-42 starts at leader epoch 16 from offset 0 with partition epoch 34 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 16. (state.change.logger)
[2025-05-22 17:45:27,070] INFO [Broker id=5] Follower __consumer_offsets-23 starts at leader epoch 21 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,071] INFO [Broker id=5] Follower financial_transactions-0 starts at leader epoch 12 from offset 306847 with partition epoch 20 and high watermark 306847. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:45:27,071] INFO [Broker id=5] Follower __consumer_offsets-30 starts at leader epoch 20 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:45:27,072] INFO [Broker id=5] Follower __consumer_offsets-28 starts at leader epoch 19 from offset 0 with partition epoch 34 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:45:27,072] INFO [Broker id=5] Follower __consumer_offsets-26 starts at leader epoch 21 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,073] INFO [Broker id=5] Follower __consumer_offsets-7 starts at leader epoch 20 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:45:27,073] INFO [Broker id=5] Follower __consumer_offsets-5 starts at leader epoch 21 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,073] INFO [Broker id=5] Follower financial_transactions-8 starts at leader epoch 13 from offset 304771 with partition epoch 21 and high watermark 304771. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-22 17:45:27,073] INFO [Broker id=5] Follower __consumer_offsets-38 starts at leader epoch 16 from offset 0 with partition epoch 34 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 16. (state.change.logger)
[2025-05-22 17:45:27,074] INFO [Broker id=5] Follower financial_transactions-10 starts at leader epoch 10 from offset 305695 with partition epoch 20 and high watermark 305695. Current leader is 6. Previous leader Some(6) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:45:27,074] INFO [Broker id=5] Follower __consumer_offsets-1 starts at leader epoch 19 from offset 0 with partition epoch 34 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:45:27,074] INFO [Broker id=5] Follower financial_transactions-12 starts at leader epoch 12 from offset 305079 with partition epoch 20 and high watermark 305079. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:45:27,074] INFO [Broker id=5] Follower __consumer_offsets-34 starts at leader epoch 20 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:45:27,075] INFO [Broker id=5] Follower __consumer_offsets-47 starts at leader epoch 16 from offset 0 with partition epoch 34 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 16. (state.change.logger)
[2025-05-22 17:45:27,075] INFO [Broker id=5] Follower __consumer_offsets-16 starts at leader epoch 21 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,076] INFO [Broker id=5] Follower _schemas-0 starts at leader epoch 21 from offset 8 with partition epoch 36 and high watermark 8. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,076] INFO [Broker id=5] Follower financial_transactions-16 starts at leader epoch 12 from offset 304634 with partition epoch 21 and high watermark 304634. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:45:27,076] INFO [Broker id=5] Follower __consumer_offsets-14 starts at leader epoch 16 from offset 0 with partition epoch 34 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 16. (state.change.logger)
[2025-05-22 17:45:27,076] INFO [Broker id=5] Follower financial_transactions-18 starts at leader epoch 10 from offset 304723 with partition epoch 20 and high watermark 304723. Current leader is 6. Previous leader Some(6) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:45:27,077] INFO [Broker id=5] Follower __consumer_offsets-41 starts at leader epoch 19 from offset 0 with partition epoch 34 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:45:27,077] INFO [Broker id=5] Follower __consumer_offsets-22 starts at leader epoch 19 from offset 0 with partition epoch 34 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:45:27,077] INFO [Broker id=5] Follower __consumer_offsets-20 starts at leader epoch 19 from offset 0 with partition epoch 34 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:45:27,078] INFO [Broker id=5] Follower __consumer_offsets-49 starts at leader epoch 21 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,078] INFO [Broker id=5] Follower __consumer_offsets-18 starts at leader epoch 21 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,078] INFO [Broker id=5] Follower __consumer_offsets-31 starts at leader epoch 16 from offset 0 with partition epoch 34 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 16. (state.change.logger)
[2025-05-22 17:45:27,079] INFO [Broker id=5] Follower __consumer_offsets-29 starts at leader epoch 21 from offset 8 with partition epoch 36 and high watermark 8. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,079] INFO [Broker id=5] Follower financial_transactions-1 starts at leader epoch 13 from offset 304797 with partition epoch 21 and high watermark 304797. Current leader is 4. Previous leader Some(4) and previous leader epoch was 13. (state.change.logger)
[2025-05-22 17:45:27,079] INFO [Broker id=5] Follower financial_transactions-3 starts at leader epoch 10 from offset 305161 with partition epoch 20 and high watermark 305161. Current leader is 6. Previous leader Some(6) and previous leader epoch was 10. (state.change.logger)
[2025-05-22 17:45:27,079] INFO [Broker id=5] Follower __consumer_offsets-25 starts at leader epoch 19 from offset 0 with partition epoch 34 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:45:27,079] INFO [Broker id=5] Follower financial_transactions-5 starts at leader epoch 12 from offset 304293 with partition epoch 21 and high watermark 304293. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:45:27,080] INFO [Broker id=5] Follower __consumer_offsets-39 starts at leader epoch 21 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,080] INFO [Broker id=5] Follower __consumer_offsets-8 starts at leader epoch 16 from offset 0 with partition epoch 34 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 16. (state.change.logger)
[2025-05-22 17:45:27,080] INFO [Broker id=5] Follower financial_transactions-7 starts at leader epoch 12 from offset 305027 with partition epoch 20 and high watermark 305027. Current leader is 6. Previous leader Some(6) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:45:27,080] INFO [Broker id=5] Follower __consumer_offsets-37 starts at leader epoch 20 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 20. (state.change.logger)
[2025-05-22 17:45:27,081] INFO [Broker id=5] Follower financial_transactions-9 starts at leader epoch 12 from offset 304868 with partition epoch 21 and high watermark 304868. Current leader is 4. Previous leader Some(4) and previous leader epoch was 12. (state.change.logger)
[2025-05-22 17:45:27,081] INFO [Broker id=5] Follower __consumer_offsets-35 starts at leader epoch 16 from offset 0 with partition epoch 34 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 16. (state.change.logger)
[2025-05-22 17:45:27,081] INFO [Broker id=5] Follower __consumer_offsets-4 starts at leader epoch 19 from offset 0 with partition epoch 34 and high watermark 0. Current leader is 6. Previous leader Some(6) and previous leader epoch was 19. (state.change.logger)
[2025-05-22 17:45:27,082] INFO [Broker id=5] Follower __consumer_offsets-2 starts at leader epoch 21 from offset 0 with partition epoch 36 and high watermark 0. Current leader is 4. Previous leader Some(4) and previous leader epoch was 21. (state.change.logger)
[2025-05-22 17:45:27,082] INFO [ReplicaFetcherManager on broker 5] Removed fetcher for partitions Set(financial_transactions-13, __consumer_offsets-13, __consumer_offsets-46, financial_transactions-17, __consumer_offsets-9, __consumer_offsets-42, financial_transactions-0, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, financial_transactions-8, __consumer_offsets-38, __consumer_offsets-1, financial_transactions-12, __consumer_offsets-34, __consumer_offsets-16, _schemas-0, financial_transactions-18, __consumer_offsets-41, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-29, financial_transactions-1, __consumer_offsets-25, financial_transactions-5, __consumer_offsets-8, __consumer_offsets-37, financial_transactions-9, __consumer_offsets-4, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-28, __consumer_offsets-7, financial_transactions-10, __consumer_offsets-47, financial_transactions-16, __consumer_offsets-14, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, financial_transactions-3, __consumer_offsets-39, financial_transactions-7, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2025-05-22 17:45:27,082] INFO [Broker id=5] Stopped fetchers as part of become-follower for 48 partitions (state.change.logger)
[2025-05-22 17:45:27,096] INFO [ReplicaFetcherThread-0-4]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,098] INFO [ReplicaFetcherManager on broker 5] Added fetcher to broker 4 for partitions HashMap(financial_transactions-13 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,305495), __consumer_offsets-13 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),20,0), __consumer_offsets-46 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),20,0), __consumer_offsets-11 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),20,0), __consumer_offsets-44 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),20,0), __consumer_offsets-23 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,0), __consumer_offsets-30 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),20,0), __consumer_offsets-26 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,0), __consumer_offsets-7 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),20,0), __consumer_offsets-5 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,0), financial_transactions-8 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,304771), __consumer_offsets-34 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),20,0), __consumer_offsets-16 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,0), _schemas-0 -> InitialFetchState(Some(RrE8eovWRKu4kLR3MRJ0fA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,8), financial_transactions-16 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,304634), __consumer_offsets-49 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,0), __consumer_offsets-18 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,0), __consumer_offsets-29 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,8), financial_transactions-1 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),13,304797), financial_transactions-5 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,304293), __consumer_offsets-39 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,0), __consumer_offsets-37 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),20,0), financial_transactions-9 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=4, host=kafka-broker-1:19092),12,304868), __consumer_offsets-2 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=4, host=kafka-broker-1:19092),21,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-22 17:45:27,101] INFO [ReplicaFetcherThread-0-6]: Starting (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,102] INFO [ReplicaFetcherManager on broker 5] Added fetcher to broker 6 for partitions HashMap(__consumer_offsets-15 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),19,0), __consumer_offsets-48 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),19,0), financial_transactions-17 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),10,305294), __consumer_offsets-9 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),16,0), __consumer_offsets-42 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),16,0), financial_transactions-0 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,306847), __consumer_offsets-28 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),19,0), __consumer_offsets-38 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),16,0), financial_transactions-10 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),10,305695), __consumer_offsets-1 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),19,0), financial_transactions-12 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,305079), __consumer_offsets-47 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),16,0), __consumer_offsets-14 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),16,0), financial_transactions-18 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),10,304723), __consumer_offsets-41 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),19,0), __consumer_offsets-22 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),19,0), __consumer_offsets-20 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),19,0), __consumer_offsets-31 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),16,0), financial_transactions-3 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),10,305161), __consumer_offsets-25 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),19,0), __consumer_offsets-8 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),16,0), financial_transactions-7 -> InitialFetchState(Some(0e8v3fGFR_uwy9DAR-lNZA),BrokerEndPoint(id=6, host=kafka-broker-3:19092),12,305027), __consumer_offsets-35 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),16,0), __consumer_offsets-4 -> InitialFetchState(Some(94Q8ilNOTgqGgcE4hkgLtw),BrokerEndPoint(id=6, host=kafka-broker-3:19092),19,0)) (kafka.server.ReplicaFetcherManager)
[2025-05-22 17:45:27,102] INFO [Broker id=5] Started fetchers as part of become-follower for 48 partitions (state.change.logger)
[2025-05-22 17:45:27,120] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,123] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,123] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,123] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,124] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,125] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,125] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,125] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,127] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,127] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,128] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,129] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,129] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,130] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,131] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,131] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,132] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,133] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,133] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,133] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,134] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,134] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,135] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[21] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,135] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,135] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,135] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,134] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,136] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,136] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,137] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,136] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[21]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,138] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,137] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[21] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,139] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,139] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,140] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,141] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,141] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[21]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,142] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,142] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[21] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,143] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,144] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[21]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,144] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,145] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,145] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,145] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,146] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,146] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,146] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,146] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,147] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,147] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,148] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[21] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,148] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,148] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,147] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,148] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,149] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,149] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,149] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,149] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[21]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,150] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,150] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,152] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,151] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,153] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,153] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,154] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,157] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,157] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[21] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,158] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,159] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[21]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,159] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[21] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,159] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,160] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[21]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,164] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition financial_transactions-13 with TruncationState(offset=305495, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=13, leaderEpoch=9, endOffset=305495) (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,160] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,165] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,165] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition financial_transactions-17 with TruncationState(offset=305294, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=17, leaderEpoch=7, endOffset=305294) (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,166] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,166] INFO [UnifiedLog partition=financial_transactions-13, dir=/tmp/kafka-logs] Truncating to 305495 has no effect as the largest offset in the log is 305494 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,166] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[21] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,166] INFO [UnifiedLog partition=financial_transactions-17, dir=/tmp/kafka-logs] Truncating to 305294 has no effect as the largest offset in the log is 305293 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,167] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,168] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,168] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition _schemas-0 with TruncationState(offset=8, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=0, leaderEpoch=16, endOffset=8) (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,168] INFO [GroupCoordinator 5]: Unloading group metadata for schema-registry with generation 8 (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,168] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition financial_transactions-18 with TruncationState(offset=304723, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=18, leaderEpoch=6, endOffset=304723) (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,168] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,169] INFO [UnifiedLog partition=financial_transactions-18, dir=/tmp/kafka-logs] Truncating to 304723 has no effect as the largest offset in the log is 304722 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,168] INFO [UnifiedLog partition=_schemas-0, dir=/tmp/kafka-logs] Truncating to 8 has no effect as the largest offset in the log is 7 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,169] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[21] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,170] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[21]. Removed 0 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,170] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,170] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,170] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition financial_transactions-16 with TruncationState(offset=304634, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=16, leaderEpoch=8, endOffset=304634) (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:27,171] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[21]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,171] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,171] INFO [UnifiedLog partition=financial_transactions-16, dir=/tmp/kafka-logs] Truncating to 304634 has no effect as the largest offset in the log is 304633 (kafka.log.UnifiedLog)
[2025-05-22 17:45:27,171] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,172] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[20] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,173] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,173] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,174] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[16] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,175] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,174] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[20]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,175] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[19] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,175] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[16]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,176] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,176] INFO [GroupCoordinator 5]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[21] (kafka.coordinator.group.GroupCoordinator)
[2025-05-22 17:45:27,176] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[19]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,177] INFO [GroupMetadataManager brokerId=5] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:27,177] INFO [GroupMetadataManager brokerId=5] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[21]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2025-05-22 17:45:28,192] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=8, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=29, leaderEpoch=17, endOffset=8) (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:28,193] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Truncating to 8 has no effect as the largest offset in the log is 7 (kafka.log.UnifiedLog)
[2025-05-22 17:45:28,193] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition financial_transactions-1 with TruncationState(offset=304797, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=1, leaderEpoch=9, endOffset=304797) (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:28,193] INFO [UnifiedLog partition=financial_transactions-1, dir=/tmp/kafka-logs] Truncating to 304797 has no effect as the largest offset in the log is 304796 (kafka.log.UnifiedLog)
[2025-05-22 17:45:28,194] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition financial_transactions-5 with TruncationState(offset=304293, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=5, leaderEpoch=8, endOffset=304293) (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:28,194] INFO [UnifiedLog partition=financial_transactions-5, dir=/tmp/kafka-logs] Truncating to 304293 has no effect as the largest offset in the log is 304292 (kafka.log.UnifiedLog)
[2025-05-22 17:45:28,194] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition financial_transactions-8 with TruncationState(offset=304771, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=8, leaderEpoch=9, endOffset=304771) (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:28,195] INFO [UnifiedLog partition=financial_transactions-8, dir=/tmp/kafka-logs] Truncating to 304771 has no effect as the largest offset in the log is 304770 (kafka.log.UnifiedLog)
[2025-05-22 17:45:28,195] INFO [ReplicaFetcher replicaId=5, leaderId=4, fetcherId=0] Truncating partition financial_transactions-9 with TruncationState(offset=304868, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=9, leaderEpoch=8, endOffset=304868) (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:28,195] INFO [UnifiedLog partition=financial_transactions-9, dir=/tmp/kafka-logs] Truncating to 304868 has no effect as the largest offset in the log is 304867 (kafka.log.UnifiedLog)
[2025-05-22 17:45:28,196] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition financial_transactions-0 with TruncationState(offset=306847, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=0, leaderEpoch=7, endOffset=306847) (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:28,196] INFO [UnifiedLog partition=financial_transactions-0, dir=/tmp/kafka-logs] Truncating to 306847 has no effect as the largest offset in the log is 306846 (kafka.log.UnifiedLog)
[2025-05-22 17:45:28,196] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition financial_transactions-3 with TruncationState(offset=305161, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=3, leaderEpoch=6, endOffset=305161) (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:28,196] INFO [UnifiedLog partition=financial_transactions-3, dir=/tmp/kafka-logs] Truncating to 305161 has no effect as the largest offset in the log is 305160 (kafka.log.UnifiedLog)
[2025-05-22 17:45:28,197] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition financial_transactions-7 with TruncationState(offset=305027, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=7, leaderEpoch=7, endOffset=305027) (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:28,197] INFO [UnifiedLog partition=financial_transactions-7, dir=/tmp/kafka-logs] Truncating to 305027 has no effect as the largest offset in the log is 305026 (kafka.log.UnifiedLog)
[2025-05-22 17:45:28,198] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition financial_transactions-10 with TruncationState(offset=305695, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=10, leaderEpoch=7, endOffset=305695) (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:28,198] INFO [UnifiedLog partition=financial_transactions-10, dir=/tmp/kafka-logs] Truncating to 305695 has no effect as the largest offset in the log is 305694 (kafka.log.UnifiedLog)
[2025-05-22 17:45:28,198] INFO [ReplicaFetcher replicaId=5, leaderId=6, fetcherId=0] Truncating partition financial_transactions-12 with TruncationState(offset=305079, completed=true) due to leader epoch and offset EpochEndOffset(errorCode=0, partition=12, leaderEpoch=7, endOffset=305079) (kafka.server.ReplicaFetcherThread)
[2025-05-22 17:45:28,199] INFO [UnifiedLog partition=financial_transactions-12, dir=/tmp/kafka-logs] Truncating to 305079 has no effect as the largest offset in the log is 305078 (kafka.log.UnifiedLog)
[2025-05-22 17:50:26,504] INFO [RaftManager id=5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-05-22 17:50:31,483] INFO [NodeToControllerChannelManager id=5 name=alter-partition] Node 2 disconnected. (org.apache.kafka.clients.NetworkClient)
